/*
SQLyog Ultimate v13.1.1 (64 bit)
MySQL - 10.4.14-MariaDB : Database - mashup
*********************************************************************
*/

/*!40101 SET NAMES utf8 */;

/*!40101 SET SQL_MODE=''*/;

/*!40014 SET @OLD_UNIQUE_CHECKS=@@UNIQUE_CHECKS, UNIQUE_CHECKS=0 */;
/*!40014 SET @OLD_FOREIGN_KEY_CHECKS=@@FOREIGN_KEY_CHECKS, FOREIGN_KEY_CHECKS=0 */;
/*!40101 SET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE='NO_AUTO_VALUE_ON_ZERO' */;
/*!40111 SET @OLD_SQL_NOTES=@@SQL_NOTES, SQL_NOTES=0 */;
CREATE DATABASE /*!32312 IF NOT EXISTS*/`mashup` /*!40100 DEFAULT CHARACTER SET latin1 */;

USE `mashup`;

/*Table structure for table `abstracts` */

DROP TABLE IF EXISTS `abstracts`;

CREATE TABLE `abstracts` (
  `PaperID` int(10) NOT NULL AUTO_INCREMENT,
  `Abstract` varchar(4000) CHARACTER SET utf8 DEFAULT NULL,
  PRIMARY KEY (`PaperID`),
  KEY `new_index` (`PaperID`)
) ENGINE=InnoDB AUTO_INCREMENT=1479 DEFAULT CHARSET=latin1;

/*Data for the table `abstracts` */

insert  into `abstracts`(`PaperID`,`Abstract`) values 
(1,'This article aims at the integration of communities of practice into work processes. Linear structures are often inappropriate for the execution of knowledge intensive tasks and work processes. The latter are characterized by non-linear sequences and dynamic, social interaction. But for the work in communities of practice the leading path, that is needed for structuring the work, is often missing. Our article exposes the requirements in order to integrate the dynamic, social processes of the knowledge generation in communities of practice with formal described knowledge intensive processes. For the support of communities the Wiki-concept is introduced. In order to integrate communities into process structures a concept for an appropriate interface is presented. On the basis of this interface concept an information retrieval algorithm is used to connect the process-oriented structures with community-oriented structures. The prototype realisation of this concept is shown by a short example.'),
(2,''),
(3,'E-learning at the workplace might be accomplished by synchronous cooperative learning sessions of small groups using net-based communication. This form of learning is suitable both for course-based e-learning as well as for knowledge transfer within the company. The small groups learn self-regulated, i.e. without the guidance of an instructor. However, the learning tasks are pre-defined and a specific learning process is precisely described. In the present study, the goal of the cooperative learning sessions is to deepen pre-existing declarative knowledge. During cooperative learning, group members are required to actively use, acquire, enrich and exchange their knowledge. In a field study carried out in a large software company, a software tool was used which supported the specific process by phase-specific delivering of instructions and learning materials as well as by means of process control (including turn-taking, role assignment, and coordination of task flow). The results of the empirical evaluation demonstrate a high amount of topic-oriented contributions and the realization of the expected learning activities. However, feedback data indicated a low acceptance of the software tool because of its restrictive process control. It is discussed that there might have been a non-optimal interaction between the factors technology and target group in the study.'),
(4,'There are various Knowledge Management Systems available currently and designed to support knowledge sharing and learning. An example of these are Experience-based Information Systems in the domain of Software Engineering, i.e., Information Systems designed to support experience management. Lately, these have become more and more sophisticated from a technical point of view. However, there are several shortcomings that appear to limit the input, the content of these systems and their usage. The problems identified in this paper relate to knowledge acquisition, learning issues, as well as to the users\'  motivation and trust. We introduce an approach meant to enhance the content of the experience base and improve learning from experiences within information spaces, namely weblogs that are maintained during daily work and serve as input for both an experience base and for an information element base. In order to enhance learning, a pedagogical information agent is envisaged for retrieving suitable experiences to be further enriched with additional information elements and produce micro-didactical learning arrangements. In addition we consider the relevance of motivation and trust issues. An empirical study demonstrates that using weblogs for such an approach is feasible.'),
(5,'Process-oriented Knowledge Management aims to provide adequate information for employees, especially in weakly structured and information-intensive business processes. Beside a technical software solution, which uses a pre-structured, context-aware and collaborative information space that combines processes, domain specific semantic structures and document parts, this requires a methodology to model the process and other context-dimensions, such as roles. Moreover, a guideline and clear service modules are necessary to introduce process-oriented Knowledge Management in companies, especially in small and medium-sized enterprises (SME). Such solutions were developed in the cooperative research project <u>PreBIS</u> (Pre-<u>Build</u> Information Space).'),
(6,'This article introduces two theoretical concepts for the emerging field Knowledge Visualization and discusses a new visualization application that was used to communicate a long-term project to various stakeholders in an organization. First, we introduce a theoretical framework and a model for Knowledge Visualization. The framework and the model identify and relate the key-aspects for successful Knowledge Visualization applications. Next, we present an evaluation of an implemented Knowledge Visualization application: The Tube Map Visualization. A quality development process had to be established in an education centre for health care professions. Traditional project plans, flyers, and mails did not manage to get the attention, did present overview and detail insufficiently, and did not motivate the employees for actions. Because Visual Metaphors are effective for Knowledge Communication we developed a customized Knowledge Map based on the tube system metaphor. The Tube Map Visualization illustrates the whole project, where each tube line represents a target group and each station a milestone. The visualization was printed as a poster and displayed at prominent locations in the organization. The findings of an evaluation indicate that the Tube Map Visualization is a powerful metaphor to communicate a complex project to different target groups and to build up a mutual story. The employees considered it useful because it provides overview and detailed information in one image and because it initiates discussion. The Tube Map Visualization is therefore helpful to complement traditional project plans of (1) <i>long-term projects</i> where (2) <i>different stakeholders</i> are involved. The theoretical framework, the model, and the findings have implications for researchers in the fields of Knowledge Management, Knowledge Visualization, Information Visualization, and Communication Sciences.'),
(7,'Current trends in collaborative knowledge management emphasize the importance of inter- and intra-organizational business process support. Enactment of business processes has primarily been a domain of workflow management systems. In this paper we propose a hybrid architecture for reconciliation of knowledge management and workflow management systems in order to support process participants in organizations, who are increasingly distributed and need to share and distribute knowledge artifacts. Today one pressing challenge is to utilize software as to create, share, and exchange (knowledge) work in collaborative knowledge activities across locations, while still being business process aware. This paper develops a conceptual framework, discusses a software architecture, and presents examples of a software system implementation for activity-based knowledge management for global project teams.'),
(8,'A well-functioning Knowledge Management is a competitive advantage for enterprises that act in co-operative and distributed networks with knowledge intensive production processes. A Knowledge Management approach that integrates both, hard factors (e.g. Information Technology) and soft factors (e.g., cultural aspects) for distributed and dynamic entrepreneurial (inter-organisational) networks is currently missing. This paper presents research findings of a project that is developing a methodology as well as an appropriate toolkit to support a service provider responsible for the KM within distributed entrepreneurial networks. The project integrates explicitly both new Information and Communication Technology driven organisational concepts, human-oriented approaches and existing KM methodologies and instruments.'),
(9,'Existing approaches in the area of knowledge-intensive processes focus on integrated knowledge and process management systems, the support of processes with KM systems, or the analysis of knowledge-intensive activities. For capturing knowledge-intensive business processes well known and established methods do not meet the requirements of a comprehensive and integrated approach of process-oriented knowledge management. These approaches are not able to visualise the decisions, actions and measures which are causing the sequence of the processes in an adequate manner. Parallel to conventional processes knowledge-intensive processes exist. These processes are based on conversions of knowledge within these processes. To fill these gaps in modelling knowledge-intensive business processes the Knowledge Modelling and Description Language (KMDL) got developed. The KMDL is able to represent the development, use, offer and demand of knowledge along business processes. Further it is possible to show the existing knowledge conversions which take place additionally to the normal business processes. The KMDL can be used to formalise knowledge-intensive processes with a focus on certain knowledge-specific characteristics and to identify process improvements in these processes. The KMDL modelling tool K-Modeler is introduced for a computer-aided modelling and analysing. The technical framework and the most important functionalities to support the analysis of the captured processes are introduced in the following contribution.'),
(10,'Knowledge technologies, the software products that support all aspects of knowledge processing and exchange, are the subject of permanent interest for software engineers at research organizations, as well as, for market analysts in commercial organizations. In order to clarify the role of knowledge management solutions in an enterprise business process, in this paper we survey the market of knowledge management solutions and analyze their functionalities from operational and strategic business perspective. Although knowledge flows are identified on an operational level, discussion will show that knowledge management solutions here serve to utilize the enterprise knowledge in an efficient performance of daily work. We argue that data and information collected on the operational level are processed by knowledge management solutions on a strategic level thus creating new knowledge that is used for strategic management of customers, suppliers and partners. This paper gives an insight into knowledge management market that can help the strategic planners to easily begin a knowledge management initiative.'),
(11,'During the last years, a large number of information and communication technologies (ICT) have been proposed to be supportive of knowledge management (KM). Several KM instruments have been developed and implemented in many organizations that require support by ICT. Recently, many of these technologies are bundled in the form of comprehensive, enterprise-wide knowledge infrastructures. The implementation of both, instruments and infrastructures, requires adequate modeling techniques that consider the specifics of modeling context in knowledge work. The paper studies knowledge work, KM instruments and knowledge infrastructures. Modeling techniques are reviewed, especially for business process management and activity theory. The concept of knowledge stance is discussed in order to relate functions from process models to actions from activity theory, thus detailing the context relevant for knowledge work.'),
(12,'Business Process Execution Business Process Execution typically represents technological knowledge infrastructures that leverage available formal, contextual information of business processes (from e.g. Workflow Management Systems) to provide relevant knowledge to process agents.  The paper _Process Oriented Knowledge Management: A Service Based Approach_ by Robert Woitsch and Dimitris Karagiannis introduces PROMOTE, a framework and a platform enabling and supporting the execution of knowledge intensive business processes. The authors utilize semantic technologies and vector functions in order to match knowledge management requirements and knowledge management services. An introduced concept for a service-based and process oriented architecture of an enterprise knowledge management system integrates the elements of the theoretical framework. _Reconciling Knowledge Management and Workflow Management Systems: The Activity-Based Knowledge Management Approach_ by Schahram Dustdar introduces _Caramba_, an activity based knowledge management approach. Based on a framework that classifies different knowledge management technologies, the author motivates the need for a consideration of activities as the central concept for the integration of knowledge and workflow management systems. The presented Caramba software demonstrates the viability of the introduced concepts. _Modeling and Implementing Pre-built Information Spaces. Architecture and Methods for Process Oriented Knowledge Management_ by Karsten Boehm, Wolf Engelbach, Joerg Haertwig, Martin Wilcken and Martin Delp presents _PREBIS_, an implementation of a business process oriented knowledge infrastructure that focuses on information provision for process agents based on contextual information about the user. The authors introduce the PREBIS modules, the kernel and a phase model that allow for the configuration of pre-built information spaces. An evaluation indicates the practical applicability of the PREBIS implementation. We hope that this special issue gives the reader an insightful overview of current concepts, methods and implementations in the emerging area of business process oriented knowledge infrastructures. The special track series BPOKI is continued in 2005 with new contributions introducing latest developments and research in the field. Graz, Austria March 2005  Markus Strohmaier Stefanie N. Lindstaedt Know-Center, Graz, Austria  Page 428'),
(13,'This paper explores a process view of call-centres and the knowledge infrastructures that support these processes. As call-centres grow and become more complex in their function and organisation so do the knowledge infrastructures required to support their size and complexity. This study suggests a knowledge-based hierarchy of advice-type call-centres and discusses associated knowledge management strategies for different sized centres. It introduces a Knowledge Infrastructure Hierarchy model, with which it is possible to analyze and classify call-centre knowledge infrastructures. The model also demonstrates different types of interventions supporting knowledge management in call-centres. Finally the paper discusses the possibilities of applying traditional maturity model approaches in this context.'),
(14,'This paper introduces a new viewpoint in knowledge management by introducing KM-Services as a basic concept for Knowledge Management. This text discusses the vision of service oriented knowledge management (KM) as a realisation approach of process oriented knowledge management. In the following process oriented knowledge management as it was defined in the EU-project PROMOTE (IST-1999-11658) is presented and the KM-Service approach to realise process oriented knowledge management is explained. The last part is concerned with an implementation scenario that uses Web-technology to realise a service framework for a KM-system.'),
(15,'This paper contains a completely formal (and mechanically proved) development of some algorithms dealing with a linked list supposed to be shared by various processes. These algorithms are executed in a highly concurrent fashion by an unknown number of such independent processes. These algorithms have been first presented in [MS96] by M.M. Michael and M.L. Scott. Two other developments of the same algorithms have been proposed recently in [YS03] (using the 3VMC Model Checker developed by E. Yahav) and in [DGLM04] (using I/O Automata and PVS).'),
(16,''),
(17,'Using the fiction of atomicity as a design abstraction and then refining atomicity as we develop an implementation is widely used in areas of concurrent computing such as database systems and transaction processing. In each of these and similar areas, associated notions of correctness are used in order to show that a particular implementation artefact which exhibits concurrency is correct in some sense with respect to a (possibly notional) description which executes with a greater degree of sequentiality. Of crucial importance in the proof and deployment of such notions of correctness is the issue of observability: i.e. in what broad sense do (human or computer) users of a particular implementation artefact observe the effects of its executions. For example, if a human user is allowed to observe directly the execution of a particular concurrent component then he or she will be able to detect the fact of concurrent - and so non-atomic - execution. In general, however, the notion of observability is treated implicitly or not at all. In this paper, we make it explicit and look at the issue of exploring more fully the connections between atomicity and observability. The ultimate aim of this consideration is to work towards constructing a more general framework for (software or hardware) development by refining atomicity.'),
(18,'In an ideal world, where we could guarantee instantaneous, atomic data transfer - whatever the type of the data being transferred - shared memory communication between two concurrent processes could be implemented directly using single variables or registers, without any attendant access control policies or mechanisms. In practice, asynchronous communication mechanisms may be used to provide the illusion of atomic transfers of data while still allowing non-blocking reads and writes: that is, reads and writes may proceed concurrently without interfering with each other. In order to prove the correctness of such mechanisms, the natural approach would be to verify them against the specification provided by an idealised register with atomic, instantaneous - and so sequential - transfers of data. Yet such a verification is complicated by the fact that, in moving to the asynchronous communication mechanism from such a specification, additional concurrency has been introduced and so the (visible) behaviours of the mechanism are not directly comparable to those of the register. In this paper, we recall an extension of standard process algebraic refinement and show how it may be used to verify the correctness of a particular asynchronous communication mechanism, Simpson s 4-slot. In so doing, we look at a number of issues which seem significant in the consideration of correctness when the real atomicity of a specification has been relaxed in the move from specification to implementation.'),
(19,'We describe the StAC language which can be used to specify the orchestration of activities in long running business transactions. Long running business transactions use compensation to cope with exceptions. StAC supports sequential and parallel behaviour as well as exception and compensation handling. We also show how the B notation may be combined with StAC to specify the data aspects of transactions. The combination of StAC and B provides a rich formal notation which allows for succinct and precise specification of business transactions. BPEL is an industry standard language for specifying business transactions and includes compensation constructs. We show how a substantial subset of BPEL can be mapped to StAC thus demonstrating the expressiveness of StAC and providing a formal semantics for BPEL.'),
(20,'Quorum systems (introduced in the late seventies)  and atomic broadcast (introduced later) are two techniques to manage replicated data. Despite of the fact that these two techniques are now well known, the advantage of atomic broadcast over quorum systems is not clearly understood. The paper explains exactly in what cases atomic broadcast is a better technique than quorum systems to handle replication.'),
(21,'This paper is a manifesto for future research on \"atomicity\" in its many guises and is based on a five-day workshop on \"Atomicity in System Design and Execution\" that took place in Schloss Dagstuhl in Germany in April 2004.      <hr align=\"left\" width=\"50%\"> <br><br>      Additional Authors:      Dagstuhl Seminar, (Organizer Authors)       <br><br>     and Alan Fekete, Marie-Claude Gaudel, Henry F. Korth, Rogerio de Lemos, Eliot Moss, Ravi Rajwar, Krithi   Ramamritham, Brian Randell, Luis Rodrigues, Dagstuhl Seminar, (Participant Authors).'),
(22,'This paper shows how the concept of atomicity can ease the development of concurrent software. It illustrates by means of a case study how atomicity is used to reduce the complexity of concurrency by presenting simplified models or views of the system at certain stages of the development cycle. As the development process goes on, the atomic views from the early stages are refined - broken up into smaller pieces - to slowly introduce concurrency back into the system. Finally, at the design stage, low-level concepts that provide atomicity, such as transaction or monitors, are used to ensure consistent concurrent updating of the application state.'),
(23,'We argue that atomicity, i.e., atomic actions with most of the traditional \"ACID\" properties, namely atomicity, consistency, and isolation but perhaps not durability, should be provided as a fundamental first class resource in computer systems. This implies coherent, convenient, and well-engineered support from the hardware, through the run-time system, programming language, and libraries, to the operating system. We articulate the advantages of this approach, indicate what has already been accomplished, and outline what remains to be done to realize the vision.'),
(24,'Visual mining methods enable the direct   integration of the user to overcome major problems of automatic data   mining methods, e.g., the presentation of uninteresting results,   lack of acceptance of the discovered findings, or limited confidence   in these. We present a novel subgroup mining approach for   explorative and descriptive data mining implemented in the VIKAMINE   system. We propose several integrated visualization methods to   support subgroup mining. Furthermore, we describe three case studies   using data from fielded systems in the medical domain.'),
(25,'In contrast with centralized recommender   systems, social recommendation algorithm is applied to the item   rating data on social networks. Meaningful recommendation can be   uncovered by the topology of social network as well as the   similarity between users. More importantly, this information becomes   propagated into the users in the estimated same groups. As the goal   of this paper, we propose a novel method for visual explanation of   the recommender system on social network. For experiments, we   simulate the recommendation flow by using the   <I>MovieLens</I> dataset on a social network constructed   with FOAF.'),
(26,'Computers are still much less useful than the   ability of the human eye for pattern matching. This ability can be   used quite straightforwardly to identify structure in a data set   when it is two or three dimensional. With data sets with more than 3   dimensions some kind of transformation is always necessary. In this   paper we review in depth and present and extension of one of these   mechanisms: Andrews\' curves. With the Andrews\' curves we use a curve   to represent each data point. A human can run his eye along a set of   curves (representing the members of the data set) and identify   particular regions of the curves which are optimal for identifying   clusters in the data set. Of interest in this context, is our   extension in which a moving three-dimensional image is created in   which we can see clouds of data points moving as we move along the   curves; in a very real sense, the data which dance together are   members of the same cluster.'),
(27,'Most visualization tools introduced in the   literature are specialized for a particular task. In this work, we   introduce a novel framework which allows visualization to take place   in the background of normal day to day operations of any GUI based   operating system such as MS Windows, OS X or Linux. Our system works   by replacing the standard file icons with automatically generated   icons that reflect the contents of the files in a principled way. We   call such icons Intelligent Icons. While there is little utility in   examining an individual icon, examining groups of them provides a   greater possibility of unexpected and serendipitous discoveries. The   utility of Intelligent Icons can be further enhanced by arranging   them on the screen in a way that reflects their   similarity/differences. We demonstrate the utility of our approach   on data as diverse as DNA, text files, electrocardiograms, and Space   Shuttle telemetry. In addition we show that our system is unique in   also supporting fast and intuitive similarity search.'),
(28,'Visualization has become an essential support   throughout the KDD process in order to extract hidden information   from huge amount of data. Visual data exploration techniques provide   the user with graphic views or metaphors that represent potential   patterns and data relationships. However, an only image does not   always convey high-dimensional data properties successfully. From   such data sets, visualization techniques have to deal with the curse   of dimensionality in a critical way, as the number of examples may   be very small with respect to the number of attributes. In this   work, we describe a visual exploration technique that automatically   extracts relevant attributes and displays their ranges of interest   in order to support two data mining tasks: classification and   feature selection. Through di#erent metaphors with dynamic   properties, the user can re-explore meaningful intervals belonging   to the most relevant attributes, building decision rules and   increasing the model accuracy interactively.'),
(29,'The data mining community is focused on a   variety of methods and algorithms to manipulate incompletely   specified or uncertain data and their dependencies. The major   obstacle in the representation and visualization of incompletely   specified data is the size explosion problem through defining   undefined or uncertain values, which commonly raises questions about   suggested heuristics and their practical applicability. Recently,   there is a renewed interest in resolving the size explosion problem   for incompletely specified and uncertain data based on symbolic   techniques. One of such techniques, decision diagram, has been   successfully applied to many knowledge visualization and data   manipulation problems.'),
(30,'Tracking and comparing psychotherapeutic data   derived from questionnaires involves a number of highly structured,   time-oriented parameters. Descriptive and other statistical methods   are only suited for partial analysis. Therefore, we created a novel   spring-based interactive Information Visualization method for   analysing these data more in-depth. With our method the user is able   to find new predictors for a positive or negative course of the   therapy due to the combination of various visualization and   interaction methods.'),
(31,'During the last decade Visual Exploration and Visual Data Mining techniques have proven to be of high value in exploratory data analysis since they combine human visual perception and recognition capabilities with the enormous storage capacity and the computational power of today\'s computer systems in order to detect patterns and trends in the data. But the ever increasing mass of information leads to new challenges on visualization techniques and concepts. Due to technological progress in computer power and storage capacity today\'s scientific and commercial applications are capable of generating, storing and processing massive amounts of data. Most existing visualization metaphors and concepts do not scale well on such large data sets as interaction capabilities and visual representations suffer from the massive number of data points. To bridge this gap, Visual Analytics aim to incorporate more intelligent means than to just retrieve and display the data items to filter the relevant from the non-relevant data. In this context the paper introduces a new approach based on a Multiresolution paradigm to increase the scalability of existing Visual data exploration techniques. The basic idea is to provide relevance driven compact representations of the underlying data set that present the data at different granularities. In the visualization step the available display space is then distributed according to the data granularity, to emphasize relevant information. The paper aims at introducing a technical base of Multiresolution visualization and provides an application example that shows the usefulness of the proposed approach.'),
(32,''),
(33,''),
(34,'Wireless networks, specifically IEEE 802.11, are inexpensive  and easy to deploy, but their signals can be detected by eavesdroppers at great distances. Even with existing and new security measures, wireless networks have a higher risk than wired nets. WIDS, Wireless Intrusion Detection System, provides an additional layer of security by combining intrusion detection with physical location determination, using directional antennas. We briefly describe WIDS and present our initial results of remote station location using inexpensive hardware.'),
(35,'In a world made of interconnected systems which  manage huge amounts of confidential and shared data, security plays a significant role. Policies are the means by which security rules are defined and enforced. The ability to evaluate policies is becoming more and more relevant, especially when referred to the cooperation of services belonging to untrusted domains. We have focused our attention on Public Key Infrastructures (PKIs), at the state of the art security policies evaluation is manually performed by technical and organizational people coming from the domains that need to interoperate. However, policy evaluation must face uncertainties derived from different perspectives, verbal judgments and lack of information. Fuzzy techniques and uncertainty reasoning can provide a meaningful way for dealing with these issues. In this paper we propose a fuzzy technique to characterize a policy and to define a Reference Evaluation Model representing different security levels against which we are able to evaluate and compare policies. The comparison takes into account not only minimal system needs but evaluator s severity, too, furthermore it gives clear information regarding policy weakness that could be used to help security administrators to better enforce rules. Finally we present a case study which evaluates the security level of a legally recognized policy.'),
(36,'In this paper, we present a novel high bit rate  LSB audio watermark ing method that reduces embedding distortion of the host audio. Using the proposed twostep algorithm, watermark bits are embedded into higher LSB layers, resulting in increased robustness against noise addition. In addition, listening tests showed that perceptual quality of watermarked audio is higher in the case of the proposed method than in the standard LSB method.'),
(37,'Intrusion Detection Systems (IDS) are responsible  for monitoring and analyzing host or network activity to detect intrusions in order to protect information from unauthorized access or manipulation. There are two main approaches for intrusion detection: signature-based and anomaly-based. Signature_based detection employs pattern matching to match attack signatures with observed data making it ideal for detecting known attacks. However, it cannot detect unknown attacks for which there is no signature available. Anomaly-based detection uses machine-learning techniques to create a profile of normal system behavior and uses this profile to detect deviations from the normal behavior. Although this technique is effective in detecting unknown attacks, it has a drawback of a high false alarm rate. In this paper, we describe our anomaly_based IDS designed for detecting malicious use of cryptographic and application-level protocols. Our system has several unique characteristics and benefits, such as the ability to monitor cryptographic protocols and application-level protocols embedded in encrypted sessions, a very lightweight monitoring process, and the ability to react to protocol misuse by modifying protocol response directly.'),
(38,'Existing ad hoc routing protocols are either unicast or multicast. In this paper we propose a simple extension to the Dynamic Source Routing Protocol (DSR) to cater for group communications where all node addresses are unicast addresses and there is no single multicast address. The proposed sliding window protocol for multiple communications results in significant improvement in total packet delivery. Due to the high frequency of mobility, attrition and reinforcement in ad hoc networks, in order to preserve confidentiality, it becomes necessary to rekey each time a member enters or leaves a logically defined group. We compare our group rekeying rate on sliding window protocol versus other kinds of Rekeying algorithms. The proposed sliding window protocol performs better. The proposed sliding window is therefore simple and improves both communications and security performance.'),
(39,'Most cryptography systems are based on the modular  exponentiation to perform the non-linear scrambling operation of data. It is performed using successive modular multiplications, which are time consuming for large operands. Accelerating cryptography needs  optimising the time consumed by a single modular multiplication and/or reducing the total number of modular multiplications performed. Using a genetic algorithm, we first yield the minimal sequence of powers, generally called addition chain, that need to be computed to finally obtain the modular exponentiation result. Then, we exploit the co-design methodology to engineer a cryptographic device that accelerates the encryption/decryption throughput without requiring considerable hardware area. Moreover the obtained designed cryptographic hardware is completely secure against known attacks.'),
(40,'Delivering electronic goods over the Internet is  one of the e-commerce applications that will proliferate in the  coming years. Certified e-goods delivery is a process where valuable  e-goods are exchanged for an acknowledgement of their reception. This  paper proposes an efficient security protocol for certified e-goods  delivery with the following features: (1) it ensures strong fairness  for the exchange of e-goods and proof of reception, (2) it ensures  non-repudiation of origin and non-repudiation of receipt for the  delivered e-goods, (3) it all ows the receiver of e-goods to verify,  during the exchange process, that the e-goods to be received are the  one he is signing the receipt for, (4) it uses an off-line and  transparent semi-trusted third party (STTP) only in cases when disput  es arise, (5) it provides the confidentiality protection for the  exchanged items from the STTP, and (6) achieves these features with  less computational and communicational overheads than related  protocols.'),
(41,'Information quality assurance under the existence  of uncertainty can be investigated in the context of soft security,  where an agent maintains trustworthiness evaluations of its  information sources to assist in the evaluation of incoming  information quality from those sources. Since dependency inherently  exists in a system where agents do not have self-sufficient sensing  or data collection capabilities, finding an appropriate set of  information sources is important for assuring the quality of  information and for increasing the agent\'s goal achievement. This  research proposes an approach for selecting information sources as  partners. In order to increase the efficiency and the accuracy, we  use trustworthiness, information cost and goal coverage as the  metrics for information valuation while adopting a lazy exploration  of information sources combination space. Experimental results show  that the proposed approach increases the efficiency and results in  quality information acquisition.'),
(42,'An important problem in digital forensics is to record  a checkpoint of a disk drive mounted as a file system on a host machine without disrupting the disk s normal operations. We present a checkpointing methodology for a disk that has a Unix-like file system. While our algorithm is built around the Unix file system, it can be used to checkpoint disks formatted for other file systems such as NTFS, etc. Our algorithm satisfies several correctness conditions.'),
(43,'This paper presents a novel encryption-less  algorithm to enhance security in transmission of data in networks. The algorithm uses an intuitively simple idea of a jigsaw puzzle to break the transformed data into multiple parts where these parts form the pieces of the puzzle. Then these parts are packaged into packets and sent to the receiver. A secure and efficient mechanism is provided to convey the information that is necessary for obtaining the original data at the receiver-end from its parts in the packets, that is, for solving the jigsaw puzzle. The algorithm is designed to provide information-theoretic (that is, unconditional) security by the use of a one-time pad like scheme so that no intermediate or unintended node can obtain the entire data. A parallelizable design has been adopted for the implementation. An authentication code is also used to ensure authenticity of every packet.'),
(44,'This work presents two new construction techniques for <i>q</i>-ary Gossip codes from <i>t</i>-designs and Traceability schemes. These Gossip codes achieve the shortest code length specified in terms of code parameters and can withstand erasures in digital fingerprinting applications. This work presents the construction of embedded Gossip codes for extending an existing Gossip code into a bigger code. It discusses the construction of concatenated codes and realisation of erasure model through concatenated codes.'),
(45,'There is considered a problem of information  profiles and information resources collection forming in distributed data- and/or knowledge bases as a result of an attempt to satisfy the information requirements of the customers represented by their information profiles. It is shown that the interests of managers of data- and knowledge bases are not fully convergent and that they participate in a composite, partially co-operative, partially non-co-operative <i>n</i>-persons games. There is given a formal description of the strategies used in such games, as well as the methods of decision making of the players on the level of open access <i>(OADB)</i> as well as on local <i>(LDB)</i> databases one.'),
(46,'An application of the method suitable for modelling and control of general discrete event dynamic systems (DEDS) to special kinds of communication systems is presented in this paper. The approach is based on Petri nets (PN) defined in [Peterson 1981] and directed graphs (DG) described e.g. in [Diestel 1997]. It is supported by the previous author s works, especially [Capkovic 2003], [Capkovic 1998], [Tzafestas and Capkovic 1997].'),
(47,'Contemporary distributed software systems, exposed  to highly unpredictable environments, are reaching extremely high complexity levels. For example, open heterogeneous multi-agent systems that may potentially be spread all around the globe are interacting with different types of dynamically changing web-services and web-technologies. Traditional control-based handling of adaptability may not be suitable any more in such systems. Therefore there is a tendency for exploring different adaptability models inspired by biological phenomena. Biological systems inherently are faced with complexity and unpredictable environments, and they exhibit high levels of ad aptability. In this article, we present a theoretical model of development of complex system, which was built originally by Andrzej Gecow, as a computational model in evolutionary biology. This model represents a generic complex system subjected to long sequences of adaptive changes. The model was used for analysis of development processes and also structural tendencies. By tendencies we mean some phenomena that should be expected in any complex system, subjected to a long development process. Some of these tendencies are not desirable, for example bloat of the system. Some of the phenomena, however, show characteristics of changes that improve the system. These characteristics can be applied to optimisation of self-producing and self-adapting algorithms of self-maintaining complex software systems. The main structural tendencies described in this article are: terminal modifications, terminal majority of additions, and covering (reconstructing within the system itself disappearing environmental signals).'),
(48,'With the exponentially increasing amount of information available on the World Wide Web, users have b een getting more difficult to seek relevant information. Several studies have been conducted on the concept of adaptive approaches, in which the user s personal interests are taken into account. In this paper, we propose a user-support mechanism based on the sharing of knowledge with other users through the collaborative Web browsing, focusing specifically on the user s interests extracted from his or her own bookmarks. Simple URL based boo kmarks are endowed with semantic and structural information through the conceptualization based on ontology. In order to deal with the dynamic usage of bookmarks, ontology learning based on a hierarchical clustering method can be exploited. This system is composed of a facilitator agent and multiple personal agents. In experiments conducted with this system, it was found that approximately 53.1% of the total time was saved during collaborat ive browsing for the purpose of seeking the equivalent set of information, as compared with normal personal Web browsing.'),
(49,'An original approach to modelling internal structure of artificial cognitive agents and the phenomenon of language grounding is presented. The accepted model for the internal cognitive space reflects basic structural properties of human cognition and assumes the partition of cognitive phenomena into conscious and non-conscious. The language is treated as a set of semiotic symbols and is used in semantic communication. Semiotic symbols are related to the internal content of empirical knowledge bases in which they are grounded. This relation is given by the so-called epistemic satisfaction relations defining situations in which semiotic symbols are adequate (grounded) representations of embodied experience. The importance of non-conscio us embodied knowledge in language grounding and production is accepted. An example of application of the proposed approach to the analysis of grounding requirements is given for the case of logic equivalences extended with modal operators of possibility, belief and knowledge. Implementation issues are considered.'),
(50,'The paper describes RankFeed a new adaptive method of recommendation that benefits from similarities between searching and recommendation. Concepts such as: the initial ranking, the positive and negative feedback widely used in searching are applied to recommendation in order to enhance its coverage, maintaining high accuracy. There are four principal factors that determine the method s behaviour: the quality document ranking, navigation patterns, textual similarity and the list of recommended pages that have been ignored during the navigation. In the evaluation part, the local site s behaviour of the RankFeed ranking is contrasted with PageRank. Additionally, recommendation behaviour of RankFeed versus other classical approaches is evaluated.'),
(51,''),
(52,'Inconsistency of knowledge may appear in many situations, especially in distributed environments in which autonomous programs operate. Inconsistency may lead to conflicts, for which the resolution is necessary for correct functioning of an intelligent system. Inconsistency of knowledge in general means a situation in which some autonomous programs (like agents) generate different versions (or states) of knowledge on the same subject referring to a real world. In this paper we propose two logical structures for representing inconsistent knowledge: conjunction and disjunction. For each of them we define the semantics and formulate the consensus problem, the solution of which would resolve the inconsistency. Next, we work out algorithms for consensus determination. Consensus methodology has been proved to be useful in solving conflicts and should be also effective for knowledge inconsistency resolution.'),
(53,'Consensus methods proved to be very effective in solving problems in many areas. In this paper a hybrid adaptation of web-based system user interfaces that us es consensus methods is presented. The hybrid recommendation is a combination of the following methods: demographic, content-based, and collaborative. Each of this meth od has its specific advantages and disadvantages. The hybrid adaptation enables over coming disadvantages of each separate solution.'),
(54,'The paper is concerned with the effective and efficient processing of spatiotemporal selection queries under varying degrees of approximation. Such queries may employ operators like <i>overlaps, north, during,</i> etc., and their result is a set of entities standing approximately in some spatiotemporal relation <img src=\"a_provably_efficient_computational/images/img1.gif\"> with respect to a query object X. The contribution of the present work is twofold: i) it presents a formal mathematical framework for representing multidimensional relations at varying granularity levels, modelling relation approximation through the concept of relation convexity, ii) it subsequently exploits the proposed framework for developing approximate spatiotemporal retrieval mechanisms, combining a set of existing as well as new main memory and secondary memory data structures that achieve either optimal or the best known performance in terms of time and space complexity, for both the static and the dynamic setting.'),
(55,'Actor programs give rise to computation structures that evolve dynamically and unpredictably both in shape and size. Therefore, their execution times cannot be statically determined. This paper describes an approach to the problem of estimating time costs of actor programs. The approach takes into account the constraints imposed both by the semantics and implementation of the model. In particular, implementation constraints can be captured and exploited to drastically reduce the number of computations generable by the program, thus simplifying the execution time evaluation. Moreover, execution times are expressed in a parametric form by using a variant of the <i>LogP</i> model able to synthetically characterize the target hardware/software platform.'),
(56,'Recently, some problems related to the use of the Real-time Control Protocol (RTCP) in very large dynamic group have arisen. Some of these problems are: feedback delay, increasing storage state at every member, and ineffective RTCP bandwidth usage, especially for the receivers that obtain incoming RTCP reports through low bandwidth links. More schemes are proposed to alleviate the RTCP problems. The famous and recent one, which was introduced by EL-Marakby and was named Scalable RTCP (S-RTCP), still has several drawbacks. This paper will evaluate the previous model by introducing all its drawbacks. Consequently, we will demonstrate a design of More Scalable RTCP (MS-RTCP) scheme based on hierarchical structure, distributed management, and EL-Marakby scheme. Also, we will show how our scheme will alleviate all the drawbacks found in the S-RTCP. Finally, we will introduce our scheme implementation to analyze and evaluate its performance.'),
(57,'One of the major difficulties in software testing is the automatic generation of test data that satisfy a given adequacy criterion. This paper presents an automatic test data generation technique that uses a genetic algorithm (GA), which is guided by the data flow dependencies in the program, to search for test data to cover its def-use associations. The GA conducts its search by constructing new test data from previously generated test data that are evaluated as effective test data. The approach can be used in test data generation for programs with/without loops and procedures. The proposed GA accepts as input an instrumented version of the program to be tested, the list of def-use associations to be covered, the number of input variables, and the domain and precision of each input variable. The algorithm produces a set of test cases, the set of def-use associations covered by each test case, and a list of uncovered def-use associations, if any. In the parent selection process, the GA uses one of two methods: the roulette wheel method or a proposed method, called the random selection method, according to the user choice. Finally, the paper presents the results of the experiments that have been carried out to evaluate the effectiveness of the proposed GA compared to the random testing technique, and to compare the proposed random selection method to the roulette wheel method.'),
(58,'Routing space estimation plays a crucial role in design automation of digital systems. We investigate the problem of estimating upper bounds for global routing of two-terminal nets in two-dimensional arrays. We show the soundness of the bounds for both wiring space and total wire-length estimation.'),
(59,'Transclusions are a technique for virtually including existing content into new documents by reference to the original documents rather than by copying. In principle, transclusions are used in HTML for the inclusion of entire text documents, images, movies and similar media. The HTML specification only takes transclusions of entire documents into account, though. Hence it is not possible, for instance, to include a part of an existing image into an HTML document. In this paper, fine-grained transclusion of multimedia documents on the Web are proposed, which presents a logical realisation of the concept of transclusions in HTML. The proposal makes it possible, for instance, to include sections of existing images or small portions of entire movies into HTML documents. Two different approaches to implementing the functionality presented are detailed. The first architecture is based on a transparent extension module to conventional HTTP servers, whereas the alternative design makes use of a CGI program. Both approaches are fully self-contained, reside on an HTTP server and do not require browser plug-ins or any other special software components to be installed on client computers. An amendment to the HTTP specification is not required either. A prototype implementation demonstrates the proposal for a number of document types.'),
(60,'A recent renewed interest in hypercube interconnection network has been concentrated to the more scalable version known as a fat cube. The paper introduces several router models for fat nodes and uses them for cost comparison of both the hypercube and fat cube topologies. Analysis of time complexity of collective communications is done next and lower bounds on the number of communication steps are derived. Examples of particular communication algorithms on the 2D-fat cube topology with 8 processors are summarized and described in detail. The performed study shows that a large variety of fat cubes can provide much desired flexibility, trading cost for performance and manufacturability.'),
(61,'In this paper, we present a novel framework TESTAF to support automatic generation and execution of test cases using object-oriented formal specifications. We use IFAD VDM++ as the specification language, but the ideas presented can be applied equally well to other object-oriented formal notations. The TESTAF framework requires a VDM++ specification for a class, a corresponding implementation in C++, and a test specification, to generate and execute test cases, and evaluate the results. The test specification defines valid test sequences in an intermediate specification language based on regular expressions. The framework uses the formal specification of the class, and the test specification to generate empty test shells, which are then filled in with the test data to create concrete test cases. The test data for a method are generated from the input space defined by the method pre condition and the class invariant. The TESTAF applies boundary value analysis strategy to generate the test data. A test driver then executes the implementation with the test data, and uses a conjunction of method post condition and the class invariant as a test oracle to evaluate the results, while reporting failed test cases to the user.'),
(62,'We propose a variant of the Paillier cryptosystem that improves efficiency in encryption, re-encryption and decryption while preserving the homomorphic property. We then use this variant to construct a new verifiable shuffle system and prove its security. We show that the new shuffle scheme has the least number of rounds and exponentiations compared to all known shuffle schemes. Finally, we show how to construct a publicly verifiable mix-net using the shuffle system.'),
(63,'In the last few years, an overwhelming amount of agent-based systems for supporting business-to-customer (B2C) e-commerce activities have been proposed. In this context, the use of agent ontologies for modelling the realities of both customers and sellers may play an important role. This paper deals with a formal model of agent ontologies, capable of describing the entities involved in the above realities (products, product features, product categories) as well as the behaviour of customers and sellers in performing their activities. Furthermore, we present some techniques that exploit the proposed ontology model for supporting the various B2C e-commerce stages represented in the Consumer Buying Behaviour (CBB) model. Finally, we briefly describe the OBA_B2C multi-agent architecture that implements in a JADE-based environment all the proposed techniques.'),
(64,'We obtain a finite binary tree algorithm to extend the domain of a universal one-way hash function (UOWHF). The associated key length expansion is only a constant number of bits more than the minimum possible. Our finite binary tree algorithm is a practical parallel algorithm to securely extend the domain of a UOWHF. Also the speed-up obtained by our algorithm is approximately proportional to the number of processors.'),
(65,'Actor-based modeling, with encapsulated active objects which communicate asynchronously, is generally recognized to be well-suited for representing concurrent and distributed systems. In this paper we discuss the actor-based language Rebeca which is based on a formal operational interpretation of the actor model. Its Java-like syntax and object-based style of modeling makes it easy to use for software engineers, and its independent objects as units of concurrency leads to natural abstraction techniques necessary for model checking. We present a front-end tool for translating Rebeca to the languages of existing model checkers in order to model check Rebeca models. Automated modular verification and abstraction techniques are supported by the tool.'),
(66,'<i>T-decomposition</i> was first proposed and implemented as an algorithm by Mark Titchener. It has applications in communication of code sets and in the fields of entropy and similarity measurement. The first implementation of a T-decomposition algorithm by Titchener was subsequently followed by a faster version named <tt>tcalc</tt>, developed in conjunction with Scott Wackrow. An improved T-decomposition algorithm was published in 2003 by the authors with the implementation <tt>tlist</tt>. This paper introduces a new algorithm that builds on our 2003 algorithm. Comparative experimental results are given to show that the new version has a significantly better time performance than previous algorithms.'),
(67,'Surface reconstruction addresses the problem of creating a surface model from a point set digitized from a physical object. After performing the surface fitting on the bases of individual patches (adjacent surfaces), it is necessary to improve the obtained results by connecting the adjacent surfaces according to the desired smoothness. This paper presents a fast method for collecting the adjacent surfaces into complex based on C0 continuity. The method works with the surfaces and their segments. Firstly, an arbitrary surface is selected, and the points with closest distances to that surface are extracted. Then, the points are sorted according to the Euclidean distance to the surface. Finally, the surface and the sorted points are joined together and presented to a refitting technique. This technique includes a procedure to decide if the data is similar to the current surface and for updating the surface parameters for each new point.'),
(68,''),
(69,'In this paper we investigate polyline grid drawing of free trees on <i>2D</i> grids which are bounded by simple polygons. We focus on achieving uniform node distribution while we also try to achieve minimum edge crossings. We do not consider achieving symmetry as a mandatory task, but our algorithm can exploit some symmetries present in both the given trees and the given polygons. To our knowledge, our work is the first attempt for developing algorithms that draw graphs on regions which are bounded by simple polygons.'),
(70,''),
(71,'Most approaches to formal semantics are based on   the assumption that all the constructs of a language are defined   together. The details of the definition of each construct can (and   usually do) depend on which other constructs are included in the   given language. This limits reuse of definitions of common   constructs.  <br><br>   With the more constructive approach proposed here, the   semantics of each basic abstract programming construct is defined   separately and independently. The semantics of a full language is   obtained by translating its constructs into the basic abstract   constructs, whose definitions are thus reused verbatim.     <br><br>The   frameworks of Modular SOS and Action Semantics can both be used in   conjunction with the proposed approach. Some illustrations are   given.'),
(72,'This paper aims to explain why I am still   fascinated by the use of functional languages in hardware design. I   hope that some readers will be tempted to tackle some of the hard   problems that I outline in the final section. In particular, I   believe that programming language researchers have much to   contribute to the field of hardware design.'),
(73,'We discuss the main novelties of the   implementation of Lua 5.0: its register-based virtual machine, the   new algorithm for optimizing tables used as arrays, the   implementation of closures, and the addition of   coroutines.'),
(74,'In this paper we describe AspectLua - a dynamic aspect-oriented language based on Lua. It relies on a meta-object protocol, LuaMOP, which unifies the introspective and reflective mechanisms provided by Lua and handles the weaving process. In order to improve support for dynamicity, AspectLua allows the association of aspects with undeclared elements of the application code (virtual join points). In addition, it provides an automatic support for managing aspects execution order.'),
(75,'Automatic finalization is a common but   inherently complex language facility that makes the garbage   collection process semantically visible to client programs.  With   finalizers, memory management becomes more flexible, and garbage   collectors can be used to recycle other resources in addition to   memory.   <br><br>   Formal language models usually ignore   garbage collection, and therefore are unable to properly describe   finalization. In this paper we use an operational approach to   develop a new abstract model that explicitly represents memory   management actions in a garbagecollected programming language based   on the calculus. We formally state and prove several important   properties related to memory management, and employ the model to   describe and explore a semantics for finalizers.'),
(76,'Recent proposals in the domain of interface description languages for web services stress the importance of specifying the dynamic, behavioral aspects of the services. The goal of this paper is to introduce a new interface description language, called PEWS, that uses predicate path expressions to define web service behaviours.  Our proposal represents a simple but expressive way to describe order and conditional constraints over web service operations. PEWS aims to be used not only to the specification of simple web services but also to be a tool for describing service composition.  <br><br>  In this paper, we use the Action Semantics framework to present the syntax and semantics of the most significant parts of PEWS and we introduce XPEWS, the XML-based version of PEWS used to publish service behaviours for future searches and composition. The definition of XPEWS is done by giving the XML Schema that defines the syntax of XPEWS programs.'),
(77,'In this work, we propose a compilation strategy for nonstrict functional languages targeting the Microsoft .NET Platform, a multilanguage platform which provides a large number of services to aid current software development. This strategy is based on the push/enter execution model, enables fast function calling mechanisms whenever possible and males use of new features present in .NET Framework, such as delegates and tail calls. Our case study was the compilation of the Haskell language, a standardized and well known nonstrict functional language. Our main contribution is the construction of an environment for the testing of different compilation techniques for functional languages targeting .NET.'),
(78,'The .NET Common Language Runtime (CLR) aims to provide interoperability among code written in several different languages, but porting scripting languages to it, so that scripts can run natively, has been hard. This paper presents our approach for running scripts written in Lua, a scripting language, on the .NET CLR.<br><br>  Previous approaches for running scripting languages on the CLR have focused on extending the CLR, statically generating CLR classes from user-defined types in the source languages. They required either language extensions or restrictions on the languages\' dynamic features.  <br><br>  Our approach, on the other hand, focused on keeping the syntax and semantics of the original language intact, while giving the ability to manipulate CLR objects. We implemented a translator of Lua virtual machine bytecodes to CLR bytecodes. Benchmarks show that the code our translator generates performs better than the code generated by compilers that use the previous approaches.'),
(79,'Interpreted languages are widely used due to   ease to use, portability, and safety. On the other hand,   interpretation imposes a significance overhead. Justin Time (JIT)   compilation is a popular approach to improving the runtime   performance of languages such as Java. We compare the performance of   a JIT compiler with a traditional compiler and with an emulator. We   show that the compilation overhead from using JIT is negligible, and   that the JIT compiler achieves better overall performance,   suggesting the case for aggresive compilation in JIT   compilers.'),
(80,'We propose a novel discipline for programming   stream functions and for the semantic description of stream   manipulation languages based on the observation that both general   and causal stream functions can be characterized as coKleisli arrows   of comonads. This seems to be a promising application for the old,   but very little exploited idea that if monads abstract notions of   computation of a value, comonads ought to be useable as an   abstraction of notions of value in a context. We also show that   causal partial-stream functions can be described in terms of a   combination of a comonad and a monad.'),
(81,'This work describes the formal semantics of   Scheme 3 as an equational theory in the Maude rewriting system. The   semantics is based on continuations and is highly modular. We   briefly investigate the relationship between our methodology for   defining programming languages and other semantic formalisms. We   conclude by showing some performance results of the interpreter   obtained for free from the executable specification.'),
(82,'We provide a complete description of   <i>m</i>Haskell, a new mobile programming language that   extends the Haskell functional language. We describe new stateful   mobility primitives that use higher-order channels, giving their   operational semantics and an implementation outline. We show how   medium-level coordination abstractions can be constructed using   monadic composition of the mobility primitives. We briefly outline   how high-level mobile coordination abstractions, or   <i>mobility skeletons</i>, can be defined using the   lower-level abstractions. The use of all three abstractions is   demonstrated with examples and a new case study: a distributed   stateless web server where a thread farm skeleton is used to   distribute work to remote locations.'),
(83,'This paper presents a system for induction of forest of functional trees from data streams able to detect concept drift. The Ultra Fast Forest of Trees (UFFT) is an incremental algorithm, which works online, processing each example in constant time, and performing a single scan over the training examples. It uses analytical techniques to choose the splitting criteria, and the information gain to estimate the merit of each possible splitting-test. For multi-class problems the algorithm builds a binary tree for each possible pair of classes, leading to a forest of trees. Decision nodes and leaves contain naive-Bayes classifiers playing different roles during the induction process. Naive-Bayes in leaves are used to classify test examples. Naive-Bayes in inner nodes play two different roles. They can be used as multivariate splitting-tests if chosen by the splitting criteria, and used to detect changes in the class-distribution of the examples that traverse the node. When a change in the class-distribution is detected, all the sub-tree rooted at that node will be pruned. The use of naiveBayes classifiers at leaves to classify test examples, the use of splitting-tests based on the outcome of naive-Bayes, and the use of naive-Bayes classifiers at decision nodes to detect changes in the distribution of the examples are directly obtained from the sufficient statistics required to compute the splitting criteria, without no additional computations. This aspect is a main advantage in the context of high-speed data streams. This methodology was tested with artificial and real-world data sets. The experimental results show a very good performance in comparison to a batch decision tree learner, and high capacity to detect drift in the distribution of the examples.'),
(84,'Mining data streams has raised a number of research challenges for the data mining community. These challenges include the limitations of computational resources, especially because mining streams of data most likely be done on a mobile device with limited resources. Also due to the continuality of data streams, the algorithm should have only one pass or less over the incoming data records. In this article, our Algorithm Output Granularity (AOG) approach in mining data streams is discussed. AOG is a novel adaptable approach that can cope with the challenging inherent features of data streams. We also show the results for AOG based clustering in a resource constrained environment.'),
(85,'The past few years have witnessed significant   increase in DDoS attacks on Internet, prompting network security as   a great concern. With the attacks getting more sophisticated,   automatically reasoning the attack scenarios in real time and   categorizing those scenarios become a critical   challenge. However,the overwhelming flow of events generated by   Intrusion Detection System (IDS) sensors make it hard for security   administrators to uncover hidden attack plans. This paper presents a   semantic vector space model to extract and categorize attack   scenarios based on First-order Logics (FOL) and linguistics. The   modified Case Grammar is introduced to formalize the heterogeneous   IDS alerts into uniform structured alert streams. The attack   resolution is then used to generate attack semantic   network. Afterwards, mutual information is used to determine the   alert semantic context range. Based on the attack ontology and alert   contexts, attack scenarios are extracted and the alerts are   represented as attack semantic space vectors. Finally text   categorization technique are used to categorize the intrusion   stages. The preliminary results show our model has better   performance than the traditional alert correlations.'),
(86,'Efficient data preparation needs to discover the   underlying knowledge from complicated Web usage data. In this paper,   we have focused on two main tasks, semantic outlier detection from   online Web request streams and segmentation (or sessionization) of   them. We thereby exploit semantic technologies to infer the   relationships among Web requests. Web ontologies such as taxonomies   and directories can label each Web request as all the corresponding   hierarchical topic paths. Our algorithm consists of two steps. The   first step is the nested repetition of top-down partitioning for   establishing a set of candidates of session boundaries, and the next   step is evaluation process of bottom-up merging for reconstructing   segmented sequences. In addition, we propose the hybrid approach of   this method, as combining with the existing heuristics.  Using   synthesized dataset and realworld dataset of the access log files   of <I>IRCache</I>, we conducted experiments and showed   that semantic preprocessing method improves the performance of rule   discovery algorithms. It means that we can conceptually track the   behavior of users tending to easily change their intentions and   interests, or simultaneously try to search various kinds of   information on the Web.'),
(87,'For many applications, it is important to   evaluate trigger conditions on streaming time series. In a resource   constrained environment, users\' needs should ultimately decide how   the evaluation system balances the competing factors such as   evaluation speed, result precision, and load shedding level. This   paper presents a basic framework for evaluation algorithms that   takes user-specified quality requirements into consideration. Three   optimization algorithms, each under a different set of user-defined   probabilistic quality requirements, are provided in the framework:   (1) minimize the response time given accuracy requirements and   without load shedding; (2) minimize the load shedding given a   response time limit and accuracy requirements; and (3) minimize one   type of accuracy errors given a response time limit and without load   shedding. Experiments show that these optimization algorithms   effectively achieve their optimization goals while satisfying the   corresponding quality requirements.'),
(88,'Online mining changes over data streams has been   recognized to be an important task in data mining. Mining changes   over data streams is both compelling and challenging. In this paper,   we propose a new, single-pass algorithm, called   <I>MFC-append</I> (<u>M</u>ining   <u>F</u>requency <u>C</u>hanges of   <u>append</u>-only data streams), for discovering the   frequent frequency-changed items, vibrated frequency changed items,   and stable frequency changed items over continuous append-only data   streams. A new summary data structure, called   <I>Change-Sketch</I>, is developed to compute the   frequency changes between two continuous data streams as fast as   possible.s Moreover, a MFC-append-based algorithm, called   <I>MFC-dynamic</I> (<u>M</u>ining   <u>F</u>requency <u>C</u>hanges of   <u>dynamic</u> data streams), is proposed to find the   frequency changes over dynamic data streams. Theoretical analysis   and experimental results show that our algorithms meet the major   performance requirements, namely single-pass, bounded space   requirement, and real-time computing, in mining data   streams.'),
(89,'Mining data streams is a challenging task that   requires online systems based on incremental learning   approaches. This paper describes a classification system based on   decision rules that may store up-to-date border examples to avoid   unnecessary revisions when virtual drifts are present in   data. Consistent rules classify new test examples by covering and   inconsistent rules classify them by distance as the nearest   neighbour algorithm. In addition, the system provides an implicit   forgetting heuristic so that positive and negative examples are   removed from a rule when they are not near one another.'),
(90,''),
(91,'We consider a nonconstant polynomial P with real coeients that has at least one negative coeient and derive new upper bounds for the real roots of P . We compare our bounds with those obtained by other methods.'),
(92,''),
(94,'Partially ordered sets are investigated from the   point of view of Bishop\'s constructive mathematics, which can be   viewed as the constructive core of mathematics and whose theorems   can be translated into many formal systems of computable   mathematics. The relationship between two classically equivalent   notions of supremum is examined in detail. Whereas the classical   least upper bound is based on the negative concept of partial order,   the other supremum is based on the positive notion of excess   relation. Equivalent conditions of existence are obtained for both   suprema in the general case of a partially ordered set; other   equivalent conditions are obtained for subsets of a lattice and, in   particular, for subsets of   <b>R</b><I>n<sup>n</I>n</sup>.'),
(95,'For the purpose of constructive reverse   mathematics, we show the equivalence of the uniform continuity   theorem to a series of propositions; this illuminates the   relationship between Brouwer\'s fan theorem and the uniform   continuity theorem'),
(96,'Self-adjoint operators and their spectra play a   crucial role in analysis and physics. For instance, in quantum   physics self-adjoint operators are used to describe measurements and   the spectrum represents the set of possible measurement results.   Therefore, it is a natural question whether the spectrum of a   self-adjoint operator can be computed from a description of the   operator. We prove that given a \"program\" of the operator one can   obtain positive information on the spectrum as a compact set in the   sense that a dense subset of the spectrum can be enumerated (or   equivalently: its distance function can be computed from above) and   a bound on the set can be computed. This generalizes some   non-uniform results obtained by Pour-El and Richards, which imply   that the spectrum of any computable self-adjoint operator is a   recursively enumerable compact set. Additionally, we show that the   spectrum of compact self-adjoint operators can even be computed in   the sense that also negative information is available (i.e. the   distance function can be fully computed). Finally, we also discuss   computability properties of the resolvent map.'),
(97,'We discuss how to compute the halting probability Omega in the limit in a cellular automata world.'),
(98,'We develop the elementary theory of iterated rational functions over the Riemann sphere <IMG SRC=\"constructive_analysis_of_iterated/images/img1.gif\"> in a constructive setting. We use Bishopstyle constructive proof methods throughout. Starting from the development of constructive complex analysis presented in [Bishop and Bridges 1985], we give constructive proofs of Montel\'s Theorem along with necessary generalisations, and use them to prove elementary facts concerning the Julia set of a general continuous rational function with complex coefficients. We finish with a construction of repelling cycles for these maps, thereby showing that Julia sets are always inhabited.'),
(99,'We present a constructive proof of the   Stone-Yosida representation theorem for Riesz spaces motivated by   considerations from formal topology. This theorem is used to derive   a representation theorem for f-algebras. In turn, this theorem   implies the Gelfand representation theorem for C*-algebras of   operators on Hilbert spaces as formulated by Bishop and Bridges. Our   proof is shorter, clearer, and we avoid the use of approximate   eigenvalues.'),
(100,''),
(101,'A class of Kripke models for intuitionistic propositional logic is `axiomatic\' if it is the class of all models of some set of formulas (axioms). This paper discusses various structural characterisations of axiomatic classes in terms of closure under certain constructions, including images of bisimulations, disjoint unions, ultrapowers and `prime extensions\'. The prime extension of a model is a new model whose points are the prime filters of the lattice of upwardlyclosed subsets of the original model. We also construct and analyse a `definable\' extension whose points are prime filters of definable sets. <br>  A structural explanation is given of why a class that is closed under images of bisimulations and invariant under prime/definable extensions must be invariant under arbitrary ultrapowers. This uses iterated ultrapowers and saturation.'),
(102,'This paper is an investigation of positive elements in a Banach algebra.  Under the firmness of the state space of a Banach algebra, it is shown that even powers of positive Hermitian elements are in fact positive.'),
(103,'Let us say that an infinite binary sequence   <I>q</I> lies <I>above</I> an infinite   binary sequence <I>p</I> if <I>q</I> can be   obtained from <I>p</I> by replacing selected 0\'s in   <I>p</I> by 1\'s. We show that above any infinite binary   Martin-L&#246;f random sequence <I>p</I> there exists an   infinite binary nonrandom sequence <I>q</I> above which   there exists an infinite binary random sequence   <I>r</I>. This result is of interest especially in   connection with the new randomness notion for sets of natural   numbers introduced in [Hertling and Weihrauch 1998, Hertling and   Weihrauch 2003] and in connection with its relation to the   Martin-L&#246;f randomness notion for infinite binary   sequences.'),
(104,'The theory of linear lattices is presented as a   system with multiple-conclusion rules. It is shown through the   permutability of the rules that the system enjoys a subterm   property: all terms in a derivation can be restricted to terms in   the conclusion or in the assumptions. Decidability of derivability   with the rules for linear lattices follows through the termination   of proof-search.'),
(105,'We give a construction of coequalisers in formal   topology, a predicative version of locale theory. This allows for   construction of quotient spaces and identification spaces in   constructive topology.'),
(106,'The paper furnishes realizability models of constructive Zermelo-Fraenkel set theory, <B>CZF</B>, which also validate Brouwerian principles such as the axiom of continuous choice (<B>CC</B>), the fan theorem (<B>FT</B>), and bar induction (<B>BI</B>), and thereby determines the proof-theoretic strength of <B>CZF</B> augmented by these principles.<BR> The upshot is that <B>CZF</B>+<B>CC</B>+<B>FT</B> possesses the same strength as <B>CZF</B>, or more precisely, that <B>CZF</B>+<B>CC</B>+<B>FT</B> is conservative over <B>CZF</B> for <img src=\"constructive_set_theory_and/images/img1.gif\"> statements of arithmetic, whereas the addition of a restricted version of bar induction to <B>CZF</B> (called decidable bar induction, <B>BI<sup>D</sup></B>) leads to greater proof-theoretic strength in that <B>CZF</B>+<B>BI<sup>D</sup></B> proves the consistency of <B>CZF</B>.'),
(107,'We define <I>interacting sequential   programs</I>, motivated originally by constructivist   considerations. We use them to investigate notions of implementation   and determinism. Process algebras do not define what can be   implemented and what cannot. As we demonstrate it is problematic to   do so on the set of all processes. Guided by constructivist notions   we have constructed interacting sequential programs which we claim   can be readily implemented and are a subset of processes.'),
(108,'This is a preliminary pass at examining some of the constructive issues in the theory of finite Markov chains. I trust that it is not all bad that there seem to be more questions raised than answered.'),
(109,'A careful analysis of the original definition of   formal topology led to the introduction of a new primitive, namely a   positivity relation between elements and subsets. This is, in other   terms, a direct intuitionistic treatment of the notion of closed   subset in formal topology. However, since formal open subsets do not   determine formal closed subsets uniquely, the new concept of   positivity relation is not yet completely clear. Here we begin to   illustrate the general idea that positivity relations can be   regarded as a further, powerful tool to describe properties of the   associated formal space. Our main result is that, keeping the formal   cover fixed, by suitably redefining the positivity relation of a   regular formal topology one can obtain any given set-indexed family   of points as the corresponding formal space.'),
(110,'The paper presents an axiomatisation for functional dependencies on trees that are defined using constructors for records, lists, sets and multisets. A simple form of restructuring permitting lists to be mapped onto multisets and multisets onto sets is added to the theory. Furthermore, the theory handles dependencies on sets treated as multisets. This adds the possibility to use the count of elements in the dependencies.'),
(111,'The concept of continuity for mappings between metric spaces should coincide with that of uniform continuity in the case of a compact domain, and still give rise to a category. In Bishop\'s constructive mathematics both requests can be fulfilled simultaneously, but then the reciprocal function has to be abandoned as a continuous function unless one adopts the fan theorem. This perhaps little satisfying situation could be avoided by moving to a point-free setting, such as formal topology, in which infinite coverings are defined mainly inductively. The purpose of this paper is to discuss the earlier situation and some recent developments.'),
(113,'We present a to following results in the   constructive theory of operator algebras. A representation theorem   for finite dimensional von Neumann-algebras. A representation   theorem for normal functionals. The spectral measure is independent   of the choice of the basis of the underlying Hilbert space. Finally,   the double commutant theorem for finite von Neumann algebras and for   Abelian von Neumann algebras.'),
(114,'The paper investigates fixed points and   attractors of infinite iterated function systems in Cantor space. By   means of the theory of formal languages simple examples of the   non-coincidence of fixed point and attractor (closure of the fixed   point) are given.'),
(115,'In our description of Brouwer\'s universe we have discussed a few basic principles which have unusual consequences in practical mathematics.'),
(116,'In the context of intuitionistic real analysis,   we introduce the set <img   src=\"perhaps_the_intermediate_value/images/img1.gif\"> consisting   of all continuous functions  from [0, 1] to <img   src=\"perhaps_the_intermediate_value/images/img2.gif\"> such that   (0) = 0 and (1) = 1.  We let <img   src=\"perhaps_the_intermediate_value/images/img3.gif\"> be the set   of all  in <img   src=\"perhaps_the_intermediate_value/images/img1.gif\"> for which   we may find x in [0, 1] such that (x) = <img   src=\"perhaps_the_intermediate_value/images/img4.gif\">. It is   well-known that there are functions in <img   src=\"perhaps_the_intermediate_value/images/img1.gif\"> that we can   not prove to belong to <img   src=\"perhaps_the_intermediate_value/images/img3.gif\">, and that,   with the help of Brouwer\'s Continuity Principle one may derive a   contradiction from the assumption that <img   src=\"perhaps_the_intermediate_value/images/img3.gif\"> coincides   with <img   src=\"perhaps_the_intermediate_value/images/img1.gif\">. We show   that Brouwer\'s Continuity Principle also enables us to define   uncountably many subsets <img   src=\"perhaps_the_intermediate_value/images/img5.gif\"> of <img   src=\"perhaps_the_intermediate_value/images/img1.gif\"> with the   property <img src=\"perhaps_the_intermediate_value/images/img6.gif\">.'),
(117,'Sylvester\'s conjecture states that, given   <I>n</I> distinct noncollinear points in a plane, there   exists a connecting line of two of the points such that no other   point is incident with the line. First a proof is given of the   six-point Sylvester conjecture from a constructive axiomatization of   plane incidence geometry. Next ordering principles are studied that   are needed for the seven-point case. This results in a symmetrically   ordered plane affine geometry. A corollary is the axiom of complete   quadrangles. Finally, it is shown that the problem admits of an   arithmetic translation by which Sylvester\'s conjcture is decidable   for any <I>n</I>.'),
(118,'We consider real sequences in   <I>I</I> = [0, 1) and real functions on   <I>I</I>. It is first shown that, as for real sequences   from <I>I</I>, <B>R</B>-computability   (computability with respect to the Euclidean topology) implies    weak Fine-computability. Using this result, we show   that  Finesequential computability and    <img src=\"sequential_computability_of_a/images/img1.gif\">   -sequential computability are equivalent for effectively   locally Fine-continuous functions as well as for Fine-continuous   functions.'),
(119,'This paper discusses the modelling of a   Unit-of-Learning'),
(120,'Despite the increasing importance gained by   e-learning standards in the past few years, and the unquestionable   goals reached (mainly regarding interoperability among e-learning   contents) current e-learning standards are yet not sufficiently   aware of the context of the learner. This means that only a limited   support for adaptation regarding individual characteristics is   currently being provided. In this article, we propose the use of   semantic metadata for Learning Object (LO) contextualization in   order to adapt instruction to the learner\'s cognitive requirements   in three different ways: background knowledge, knowledge objectives   and the most suitable learning style. In our pilot e-learning   platform (<e-aula>) the context for LOs is addressed in two   different ways: knowledge domain and instructional design. We   propose the use of ontologies as the knowledge representation   mechanism to allow the delivery of learning material that is   relevant to the current situation of the learner.'),
(121,'In this paper we describe the DeepTest tool, which is intended to reinforce the conceptual learning of any subject by means of interactive exercises for the detection of incorrect texts. DeepTest can be used through Internet. The generic aspects of the tool are analyzed, and a first report on conclusions from the use of the tool by a group of students and teachers is presented. The main conclusion is that DeepTest can be used effectively in assessment tasks and its use is very simple and intuitive.'),
(122,'In recent years, there have been many efforts at   research towards obtaining the simple and natural use of computers,   with interfaces closer to the user. New visions such as that of the   Ubiquitous Computing paradigm emerge. In Ubiquitous Computing the   computer is distributed in a series of devices with reduced   functionality, spread over the user\'s environment and communicating   wirelessly. With these, context-aware applications are obtained. In   this paper we present an approach to the classroom context by   identification process using RFID technology, as an implicit input   to the system. The main goal is to acquire natural interaction,   because the only requirement for the user (teacher or student) is to   carry a device (smart label), identifying and obtaining context   services. Some of these services and the mechanisms that make them   available are described here, together with a scenario of their use   in the classroom.'),
(123,'Computer Programming learning is a difficult process. Experience has demonstrated that many students find it difficult to use programming languages to write programs that solve problems. In this paper we describe several educational computer tools used successfully to support Programming learning and we present a global environment which integrates them, allowing a broader approach to Programming teaching and learning. This environment uses program animation and the Computer-Supported Collaborative Learning (CSCL) paradigm.'),
(124,'This paper describes the evolution experienced   by the concept of <I>virtual object</I>. This concept   has evolved in the context of several e-learning projects developed   by the <I>Software Engineering and Artificial   Intelligence</I> (ISIA) group at the <I>Complutense   University of Madrid</I> (UCM). The initial goal of the first   of these projects, the <I>Chasqui</I> Project, was to   facilitate the didactic and scientific use of real objects belonging   to the <I>Archaeology Museum</I> of the   <I>Department of American History II</I> at this   University. As a concept intended for organizing learning and   scientific information, the concept of virtual object has undergone   an important transformation as it has been applied to two other   projects: the virtualization of the <I>Museum of the History   of Computing</I> at the <I>School of Computer   Science</I> at the same university, and the <I>Chasqui   II</I> project, an improved version of the first   <I>Chasqui</I>, which is now under development by the   ISIA group and the <I>Telef&#243;nica I+D</I>   corporation.'),
(125,'This paper describes KADD ET, a cognitive   diagnostic environment created to assess the conceptual and   procedural learning activities of students. It is composed of a   diagnostic engine, DETECTive, and a knowledge acquisition tool   developed to fulfil its knowledge representation needs, KADI. Both   of them share a Model of Learning Tasks (MLT) as a diagnostic   basis. One of the main goals of this environment is to provide   teachers with easy-to-use tools that facilitate the construction of   learning environments with diagnosis capabilities customized to   their particular subject domains and adaptation styles.'),
(126,'Some tasks supported by educative and   collaborative tools can be more realistic and accessible if they are   tackled using mobile devices. This approach allows students to   benefit from the mobility features of this kind of devices, which   are expected to revolutionize teaching in the next decade. In this   paper we present an application called DomoSim-Mob to carry out   practical activities of domotical design using PDAs. We introduce   the task of collaborative planning of design which is supported by   DomoSim-Mob and describe its materialization starting from the user   tasks supported by the previous desktop computer version.'),
(127,'ICT (Information and Communication Technologies)   are a very important educational resource at the present time   because they allow place and time limitations to be overcome and   therefore reduce costs. In addition, multimedia applications offer a   set of characteristics in order to improve learning by means of   interactive activities. For these reasons, we believe it is   important to devote efforts to the development of proposals and   prototypes for teaching via Internet. Since 1998, our research group   has been focused on this objective and working on diverse projects   such as SD2I, TEDA or PDIWeb. In this paper we overview all these   projects, presenting a general description of each web platform, as   well as the tools and methods used for their implementation. This   paper also includes the results obtained after the use of each   platform and the feedback from surveys. In this way, several   conclusions are presented in the last section of this   paper.'),
(128,'Authoring learning material is a multi-disciplinary undertaking where different people can play their role. Any support that can be provided for the collaboration of instructional designers, pedagogues, media designers, and students, among others, is welcome. In particular, metadata annotation of learning objects is an important task within the whole authoring process. This work presents the first resulting products and approaches from the MD2 project, consisting of a service-oriented framework and a tool to support the integrated, ontology-based collaborative annotation of learning objects.'),
(129,''),
(130,'Each objectoriented programming language   proposes various grouping mechanisms to bundle interacting classes   (i.e., packages, modules, selector namespaces, etc). To understand   this diversity and to compare the different approaches, a common   foundation is needed. In this paper we present a simple module   calculus consisting of a small set of operators over environments   and modules. Using these operators, we are then able to specify a   set of module combinators that capture the semantics of Java   packages, C# namespaces, Ruby modules, selector namespaces, gbeta   classes, classboxes, MZScheme units, and MixJuice modules. We   develop a simple taxonomy of module systems, and show how particular   combinations of module operators help us to draw sharp distinctions   between classes of module systems that share similar   characteristics.'),
(131,''),
(132,'Component-based Software Development is an   emerging discipline in the field of Software Engineering. In this   context, coordination languages may be used to specify the   interactive behavior of software components. Our proposal is   oriented towards defining a framework for describing the behavior of   components in terms of coordination models. In particular, we define   a way to complement interface description languages in order to   describe components such that the information about the services   provided by a component can be extended with details on how these   services should be used. We illustrate our approach by applying the   proposed framework to two substantially different coordination   models: Linda and Reo; the former representing the family of   data-oriented coordination models, and the latter a new   channel-based model. Although we consider both models to show the   feasibility of our proposal we hope this study help us to define an   interaction description language based on Reo for component   coordination, as has already been done in the context of   Linda.'),
(133,'This paper addresses   <I>objectification</I>, a formal specification technique   which inspects the potential for object-orientation of a declarative   model and brings the \'implicit objects\' explicit. Criteria for such   objectification are formalized and implemented in a runnable   prototype tool which embeds Vdm-sl into Vdm++. The paper also   includes a quick presentation of a (coinductive) calculus of such   generated objects, framed as generalised Moore machines.'),
(134,'Component-based software composition offers a   development approach with reduced time-to-market and cost while   achieving enhanced productivity, quality and   maintainability. Existent work on the composition paradigm focuses   on static composition, which is not sufficient in a distributed   environment, in which both constituent components and the assembled   distributed system are subject to dynamic adaptation. This paper   presents two types of dynamic composition for distributed   components: assertive and autonomous over a .NET based Web Services   environment. Three case studies are provided to illustrate the use   of assertive and autonomous composition.'),
(135,'Constraint automata have been used as an   operational model for Reo which offers a channel-based framework to   compose complex component connectors. In this paper, we introduce a   variant of constraint automata with discrete probabilities and   nondeterminism, called <I>probabilistic constraint   automata</I>. These can serve for compositional reasoning about   connector components, modelled by Reo circuits with unreliable   channels, e.g., that might lose or corrupt messages, or channels   with random output values that, e.g., can be helpful to model   randomized coordination principles.'),
(136,'Rebeca is an actorbased language for modeling concurrent and distributed systems as a set of reactive objects which communicate via asynchronous message passing. Rebeca is extended to support synchronous communication, and at the same time components are introduced to encapsulate the tightly coupled reactive objects which may communicate by synchronous messages. This provide us a language for modeling globally asynchronous and locally synchronous systems. Components interact only by asynchronous messages. This feature and also the event-driven nature of the computation are exploited to introduce a modular verification approach in order to overcome the state explosion problem in model checking. In this paper we elaborate on the corresponding theory of the modular verification approach which is based on the formal semantics of components in extended Rebeca.'),
(137,'Traditional software used for student-centered learning typically provides for a uniform user interface through which the student can interact with the software, and through which the information is delivered in a uniformly identical fashion to all users without regard to their learning style. This research classifies personality types of computer science undergraduate students using the Myers-Briggs Type Indicator, relates these types of personalities to defined learning preferences, and tests if a given user interface designed for a given learning preference enhances learning. The general approach of this study is as follows: given a set of user interfaces designed to fit personality types, provide a given user interface to participants with the matching personality type. In the control group, provide participants with a randomly chosen user interface. Observe the performance of all participants in a post-test. Additionally, observe if the test group had an enhanced learning experience. Quantitative results indicate that personality-aware user interfaces have a significant effect on learning. Qualitative results show that in most cases, users preferred user interfaces designed for their own personality type. Preliminary results show that for introverted intuitive persons and extraverted intuitive persons, the effect of a personality-aware human-computer interface on learning is significant.'),
(138,'This paper depicts the interrelation between situated learning and learning management from an organizational and personal perspective. Based on this introduction we show how educational metadata can be used for approaches of situated learning and how we can take care of contexts using context specific role-based metadata.'),
(139,'Current E-Learning solutions are not sufficiently aware of the context of the learner, that is the individual\'s characteristics and the organizational context such as the work processes and tasks. Nevertheless, this awareness can be achieved by modular learning objects and semantical metadata for their contextualization. By that delivering of learning material, which is relevant to the current situation of the learner, is supported. This paper presents a general approach and architecture.'),
(140,'This paper is based on on-going research carried out in the framework of an EU project aimed at enhancing knowledge management (KM) in enterprises. It deals with the impact of intercultural factors on the accessibility and presentation of eLearning content. <br><br>      It reports on preliminary findings and discusses the issues which have emerged so far in the contextual study and requirements analysis conducted in preparation for designing Web-based training modules.       <br><br>Once the empirical research is completed and the data analyzed, guidelines will be proposed for developing Web-based training modules for culturally heterogeneous user groups sharing the same professional background. Special consideration will be given to interactive and community features.'),
(141,'In all areas of the e-era, personalization plays an important role. Particularly in e-learning a main issue is student modeling, that is the analysis of student behavior and prediction of his/her future behavior and learning performance. In fact, nowadays, the most prevailing issue in the e-learning environment is that it is not easy to monitor students\' learning behaviors. In this paper we have focused our attention on the system (the Profile Extractor) based on Machine Learning techniques, which allows for the discovery of preferences, needs and interests of users that have access to an e-learning system. The automatic generation and the discovery of the user profile, to agree as simple student model based on the learning performance and the communication preferences, allow creating a personalized education environment. Moreover, we presented an evaluation of the accuracy of the Profile Extractor system using the classical Information Retrieval metrics.'),
(142,'Cooperative learning is characterized by communication and interaction in a group. Computer-supported cooperative/collaborative learning (CSCL) is a specific form doing away with previous spatial and temporal restrictions. At the starting point of our examinations lie known learning processes and their possible support through a CSCL system. Both the significance of different user models and the variety of styles and roles of learning are analyzed in CSCL systems and illustrated in scenarios supporting cooperative or collaborative learning processes. The practical evaluation of the methods and algorithms was done using an adaptive CSCL system to promote learning effects on the domain of software engineering.'),
(143,'How to design effective learning opportunities? Why is learning by experience often more efficient than learning by studying? How to provide the learning experiences needed to respond to current challenges? Using computer games and games in general for educational purposes offers a variety of knowledge presentations and creates opportunities to apply the knowledge within a virtual world, thus supporting and facilitating learning processes. An innovative educational paradigm such as game-based learning, which is considered suitable for the given purpose, is described in this article. The connection of the collaborative social context of education with game-based learning is discussed in the first part of the paper.  <br><br>      The second part of the paper introduces the game concept of \"UniGame: Social Skills and Knowledge Training\". Game ideas along the educational background of the UniGame game concept are outlined. UniGame scenarios presented and possible use cases should stimulate users to apply game-based learning approach in the future for their classes.'),
(144,'The paper focuses on adaptability, knowledge mediation and knowledge flows in face-to-face classes compared to computer-based or Internet-based classes. The paper gives an overview of features of on-line learning systems that facilitate the learning process and gives some aspects on adaptation and personalisation issues within those systems. Some recent developments of intelligent tutors capable of expressing emotions are presented. Application examples of adaptable multimedia e-learning solutions for different user groups are described. An outlook on possible future developments and constraints is provided. The paper starts an important discussion about how to design effective human-computer interaction.'),
(145,''),
(146,'This paper presents a distributed algorithm for the partial precedence constrained scheduling problem. In the classical precedence constrained scheduling problem all the dependent tasks must be scheduled before the task itself can be scheduled. The partial precedence constrained scheduling problem is a generalized version of the original precedence constrained problem in the sense that the number of dependent tasks to be scheduled before the task itself can be scheduled is considered a variable. Using a directed graph to model the partial precedence constrained scheduling problem in which n nodes represent the tasks and e edges represent the precedence constraints, it is shown that the distributed algorithm requires O(e) messages and O(n) units of time and is optimal in communication complexity to within a constant factor.'),
(147,'In this paper we present <tt>PEPSY</tt>, a novel <b>p</b>rototyping <b>e</b>nvironment for multi-DS<b>P</b> <b>s</b>ystems, with the primary goal to support the design and implementation of parallel digital signal processing (DSP) applications subject to various design constraints. Given a specification of the prototyping problem in the form of an application model, a hardware model and mapping constraints, <tt>PEPSY</tt> automatically maps and schedules the DSP application onto the multi-processor system and synthesizes the complete code for each processor. A detailed performance model of the parallel application is an integral part of <tt>PEPSY</tt>. Important performance parameters such as computation and communication times as well as memory consumption can be estimated prior to the implementation. <tt>PEPSY</tt> not only solves the standard mapping and scheduling problem, but it is also able to explore various important design goals for embedded systems and DSP applications such as minimizing memory and power consumption and enforcing the timeliness of tasks. Two complex case studies demonstrate the feasibility of our prototyping environment.'),
(148,'Learning management systems may offer web-based exam facilities. Such facilities entail a higher risk to exams fraud than traditional paper-based exams. The article discusses security issues with web-based exams, and proposes precautionary measures to reduce the risks. A security model is presented that distinguishes supervision support, software restrictions, and network restrictions. Solutions to security problems are tools to supervise and monitor web-based exams, measures for exam computers with Windows and Linux, and secure network setups in common network architectures. The article intends to raise risk awareness among faculty in higher education, and to help technical staff to implement precautions.'),
(149,''),
(150,'The new knowledge-based economy necessitates increasingly the collaboration between different organisations. Despite the recent upsurge in knowledge management and decision support systems, the vast majority of these systems focus on individual organisations. This article introduces the concept of the Learning Network - inter-organisational structures, formally established to increase the participants\' knowledge and innovative capability - and examines the main functions and roles of a Learning Network. It presents an integrated toolkit for supporting knowledge sharing and decision making in Learning Networks that consists of a software system and a methodology. It also briefly presents how the toolkit has been piloted in an automotive cluster. Finally it provides a constructive set of recommendations for using IT to support learning and knowledge sharing in Learning Networks.'),
(151,'KM is more an archipelago of theories and practices rather than a monolithic approach. We propose a conceptual map that organizes some major approaches to KM according to their assumptions on the nature of knowledge. The paper introduces the two major views on knowledge -objectivist, subjectivist - and explodes each of them into two major approaches to KM: knowledge as a market, and knowledge as intellectual capital (the objectivistic perspective), knowledge as mental models, and knowledge as practice (the subjectivist perspective). We argue that the dichotomy between objective and subjective approaches is intrinsic to KM within complex organizations, as each side of the dichotomy responds to different, and often conflicting, needs: on the one hand, the need to maximize the value of knowledge through its replication, on the other hand, the need to keep knowledge appropriate to an increasingly complex and changing environment. Moreover, as a proposal for a deeper discussion, such trade-off will be suggested as the origin of other relevant KM related trade-offs that will be listed. Managing these trade-offs will be proposed as a main challenge of KM.'),
(152,'In our work a new approach, the Distributed Knowledge Management (DKM) approach, is used and organizations are seen as constellations of communities, which \"own\" local knowledge and exchange it through meaning negotiation coordina_ tion processes. In order to reify communities within a DKM system, the concept of Knowledge Node (KN) is used and then applied in a case study: a complex Italian national firm, the Impresa Pizzarotti  [and]  C. S.p.A. All communities of practices are un_ veiled and reified as KNs within a high level architecture of a DKM system. In this paper it is argued that, even if knowledge has to be organized and made useful to the whole organization, there are types of knowledge that must be managed in an autonomous way, and the DKM approach is a good system to deal with coordination/negotiation processes.'),
(153,'An ontology-based knowledge sharing system OntoShare and its evaluation as part of a case study is described. RDF(S) is are used to specify and populate an ontology, based on information shared between users in virtual communities. We begin by discussing the advantages that use of Semantic Web technology afford in the area of knowledge management tools. The way in which OntoShare supports WWW-based communities of practice is described. Usage of OntoShare semiaautomatically builds an RDF-annotated information resource for the community (and potentially for others also). Observing that in practice the meanings of and relationships between concepts evolve over time, OntoShare supports a degree of ontology evolution based on usage of the system - that is, based on the kinds of information users are sharing and the concepts (ontological classes) to which they assign this information. A case study involving OntoShare was carried out. The evaluation exercise for this case study and its results are described. We conclude by describing avenues of ongoing and future research.'),
(154,'In software engineering, the quality of development and business processes and their models is of utmost importance for (a) the quality of the software products developed and (b) the operational success of the organization. Nevertheless, many organizations neglect these processes and leave the knowledge about them in the heads of their experts. In this paper, we present the indiGo method and platform for eParticipative Process Learning. Furthermore, we present the results of a three case studies for the evaluation of these methods. The results indicate that processes introduced and modeled with process user participation result in process models with higher acceptance and better perceived quality.'),
(155,'It has been observed that for a Community of Practice (CoP) to be successful, a significant amount of time shall be devoted to understanding the needs of community members. Furthermore, a tool to support the CoP shall be selected based on the kind of activities that are most important for that CoP. Since many of the tools available today place emphasis on a single type of application such as e-learning or document management, unplanned selection may rise unwanted barriers. In this paper, we examine the benefits of integrating some of the following types of technologies into one single technological platform and their impact on CoP: (1) content- and document Management, (2) collaboration / groupware, (3) web conferencing, and (4) e-learning.'),
(156,'Communities of Practice (CoPs) are among the most promising concepts to promote the genesis, evolution and exchange of knowledge in organizations. However, there is a gap between CoP theories and their implementation in companies. Our case studies of four attempts to introduce CoP-related structures show that the different underlying management principles can systematically be analyzed in at least two dimensions, technology \"vs.\" the social and exchange \"vs.\" production. We argue that the choice is not contingent, but that emphasis on the social and the creative production of new knowledge leads to more productive structures in the area and in the sense of knowledge intensive services. For the conception of such approaches we show that it is useful to think in terms of another structure between \"teams\" and \"communities\", which we call \"nets of experts\".'),
(157,'Our research on knowledge management is rooted in the community perspective. We believe that knowledge systems should serve primarily to help people create and share new knowledge. But we also acknowledge the role of stable, structured and reliable information, both as a component of our systems and as a component of the organizations within which we work. The contribution of the paper is a framework for integrating organizational and community perspectives on knowledge management and its computational support. Our basic idea is that knowledge is not a static chunk of information, but rather, knowledge evolves in a cycle of knowledge liquidization and crystallization. The evolving process takes place through the interactions among conceptual worlds, representational worlds, and the real world. This paper first describes the knowledge liquidization and crystallization framework. We then illustrate the approach with three systems, Knowledge Nebula Crystallizer, livingOM, and ART-SHTA.'),
(158,''),
(159,'This paper discusses some implications of knowledge creation processes in informal social networks for the development of technologies to support them. The principal point of departure are social theories of learning and the theories of organisational knowledge creation. The focus is on models for the exchange and sharing of implicit knowledge. A model of personalised learning knowledge maps is presented as one possible way of addressing the problem of capturing, visualising and sharing implicit knowledge of a community of users. In particular, we discuss how this model resolves one critical shortcoming of the existing socialisation and externalisation approaches: the creation of a semantic representation of a shared understanding of the community which reflects implicit knowledge and incorporates personal views of individual users. Finally, we outline the application to a real-world interdisciplinary Internet platform netzspannung.org.'),
(160,'Creating online communities of practice involves much more than creating software. Software houses online communities of practice activities but social interactions also depend on who is involved, what their goals are, their personalities and the community\'s norms and policies. By paying attention to these sociability issues, community members can influence how their community develops. Norms that lead to good online etiquette, empathy and trust between community members provide stepping-stones for social capital development.'),
(161,'It is commonly believed that a significant part of  the computational power of membrane systems comes from their inherent non-determinism. Recently, R. Freund and Gh. Paun have considered deterministic P systems, and formulated the general question whether the computing (generative) capacity of non-deterministic P systems is strictly larger than the (accepting) capacity of their deterministic counterpart.<br><br>      In this paper, we study the computational power of deterministic P systems in the evolution{communication framework. It is known that, in the generative case, two membranes are enough for universality. For the deterministic systems, we obtain the universality with three membranes, leaving the original problem open.'),
(162,'This paper introduces a notion of population P systems as a class of tissue P systems where the links between the cells can be modified by means of a specific set of bond making rules. As well as this, cell division rules which introduce new cells into the system, cell differentiation rules which change the set of rules that can be used inside of a cell, and cell death rules which remove cells from the system are also considered by introducing a particular notion of population P systems with active cells. The paper mainly reports universality results for the following models: (a) population P systems where cells are restricted to communicate only by means of the environment but never forming any bond, (b) population P systems with bond making rules with restricted communication rules, (c) population P systems possessing only the cell differentiation operation, and (d) population P systems equipped with cell division rules and bond making rules.'),
(163,'Moving \"instructions\" instead of \"data\" using transport mechanisms inspired by biology is the basic idea of the computing device presented in this paper. Specifically, we propose a new class of P systems that use both evolution rules and symport/antiport rules. The idea of this kind of systems is the following: during a computation, symbol-objects (the \"data\") evolve using evolution rules, but they cannot be moved, on the other hand, the evolution rules (the \"instructions\") can be moved across the membranes using classical symport/antiport rules. We present a number of results using different combinations of evolution rules (catalytic, non-cooperative) and the weight of the symport/antiport rules. In particular, we show that using non-cooperative rules and antiports of unbounded weight makes it possible to obtain at least the Parikh set of ET0L languages. On the other hand, using catalytic rules (one catalyst) and antiports of weight 2, these system become universal. Several open problems are also presented.'),
(164,'In this paper we present a first approach to the definition of different entropy measures for probabilistic P systems in order to obtain some quantitative parameters showing how complex the evolution of a P system is. To this end, we define two possible measures, the first one to reflect the entropy of the P system considered as the state space of possible computations, and the second one to reflect the change of the P system as it evolves.'),
(165,'A nondeterministic, maximally parallel methodology for finding the maximum element in a set of numerical values is presented, suitable for being implemented on P systems. Several algorithms of maximum search are then developed for different types of such systems, namely using priorities, nested membranes and linked transport, and their performances are evaluated accordingly. The proposed solutions are expected to find application inside membrane models devoted to compute algorithmic procedures in which the greatest element in a data set must be found. Dynamic algorithms for DNA sequence alignment are an example of such procedures.'),
(166,'This article shows how the computational universality can be reached using P systems with object rewriting non-cooperative rules, promoters/inhibitors at the level of rules, and only one catalyst. Both generative and accepting cases are studied. The theoretical issues presented are illustrated by several examples.'),
(167,'Reversibility plays a fundamental role when the possibility to perform computations with minimal energy dissipation is considered. Many papers on reversible computation have appeared in literature, the most famous of which is certainly the work of Bennett on (universal) reversible Turing machines. Here we consider the work of Fredkin and Toffoli on conservative logic, which is a mathematical model that allows to describe computations which reflect some properties of microdynamical laws of physics, such as reversibility and conservation of the internal energy of the physical system used to perform the computations. The model is based upon the Fredkin gate, a reversible and \"conservative\" (according to a definition given by Fredkin and Toffoli) three-input/three-output boolean gate.  In this paper we introduce energy{based P systems as a parallel and distributed model of computation in which the amount of energy manipulated and/or consumed during computations is taken into account. Moreover, we show how energy-based P systems can be used to simulate the Fredkin gate. The proposed P systems that perform the simulations turn out to be themselves reversible and conservative.'),
(168,'Membrane Computing is a recent area of Natural Computing, a topic where much work has been done but still much remains to be done. There are some applications which have been developed in imperative languages, like C++, or in declaratives languages, as Prolog, working in the framework of P systems. In this paper, a software tool (called <i>SimCM</i>, from Spanish <i>Simulador de Computacion con Membranas</i>) for handling P systems is presented. The program can simulate basic transition P Systems where dissolution of membranes and priority rules are allowed. The software application is carried out in an imperative and object-oriented language - Java. We choose Java because it is a scalable and distributed language. Working with Java is the first step to cross the border between simulations and a distributed implementation able to capture the parallelism existing in the membrane computing area. This tool is a friendly application which allows us to follow the evolution of a P system easily and in a visual way. The program can be used to move the P system theory closer to the biologist and all the people who wants to learn and understand how this model works.'),
(169,'In this paper we present an effective solution to the Bin Paching problem using a family of recognizer P systems with active membranes. The analysis of the solution presented here will be done from the point of view of complexity classes. A CLIPS simulator for recognizer P systems is used to describe a session for an instance of Bin Packing, using a P system from the designed family.'),
(170,'The P systems are a class of distributed parallel computing devices of a biochemical type. In this paper, a new definition of separation rules in P systems with active membranes is given. Under the new definition, the efficiency and universality of P systems with active membranes and separation rules instead of division rules are investigated.'),
(171,''),
(172,'The paper discusses the integration of codified and tacit knowledge as a potential source of competitive advantage. The management of explicit knowledge is viewed through knowledge management practices, whereas the management of tacit knowledge is conceptualised through strategic human resource management. The paper presents the empirical results of testing of low- and high-synergy models of knowledge integration on a representative sample of large Croatian enterprises.'),
(173,'Knowledge audit lays a concrete foundation for any knowledge management programs. The central topic of this paper is to integrate various knowledge audit related techniques into pre-audit preparation, in-audit process and post-audit analysis in a systematic manner. Culture assessment, in the form of surveys and radar charts, along with orientation program make up the pre-audit preparation. Structured interviews are carried out to capture process-critical knowledge. Knowledge inventory, knowledge maps and knowledge flow analysis compose of post-audit analysis. Knowledge inventory is then built for stocktaking knowledge assets and thus revealing the key knowledge assets by measuring them against four performance criteria. Knowledge mapping together with social network analysis are to show the knowledge exchange path and make the key knowledge suppliers and customers visible. They are then being further applied into knowledge flow analysis, which serves to reveal the strength and weakness of the current knowledge flow. A case study of applying the designed instruments in the Engineering Division of the Hong Kong Dragon Airlines Limited and the related analysis are also present in this paper.'),
(174,'This paper is focused on the problem of skill matching in an organizational context. We endow the classical weighted bipartite graph approach with a semantic based assignment of arcs weight and we describe a skill matching system implementing the approach. The system takes curricula and project specifications as inputs and extracts from them individual profiles respectively offered and requested, according to an ontology modeling skill management context. The suitability of each available individual to each task to assign is evaluated based on an algorithm whose returned scores are used as arc weights. As a result the semantics of profile descriptions is taken into account in the assignment process.'),
(175,'This paper presents further research findings on the use of software-based, collaborative visual communication tools for the transfer and creation of professional knowledge in organizational decision making contexts. The paper begins by describing typical knowledge communication situations and summarizes dominating problems in these contexts. It then reports on the real-life experiences in using three visual knowledge communication tools, namely the OnTrack visual protocol tool, the Parameter Ruler application, and the Synergy Map. The application experiences with these tools in four companies show that they can reduce some of the discussed problems. Their main benefits are focus, coordination, documentation, consistency, accountability and traceability. Their major improvement areas are accessibility and flexibility. Implications for further research and for further tool developments are highlighted.'),
(176,'This document describes our current work on developing a framework which supports organizations in the successful implementation of Knowledge Management (KM). It follows the holistic approach of a KM introduction by considering technological, organizational and human aspects, as well as the organizational culture in equal measure. The framework provides recommendations based on Case-Based Reasoning (CBR) techniques and Semantic Web technologies. It supports the four processes of Aamodt  &  Plaza\'s CBR-cycle. The best practice cases for a successful KM implementation are structured by the use of an ontology.'),
(177,'In Higher Education and Research Organisations (HEROs), one of the most important activities in the R & D process is the effective management of knowledge transference. A correct analysis and diagnosis of that process through knowledge management methodology is essential for the correct orientation of organisation strategy. The aim of this paper is to describe the analysis carried out in order to diagnose the research  &  development  &  transference (R & D & T) activities at a public university in Spain. The diagnosis analyses the key phases in the knowledge transference process, because these different stages define important implications for the monitoring of the intellectual capital and the organisation s performance. Also with in the diagnostic analysis preformed here an methodological innovation is introduced related with the cause and effect relations of the knowledge collaboration and a process witch deals mainly with intangibles.'),
(178,'In many organisations, conservation of specialised expertise is picked out as a central theme only after experienced members have already left. The paper presents the SELaKT method, a method for Sustainable Expert Localisation and Knowledge T ransfer based on social network analysis (SNA). It has been developed during a project co-operation between the Department of Information Science at the Institute for Media and Communication Studies, Free University Berlin, and the Fraunhofer Institute for Production Systems and Design Technology IPK, Berlin. The SELaKT method uses recent insights into network analysis and pragmatically adapts SNA to suit organisational practice. Thus it provides a strategic tool to localise experts, to identify knowledge communities and to analyse the structure of knowledge flows within and between organisations. The SELaKT method shows its advances and increasing relevance for practical use by integration of specific organisational conditions and requirements into the process of analysis.'),
(179,'A small collection of metadata concepts has been jointly negotiated among a group of specialists to be relevant for classifying data used in their field. A series of comparisons are made to test levels of agreement between individuals when these concepts are used to tag data items. Inter-coder agreement measures are presented for a range of data sets and individuals with varying relationships to the data sets. The implications of the results for the use of metadata as a supporting mechanism for knowledge sharing are discussed.'),
(180,''),
(181,'This paper provides guidelines and examples for visualising lexical relations using Formal Concept Analysis. Relations in lexical databases often form trees, imperfect trees or poly-hierarchies which can be embedded into concept lattices. Many-to-many relations can be represented as concept lattices where the values from one domain are used as the formal objects and the values of the other domain as formal attributes. This paper further discusses algorithms for selecting meaningful subsets of lexical databases, the representation of complex relational structures in lexical databases and the use of lattices as basemaps for other lexical relations.'),
(182,'We apply the graph decomposition method known as <i>rooted level aware breadth first search</i> to partition graph-connected formal contexts and examine some of the consequences for the corresponding concept lattices. In graph-theoretic terms, this lattice can be viewed as the lattice of maximal bicliques of the bipartite graph obtained by symmetrizing the object-attribute pairs of the input formal context. We find that a rooted breadth-first search decomposition of a graph-connected formal context leads to a closely related partition of the concept lattice, and we provide some details of this relationship. The main result is used to describe how the concept lattice can be unfolded, according to the information gathered during the breadth first search. We discuss potential uses of the results in data mining applications that employ concept lattices, specifically those involving association rules.'),
(183,'The recent advances in Formal Concept Analysis (FCA) together with the major changes faced by modern Information Retrieval (IR) provide new unprecedented challenges and opportunities for FCA-based IR applications. The main advantage of FCA for IR is the possibility of creating a conceptual representation of a given document collection in the form of a document lattice, which may be used both to improve the retrieval of specific items and to drive the mining of the collection\'s contents. In this paper, we will examine the best features of FCA for solving IR tasks that could not be easily addressed by conventional systems, as well as the most critical aspects for building FCA-based IR applications. These observations have led to the development of CREDO, a system that allows the user to query Web documents and see retrieval results organized in a browsable concept lattice. This is the second major focus of the paper. We will show that CREDO is especially useful for quickly locating the documents corresponding to the meaning of interest among those retrieved in response to an ambiguous query, or for mining the contents of the documents that reference a given entity. An on-line version of the system is available for testing at http://credo.fub.it.'),
(184,'Additive diagrams are used by several interactive lattice layouters. We discuss a method to avoid unwanted incidences when working with additive diagrams.'),
(185,''),
(186,'Implications of a formal context <i>(G, M, I)</i> obey Armstrong rules, which allows for definition of a minimal (in the number of implications) implication base, called Duquenne-Guigues or stem base in the literature. A long-standing problem was that of an upper bound for the size of a stem base in the size of the relation <i>I</i>. In this paper we give a simple example of a relation where this boundary is exponential. We also prove <i>#P</i>-hardness of the problem of determining the size of the stem base (i.e., the number of pseudo-intents).'),
(187,'This paper presents an original application of the Galois lattice theory, the visual landmark selection for topological localization of an autonomous mobile robot, equipped with a color camera. First, visual landmarks have to be selected in order to characterize a structural environment. Second, such landmarks have to be detected and updated for localization. These landmarks are combinations of attributes, and the selection process is done through a Galois lattice. This paper exposes the landmark selection process and focuses on probabilistic landmarks, which give the robot thorough information on how to locate itself. As a result, landmarks are no longer binary, but probabilistic. The full process of using such landmarks is described in this paper and validated through a robotics experiment.'),
(188,''),
(189,'This paper presents the FBT (FIL to Buechi automaton Translator) tool which automatically translates any formula from FIL (Future Interval Logic) into its semantically equivalent Buechi automaton. There are two advantages of using this logic for specifying and verifying system properties instead of other more traditional and extended temporal logics, such as LTL (Linear Temporal Logic): firstly, it allows a succinct construction of specific temporal contexts, where certain properties must be evaluated, thanks to its key element, the interval, and secondly, it also permits a natural, intuitive, graphical representation. The underlying algorithm of the tool is based on the <i>tableau</i> method and is specially intended for application in <i>on-the-fly model checking</i>. In addition to a description of the design and implementation structure of FBT, we also present some experimental results obtained by our tool, and compare these results with the ones produced by an other tool of similar characteristics (i.e. based on an <i>on-the-fly tableau</i> algorithm), but for LTL.'),
(190,'The B Abstract Machine Notation (AMN) and the notation of Communicating Sequential Processes (CSP) have previously been applied to formalise the UML class and state diagrams, respectively. This paper discusses their integrated use in checking the consistency between the two kinds of UML diagrams based on some recent results of research in integrated formal methods. Through a small information system example, the paper illustrates a clear-cut separation of concerns in employing the two formal methods. Of particular interest is the treatment of recursive calls within a single class of objects.'),
(191,'A condensed specification of a multi-level marketing (MLM) enterprise which can be modelled by mathematical forests and trees is presented in Z. We thereafter identify a number of proof obligations that result from operations on the state space. Z is based on first-order logic and a strongly-typed fragment of Zermelo-Fraenkel set theory, hence the feasibility of using certain reasoning heuristics developed for proving theorems in set theory is investigated for discharging the identified proof obligations. Using the automated reasoner OTTER, we illustrate how these proof obligations from a real-life enterprise may successfully be discharged using a suite of well-chosen heuristics.'),
(192,'The increasing popularity of SAT and BDD techniques in formal hardware verification and automated synthesis of logic circuits encourages the search for additional speedups. Since typical SAT and BDD algorithms are exponential in the worst-case, the structure of realworld instances is a natural source of improvements. While SAT and BDD techniques are often presented as mutually exclusive alternatives, our work points out that both can be improved via the use of the same structural properties of instances. Our proposed methods are based on efficient problem partitioning and can be easily applied as pre-processing with arbitrary SAT solvers and BDD packages without modifying the source code of SAT/BDD tools. <br><br>      Finding a better variable ordering is a well recognized problem for both SAT solvers and BDD packages. Currently, the best variable-ordering algorithms are dynamic, in the sense that they are invoked many times in the course of the host algorithm that solves SAT or manipulates BDDs. Examples include the DLCS ordering for SAT solvers and variable sifting during BDD manipulations. In this work we propose a universal variable-ordering algorithm MINCE (MIN Cut Etc.) that pre-processes a given Boolean formula in CNF. MINCE is completely independent from target SAT algorithms and in some cases outperforms both the variable state independent decaying sum (VSIDS) decision heuristic for SAT and variable sifting for BDDs. We argue that MINCE tends to capture structural properties of Boolean functions arising from real-world applications. Our contribution is validated on the ISCAS circuits and the DIMACS benchmarks. Empirically, our technique often outperforms existing SAT/BDD techniques by a factor of two or more. Our results motivate the search for better dynamic ordering heuristics and combined static/dynamic techniques.'),
(193,'We propose a novel technique to improve SAT-based Combinational Equivalence Checking (CEC). The idea is to perform a low-cost preprocessing that will statically induce global signal relationships into the original CNF formula of the miter circuit under verification, and hence reduce the complexity of the SAT instance. This efficient and effective preprocessing quickly builds up the implication graph for the miter circuit under verification, yielding a large set of direct, indirect and extended backward implications. These two-node implications spanning the entire circuit are converted into binary clauses, and they are added to the miter CNF formula. The added clauses constrain the search space of the SAT solver and provide correlation among the different variables, which enhances the Boolean Constraint Propagation (BCP). Experimental results on large and difficult ISCAS\'85, ISC AS\'89 (full scan) and ITC\'99 (full scan) CEC instances show that our approach is independent of the state-of-the-art SAT solver used, and that the added clauses help to achieve not eworthy speedup for each of the cases. Also, comparison with Hyper-Resolution (Hypre), Non-Increasing Variable Elimination Resolution (NIVER) and the propositional formula checker HeerHugo, suggests that our technique is more powerful, yielding non-trivial clauses that significantly simplify the SAT instance complexity.'),
(194,'Binary Decision Diagrams (BDDs) have been widely  used in synthesis and verification. Boolean Satisfiability (SAT) Solvers, on the other hand, have been gaining ground only recently, with the introduction of efficient implementation procedures. Specifically, while BDDs have been mainly adopted to formally verify the correctness of hardware devices, SAT-based Bounded Model Checking (BMC) has been widely used for debugging.<br><br>      In this paper, we combine BDD and SAT-based methods to increase the efficiency of BMC. We first exploit affordable BDD-based symbolic approximate reachability analysis to gather information on the state space. Then, we use the collected overestimated reachable state sets to restrict the search space of a SAT-based BMC. This is possible by feeding the SAT solver with a description that is the combination of the original BMC problem with the extra information coming from BDD-based symbolic analysis. We develop specific strategies to appropriately mix BDD and SAT efforts, and to efficiently convert BDD-based symbolic state set representations into SAT-oriented ones.<br><br>Experimental results prove the validity of our strategy to reduce the amount of variable assignments and variable conflicts generated by SAT solvers, with a subsequent significant performance gain. We gather results with four among the most used SAT solvers, namely <tt>Chaff, Limmat, BerkMin,</tt> and <tt>Siege</tt>. We could reduce the number of conflicts up to more than 100x, and the verification time up to 30x.'),
(195,'Recent work has shown the value of using propositional  SAT solvers, as opposed to pure BDD solvers, for solving many real-world Boolean Satisfiability problems including Bounded Model Checking problems (BMC). We propose a SAT solver paradigm which combines the use of BDDs and search methods to support efficient implementation of complex search heuristics and effective use of early (preeprocessor) learning. We implement many of these ideas in software called SBSAT. We show that SBSAT solves many of the benchmarks tested competitively or substantially faster than state-of-the-art SAT solvers.<br><br>      SBSAT differs from standard propositional SAT solvers by working directly with non-CNF propositional input, its input format is BDDs. This allows some BDD-style processing to be used as a preprocessing tool. After preprocessing, the BDDs are transformed into state machines (different state machines than the ones used in the original model checking problem) and a good deal of lookahead information is precomputed and memoized. This provides for fast implementation of a new form of look ahead, called local-function-complete lookahead (contrasting with the depth-first lookahead of zChaff [Moskewicz et al. 01] and the breadth-first lookahead of Prover [Stlmarck 94]). SBSAT provides a choice of search heuristics, allowing users to exploit domain-specific experience. We describe SBSAT in this paper.<br><br>      We use SBSAT in conjunction with the tool bmc from Carnegie Mellon to translate a bounded model checking problem to classical propositional logic and then use SBSAT to solve the bmc output. We show this approach is faster than the now traditional approach of translating the bmc output to CNF clauses and using a CNF-based SAT solver, such as zChaff. The work continues that of [Franco et al. 01] and [Franco et al. 04].'),
(196,'We propose two heuristics, implicit learning and explicit learning, that utilize circuit topological information and signal correlations to derive conflict clauses that could efficiently prune the search space for solving circuit based SAT problem instances. We implemented a circuit-SAT solver SC-C-SAT based on the proposed heuristics and the concepts used in other state-of-the-art SAT solvers. For solving unsatisfiable circuit examples and for solving difficult circuit-based problems at Intel, our solver is able to achieve speedup of one order of magnitude over other state-of-the-art SAT solvers that do not use the heuristics.'),
(197,''),
(198,'As we move from developing procedure-oriented to O-O programs, the complexity traditionally found in functions and procedures is moving to the connections among components. More faults occur as components are integrated to form higher level aggregates. Consequently, we need to place more effort on testing the connections among components. Although O-O technology provides abstraction mechanisms to build components to integrate, it also adds new compositional relations that can contain faults, which must be found during integration testing. This paper describes new techniques for analyzing and testing the polymorphic relationships that occur in O-O software. The application of these techniques can result in an increased ability to find faults and overall higher quality software.'),
(199,'Developing multimedia applications entails understanding a variety of advanced technologies, in addition, multimedia programming poses a significant challenge in terms of handling a variety of hardware devices, multimedia formats or communication protocols. Therefore, break-through software engineering technologies should be applied to produce reference architectures able to support ever-changing requirements. In this paper, we first present the challenges that designers must face in multimedia programming, and how current frameworks address them, specially regarding the management of architectural evolution. We then show what breakthrough approaches or technologies can be used to produce more reusable, extensible and open multi-media systems. We focus on presenting the benefits of applying component-based software development and application framework technologies. We also illustrate how to componentize all multimedia functionalities and (re)use the resulting components as COTS in application frame-works. This approach helps to add multimedia capabilities to an application without requiring specific knowledge on multimedia.'),
(200,'In this paper, we address some of the challenges raised by the emerging service-oriented computing paradigm in what concerns the ability to define dynamic interactions between core services for flexible and agile business processes. We claim that, from this point of view, service interaction and composition is well beyond the reach of object-oriented and component_ based techniques. We argue instead for the use of architectural modelling techniques that promote the externalization of coordination mechanisms. We show how what we call composition laws and interfaces can be used to define the coordination logic according to which the behavior of a business process can be described in terms of interactions with given partners. These primitives provide a business modelling level that can be mapped onto the specifications that are being proposed for web services, e.g., BPEL, WS-Coordination or WS-Transaction.'),
(201,'This paper presents an outline of a formal model management framework that provides breakthroughs for legacy systems recovery (RELS) and for data migration (ADAM). To recover a legacy system, we use an algebraic approach by using algebras in order to represent the models and manipulate them. RELS also generates automatically a data migration plan that specifies a data transfer process to save all the legacy knowledge in the new recovered data-base. The data migration solution is also introduced as a support for the O-O conceptual schemas evolution where their persistent layers are stored by means of relational databases, in the ADAM tool. Contents and structure of the data migration plans are specified using an abstract data migration language. Our past experience in both projects has guided us towards the model management research field. We present a case study that illustrates the application of both tools.'),
(202,''),
(203,'Today\'s organisations are increasingly relying on the web to support their operations and the integration of their processes with their partners\'. Portlets, which are distributed web components that encapsulate web applications, are considered a promising breakthrough towards this aim. The goal is to define a component model to enable portlets to be easily plugged into web portals. This article outlines the main challenges associated with the definition and use of portlets. As any component model, portlets should have clear interfaces so that they can be plugged into third_party applications. This includes the communication between the consumer of a portlet and the portlet container, as well as the communication between the portlet container and the portlet itself. Two standards, WSRP and JSR168, address these issues. After outlining them, we conclude with some insights into the implementation of portlets.'),
(204,'As agent technology has matured with the deployment of a variety of applications, particularly in open and dynamic environments such as the web, several methodologies and tools have been proposed to support software engineers during the development process of such systems. This article takes an overall look at representative agent-oriented methodologies by considering how they support specific agent-related concepts. This serves to identify areas in which this technology has shown its potential to solve new problems, e.g., the ability to manage complexity with an organizational perspective, goal-driven modelling as a way to build robust behaviors for adaptive systems, or the definition of notation and mechanisms to implement high-level interactions and protocols between agents. In order to be fully applicable, the challenge today is the maturity of supporting tools, and new methods for validation and verification of multi-agent systems.'),
(205,'Adopting the most appropriate methodology for particular software developments remains a challenge for all industrial IT organizations. Previous attempts to promote a single approach as useful for <i>all</i> occasions has proven untenable. Rather, a combination of a metamodel and a repository of process/method components (\"method engineering\") provides a more efficacious approach, particularly as elements of the method engineering approach are able to be automated. In this paper, we advocate the use of method engineering, illustrating its utility by the construction of methodologies at various levels of process capability.'),
(206,'Many software projects have failed because their requirements were poorly negotiated among stakeholders. Reaching agreements of negotiated requirements among stakeholders who have different concerns, responsibilities, and priorities is quite challenging. Formal (fully-automated) approaches of requirements negotiation require significant efforts of knowledge representation and validation, whereas informal (manual) approaches do not provide systematic methods of requirements negotiation. This paper proposes a novel light-weighted, yet systematic requirements negotiation model, called \"Multi-Criteria Preference Analysis Requirements Negotiation (MPARN)\" to guide stakeholders to evaluate, negotiate, and agree upon alternatives among stakeholders using multi-criteria preference analysis theory. This eight-step MPARN model was applied to requirements gathered for an industrial-academic repository system. The result showed that the MPARN model assisted stakeholders to have unbiased aspects within a requirements negotiation in a light-weighted way and increase stakeholders\' levels of cooperation and trust by measuring each stakeholder\'s preference and value function explicitly through a step-by-step process.'),
(207,'This paper proposes a <i>document oriented paradigm</i> to the development of content-intensive, document-based applications (e.g. educational and hypermedia applications, and knowledge based systems). According to this paradigm, the main aspects of this kind of applications can be described by means of <i>documents</i>. Afterwards, these documents are marked up using descriptive domain-specific markup languages and applications are produced by the automatic processing of these marked documents. We have used this paradigm to improve the maintenance and portability of content-intensive educational and hypermedia applications. ADDS (<i>Approach to Document-based Development of Software</i>) is an approach to software development based on the document oriented paradigm. A key feature of ADDS is that formulation of domain-specific markup languages is a dynamic and eminently pragmatic activity, and markup languages evolve according to the authoring needs of the different participants in the development process (<i>domain experts and developers</i>). The evolutionary nature of markup languages in ADDS leads to OADDS (<i>Operationalization in ADDS</i>), the proposed operationalization model for the incremental development of <i>modular</i> markup language processors. Finally, the document-oriented paradigm can also be applied in the construction of OADDS processors that are also described using marked documents. This paper presents our ADDS approach, including the operationalization model and its implementation as an object-oriented framework. The application of our document-oriented paradigm to the construction of OADDS processors is also presented.'),
(208,'Digital Libraries have been the subject of more than a decade of attention by researchers and developers, and yet in all this time the implementations have not matched the promises. By far the majority of systems have concentrated on content and provided limited or basic functions for users. In this article we offer a new look at what can be expected from a digital library system based on contemporary developments in Information and Communications Systems and Technology. First, we sketch out the basic functions which are provided to support finding and accessing material by a reader. Next we explain some extended functions which support the use and re-use of documents - links and annotations - and the need to support learners in addition to readers and writers. Finally, we present our visions for a modern digital library and e-Learning portal system which includes for example intelligent and conceptual search support including results visualization, white lists, and adaptive user interfaces.      <hr align=\"left\" width=\"50%\"> <br><br>      1) On sabbatical from Curtin University of Technology, Perth, Western Australia'),
(209,'We present an embedded_system design flow, discuss its details, and demonstrate its advantages. We adopt the object_oriented methodology for the system_level model because software dominates hardware in embedded systems and the object_oriented methodology is already established for software design and reuse. As the building_block of system implementation, we synthesise application_specific processors that are reusable, through programming, for several related applications. This addresses the high cost and risk of manufacturing specialised hardware tailored to only a single application. Both the processor and its software are generated from the model of the system by the synthesis and compilation procedures provided. We observe that the key point in object_oriented methodology is the class library, and hence, we implement methods of the class library as the instruction_set of the processor. This allows the processor to be synthesised just once, but, by programming, to be reused several times and specialised to new applications that use the same class library. An important point here is that the processor allows its instructions to be selectively overridden by software routines, this not only allows augmentation of processor capabilities in software, but also enables a structured approach to make software patches to faulty or outdated hardware. A case study illustrates application of the methodology to various applications modelled on top of a common basis class library, and moreover, demonstrates new application_specific opportunities in power management (and area_management for FPGA implementations) resulting from the structure of the processor based on de_activation of unused features.'),
(210,'The specification and validation of security protocols often requires viewing function calls - like encryption/decryption and the generation of fake messages - explicitly as actions within the process semantics. Following this approach, this paper introduces a symbolic framework based on value-passing processes able to handle symbolic values like fresh nonces, fresh keys, fake addresses and fake messages. The main idea in our approach is to assign to each value-passing process a formula describing the symbolic values conveyed by its semantics. In such symbolic processes, called <i>constrained processes</i>, the formulas are drawn from a logic based on a message algebra equipped with encryption, signature and hashing primitives. The symbolic operational semantics of a constrained process is then established through semantic rules updating formulas by adding restrictions over the symbolic values, as required for the process to evolve. We then prove that the logic required from the semantic rules is decidable. We also define a bisimulation equivalence between constrained processes, this amounts to a generalisation of the standard bisimulation equivalence between (non-symbolic) value-passing processes. Finally, we provide a complete symbolic bisimulation method for constructing the bisimulation between constrained processes.'),
(211,'Lempel-Ziv (LZ) is a popular lossless data compression algorithm that produces good compression performance, but suffers from relatively slow processing speed. This paper proposes an enhanced version of the Lempel-Ziv algorithm, through incorporation of a neural pre-processor in the popular predictor-encoder implementation. It is found that in addition to the known dramatic performance increase in compression ratio that multi-stage predictive techniques achieve, the results in this paper show that overall processing speed for the multi-stage scheme can increase by more than 15 times for lossless LZ compression of numeric telemetry data. The benefits of the proposed scheme may be expanded to other areas and applications.'),
(212,'The splitting method was defined by the author in [Margenstern 2002a], [Margenstern 2002d]. It is at the basis of the notion of combinatoric tilings. As a consequence of this notion, there is a recurrence sequence which allows us to compute the number of tiles which are at a fixed distance from a given tile. A polynomial is attached to the sequence as well as a language which can be used for implementing cellular automata on the tiling.  <br><br>      The goal of this paper is to prove that the tiling of hyperbolic 4D space is combinatoric. We give here the corresponding polynomial and, as the first consequence, the language of the splitting is not regular, as it is the case in the tiling of hyperbolic <i>3D</i> space by rectangular dodecahedra which is also combinatoric.'),
(213,'After a brief introduction to hash-coding (scatter storage) and discussion of methods described in the literature, it is shown that for hash tables of length p > 2, prime, the primitive roots r of the cyclic group <b>Z/p</b> of prime residues mod p can be used for a simple collision strategy q(p,i) = r<sup>i</sup> mod p for fi(k) = f0(k) + q(p,i) mod p. It is similar to the strategy which uses quadratic residues q(p,i) = i<sup>2</sup> mod p in avoiding secondary clustering, but reaches all table positions for probing. A table of n primes for typical table lengths and their primitive roots is added. In cases where r = 2<sup>j</sup> is such a primitive root, the collision strategy can be implemented simply by repeated shifts to the left (by j places in all).  To make the paper self-contained and easy to read, the relevant definitions and the theorems used from the Theory of Numbers are included in the paper.'),
(214,'By considering string-objects and rewriting rules, we propose a variant of tissue P systems, namely, <i>rewriting tissue P systems</i>. We show the computational efficiency of rewriting tissue P systems by solving the Satisfiability and the Hamiltonian path problems in linear time. We study the computational capacity of rewriting tissue P systems and show that rewriting tissue P systems with at most two cells and four states are computationally universal. We also show the universality result of rewriting tissue P systems with at most one cell and five states. Finally we propose some new directions for future work.'),
(215,'In this paper we develop a formal specification of  aspects of the Forth programming language. We describe the operation of the Forth compiler as it translates XSForth control words, dealing in particular with the interpretation of immediate words during compilation. Our goal here is to provide a basis for the study of safety properties of embedded systems, many of which are constructed using Forth or Forth-like languages. To this end we construct a model of the Forth compiler in the <img src=\"a_formal_model_of/images/img1.gif\">-calculus, and have simulated its execution by animating this model using the Pict programming language.'),
(216,'We propose an alternative solution to the problems solved in [1]. Our aim is to advocate the efficiency of algebraic methods for the solution of the Boolean equations which occur in the decomposition of Boolean functions.'),
(217,'We consider the problem of d-dimensional searching (d <img src=\"geometric_retrieval_for_grid/images/img1.gif\"> 3) for four query types: <i>range, partial range, exact match and partial match searching</i>. Let <i>N</i> be the number of points, s be the number of keys specified in a partial match and partial range query and t be the number of points retrieved. We present a data structure with worst case time complexities O(t + logd-2 N), O(t + (d - s) + logs N), O(d + <img src=\"geometric_retrieval_for_grid/images/img2.gif\">) and O(t + (d - s) + s <img src=\"geometric_retrieval_for_grid/images/img2.gif\">) for each of the aforementioned query types respectively.  We also present a second, more concrete solution for exact and partial match queries, which achieves the same query time but has different space requirements. The proposed data structures are considered in the RAM model of computation.'),
(218,'We see a temporal data warehouse as a set of temporal views defined in the past fragment of the temporal relational algebra extended with set-valued attributes and aggregation. This paper proposes an incremental maintenance method for temporal views that allows improvements over the re-computation from scratch. We introduce a formalism for temporal data warehouse specification that summarizes information needed for its incremental maintenance. According to this formalism, a temporal data warehouse <b>W</b> is a pair of two sets of views : the <i>materialized component</i> and the <i>virtual component</i>. The materialized component of <b>W</b> represents the set of views physically stored in the warehouse. The virtual component of <b>W</b> is a set of non-temporal expressions involving only relations kept in the materialized component. Several features of our approach make it especially attractive as a maintenance method for warehouses: (a) there is no need for storing the entire history of source databases, (b) maintenance of the temporal data warehouse is reduced to maintaining the (non-temporal) materialized component, and (c) the materialized component is self-maintainable. We build a uniform algorithm by combining two previously unrelated techniques based on auxiliary views. Our method is sufficiently general so that it can be easily adapted to treating databases with complex-valued attributes.'),
(219,'In this paper we propose <i>X-Global</i>, a novel, almost automatic and semantic system for integrating a set of XML sources. <i>X-Global</i> is parametric w.r.t. the flexibility level against which the integration task is performed, indeed, it can operate on rigid contexts, when two concepts are merged only if they have exactly the same meaning, as well as on flexible and informal situations, when two concepts are merged if they have close, even if not exactly identical, meanings. In this paper we describe the system in all details, illustrate various theoretical results as well as several experiments we have carried out for verifying its performance. Finally, we examine related literature and point out similarities and differences between <i>X-Global</i> and several other approaches already proposed in the past.'),
(220,''),
(221,'After a period of oblivion, a renewal of interest in coroutines is being observed. However, most current implementations of coroutine mechanisms are restricted, and motivated by particular uses. The convenience of providing true coroutines as a general control abstraction is disregarded. This paper presents and discusses the coroutine facilities provided by the language Lua, a full implementation of the concept of asymmetric coroutines. It also shows that this powerful construct supports easy and succint implementations of useful control behaviors.'),
(222,'The basic motivation of component based development is to replace conventional programming by the composition of reusable off-the-shelf units, externally coordinated through a network of connecting devices, to achieve a common goal. This paper introduces a new <i>relational</i> model for software connectors and discusses some preliminary work on its implementation in <tt>HASKELL</tt>. The proposed model adopts a coordination point of view in order to deal with components\' temporal and spatial decoupling and, therefore, to provide support for looser levels of inter-component dependency and effective external control.'),
(223,'This paper presents a modular rewriting semantics (MRS) specification for Reppy\'s Concurrent ML (CML), based on Peter Mosses\' modular structural operational semantics specification for CML. A modular rewriting semantics specification for a programming language is a rewrite theory in rewriting logic written using techniques that support the modular development of the specification in the precise sense that every module extension is <i>conservative</i>. We show that the MRS of CML can be used to <i>interpret</i> CML programs using the rewrite engine of the Maude system, a high-performance implementation of rewriting logic, and <i>to verify</i> CML programs using Maude\'s built-in LTL model checker. It is assumed that the reader is familiar with basic concepts of structural operational semantics and algebraic specifications.'),
(224,'Exception handling is a very popular technique for incorporating fault tolerance into software systems. However, its use for structuring concurrent, distributed systems is hindered by the fact that the exception handling models of many mainstream object-oriented programming languages are sequential. In this paper we present an aspect-based framework for incorporating concurrent exception handling in Java programs. The framework has been implemented in AspectJ, a general purpose aspect-oriented extension to Java. Our main contribution is to show that AspectJ is useful for implementing the concerns related to concurrent exception handling and to provide a useful tool to developers of distributed, concurrent fault-tolerant applications.'),
(225,''),
(226,'Categorical Multi-Combinators form a rewriting system developed with the aim of providing efficient implementations of lazy functional languages. The core of the system of Categorical Multi-Combinators consists of only four rewriting laws with a very low pattern-matching complexity. This system allows the equivalent of several <img src=\"partial_categorical_multi_combinators/images/img1.gif\">-reductions to be performed at once, as functions form frames with all their arguments. Although this feature is convenient for most cases of function application it does not allow partially parameterised functions to fetch arguments. This paper presents Partial Categorical Multi-Combinators, a new rewriting system, which removes this drawback.'),
(227,'We present LuaInterface in this paper, a library for scripting the .NET CLR with Lua. The .NET Common Language Runtime (CLR) aims to provide interoperability among objects written in several different languages. LuaInterface gives Lua the capabilities of a full CLS consumer. The Common Language Specification (CLS) is a subset of the CLR specification, with rules for language interoperability, and CLS consumers are languages that can use CLS_compliant libraries. LuaInterface lets Lua scripts instantiate and use CLR objects, and even create new CLR types. CLR applications may also use LuaInterface to embed a Lua interpreter, using Lua scripts to extend the application. LuaInterface is implemented as a bridge between the Lua interpreter and the CLR. The implementation required no changes to the interpreter, and the level of integration is the same that a Lua compiler would have.'),
(228,'Conventional object oriented middleware platforms rely on the notion of remote interfaces to describe distributed services. This notation is very similar to the one currently used in centralized systems, which increases the productivity of programming. This paper is founded in the observation that remote interfaces foster a programming model that ignores the differences between local and remote interactions. This can result in distributed applications with poor performance, that are not robust to failures, and that can not scale beyond local networks. Therefore, we propose that remote interfaces should be accompanied by the specification of tactics that deal with typical events in distributed computing, such as concurrency, partial failures and high latencies. The paper proposes a tactics definition language and describes the implementation of a middleware system that supports this language.'),
(229,'The driving idea of functional programming is to make programming more closely related to mathematics. A program in a functional language such as Haskell or Miranda consists of equations which are both computation rules and a basis for simple algebraic reasoning about the functions and data structures they define. The existing model of functional programming, although elegant and powerful, is compromised to a greater extent than is commonly recognised by the presence of partial functions. We consider a simple discipline of total functional programming designed to exclude the possibility of non-termination. Among other things this requires a type distinction between data, which is finite, and codata, which is potentially infinite.'),
(230,'This paper presents an overview of the Redwood programming environment and details one of its key features, snippets. Through snippets, developers can both make use of a variety of predefined programming constructs and build their own reusable program components. Language-independent, snippets are descriptions of program parts that can be as simple as an assignment statement or as complex as a sophisticated optimization algorithm. In Redwood, snippets also provide support for a distinguishing facility of visual environments: direct manipulation via drag-and-drop. An example of working with snippets, including snippet definition, visualization, customization, and mapping to code is also presented in the paper.'),
(231,'MetaJ is a programming environment that supports metaprogramming in the Java language. The environment is designed to allow extensions via plug-ins which permit the user to manipulate programs written in different languages. This facilities concern only syntactic aspects. Semantics aspects are language-dependent and are not addressed here, but could be tackled with other tools, which could even be layered on the top of MetaJ. Accessing patterns by example inside ordinary Java programs is a major feature of MetaJ programming. This paper presents a conceptual description of the environment, implementation details and three applications on analysis, restructuring and generation of programs.'),
(232,'We concentrate on two major aspects of reactive system design: behavior control and modularity. These are studied from a formal point of view, within the framework of action systems. The traditional interleaving paradigm is completed with a barrier synchronization mechanism. This is achieved by introducing a new parallel composition operator, applicable to bot h discrete and hybrid models. While offering improvements with respect to control and modularity, the approach uses the correctness preserving mechanisms provided by the underlying reasoning environment 1.   <hr align=\"left\" width=\"50%\"> <br><br> 1) A shorter version of this study appeared as \"Modular Design of Reactive Systems\", in Proceedings of the 28th Annual International Computer Software and Applications Conference (COMPSAC 2004), IEEE Computer Society Press, September 2004, Hong Kong. Pages 265-271.'),
(233,'The design of large, complex computer based systems, based on their architecture, will benefit from a formal system that is intuitive, scalable and accessible to practitioners. The work herein is based in graphs which are an efficient and intuitive way of encoding structure, the essence of architecture. A model of system architectures and architectural abstraction is proposed, using poset labelled graphs and their transformations. The poset labelled graph formalism closely models several important aspects of architectures, namely topology, type and levels of abstraction. The technical merits of the formalism are discussed in terms of the ability to express and use domain knowledge to ensure sensible refinements. An abstraction / refinement calculus is introduced and illustrated with a detailed usage scenario. The paper concludes with an evaluation of the formalism in terms of its rigour, expressiveness, simplicity and practicality.'),
(234,'The Meta Object Facility (MOF) forms one of the core standards of the Object Management Group\'s Model Driven Architecture. It has several use-cases, including as a repository service for storing abstract models used in distributed object-oriented software development, a development environment for generating CORBA IDL, and a metamodeling language for the rapid specification, construction, and management of domain-specific technology-neutral modeling languages. This paper will focus on the use of MOF as a metamodeling language and describe our latest work on changing the MIC metamodeling environment from UML/OCL to MOF. We have implemented a functional graphical metamodeling environment based on the MOF v1.4 standard using GME and GReAT. This implementation serves as a testament to the power of formally well-defined metamodeling and metamodel-based model transformation approaches. Furthermore, our work gave us an opportunity to evaluate sevaral important features of MOF v1.4 as a metamodeling language:<ul><li>Completeness of MOF v1.4 for defining the abstract syntax for complex (multiple aspect) DSML-s</li><li>The Package concept for composing and reusing metamodels</li><li>Facilities for modeling the mapping between the abstract and concrete syntax of DSML-s</li></ul>'),
(235,'Protocol analysis involves several parameters in  model specification, for instance, transmission delay or the length of the transmitting window. Verification of the model with parameters is a semi-decision process that depends on the number of clocks, parameters and counters in the model. Using combination of different verification tools for timed models as HyTech, TReX and UPPaal we are able to find relation between parameters satisfying desired property. The paper gives a report on the synthesis of parameters of PGM protocol. We built a formal model based on extended time automata with parameters and verified the reliability property. Our results automatically obtained from the model are consistent with previous results derived manually. The paper describes our experience with parametric verification of multicast protocol PGM. Results mentioned in the work were made with collaboration with Mihaela Sighireanu (Mihaela.Sighireanu@liafa.jussieu.fr) from LIAFA, Paris.'),
(236,''),
(237,'The model-based approach to the development of embedded systems relies on the use of explicit models in the design process. If these models faithfully represent the components of the system with respect to their properties as well as their interactions, then they can be used to predict the dynamic behavior of the system under construction. In this paper we argue for modeling the execution platform that facilitates the component interactions, and show how models of the application and the knowledge of the platform can be used to translate system configurations into another abstract formalism (timed automata, in our case) that allows system verification through model checking.'),
(238,''),
(239,'We present a new proof environment for the specification language Z. The basis is a semantic representation of Z in a structure-preserving, shallow embedding in Isabelle/HOL. On top of the embedding, new proof support for the Z schema calculus and for proof structuring are developed. Thus, we integrate Z into a well-known and trusted theorem prover with advanced deduction technology such as higher-order rewriting, tableaux-based provers and arithmetic decision procedures. A further achievement of this work is the integration of our embedding into a new tool-chain providing a Z-oriented type checker, documentation facilities and macro support for refinement proofs, as a result, the gap has been closed between a logical embedding proven correct and a tool suited for applications of nontrivial size.'),
(240,'We present a method for checking global conditions for object systems in a way that avoids state space explosion. The objects referred to in a global condition are checked step by step against local conditions and communication requirements derived from the global condition. The derivation is automatic, based on information about the system structure contained in the global condition. The approach is demonstrated using model checking, but the idea works for other approaches to verification or testing as well. In our current investigation, a multi-object variant of CTL is used for expressing global conditions. The local conditions and communication requirements can be verified independently using standard model checkers. The method is illustrated by a large example (about 10 24 states) where our method shows a spectacular speedup over global model checking.'),
(241,'This paper describes the program transformation system <i>Ultra</i>. The intended use of <i>Ultra</i> is to assist programmers in the formal derivation of correct and efficient programs from high-level descriptive or operational specifications. We illustrate its utility by deriving a version of the Heapsort algorithm from a non-deterministic specification.            <hr align=\"left\" width=\"50%\">            <i>Ultra</i> supports equational reasoning about functional programs using defining equations, algebraic laws of underlying data structures, and transformation rules. The system does not only support modifying terms, but is also useful for bookkeeping and development navigating tasks.            <hr align=\"left\" width=\"50%\">            The most salient features of <i>Ultra</i> are its sound theoretical foundation, its extendability, its flexible and convenient way to express transformation tasks, its comfortable user interface, and its lean and portable implementation. <i>Ultra</i> itself is written in the functional language Gofer.'),
(242,'Verification and validation are the major tasks during the design of digital hardware/software systems. Often more than 70% of the development time is spent for locating and correcting errors in the design. Therefore, many techniques have been developed to support the debugging process. Recently, simulation and test methods have been accompanied by formal methods such as equivalence checking and property checking. However, their industrial applicability is currently restricted to small or medium sized designs or to a specific phase in the design process. Therefore, simulation is still the most commonly applied verification technique.             <hr align=\"left\" width=\"50%\">            In this paper, we present a method for asserting temporal properties during simulation and also during emulation of hardware prototypes. The properties under verification are efficiently translated into an intermediate language (of a virtual machine). This intermediate representation can then be interpreted during simulation. We may also produce executable checkers running in parallel to the simulation. Furthermore, we are able to translate the properties into synthesizable hardware modules which can then be used during system emulation on FPGA-based emulators or as self test components checking the functionality during the lifetime of the system.'),
(243,'We discuss our experience obtained during the PROFIsafe verification and test case generation project at Siemens Corporate Technology. In this project, a formal analysis of the PROFIsafe protocol for failsafe communication has been carried out. A formal model based on denite-state machines has been obtained from the UML specification of the protocol. This model has been analysed with formal verification techniques, and several important properties have been proven. Based on the verified model, a set of test cases for the automatic execution of conformance tests has been derived. The paper explains how the UML statecharts defining the PROFIsafe protocol are translated into denite-state machines, and points out important aspects and problems occurring during the modelling and verification of industrial applications.'),
(244,'The tool Moby/RT supports the design of realtime systems at the levels of requirements, design specifications and programs. Requirements are expressed by constraint diagrams [Kleuker, 2000], design specifications by PLC-Automata [Dierks, 2000], and programs by Structured Text, a programming language dedicated for programmable logic controllers (PLCs), or by programs for LEGO Mindstorm robots. In this paper we outline the theoretical background of Moby/RT by discussing its semantic basis and its use for automatic verification by utilising the model-checker UPPAAL [Larsen et al., 1997].'),
(245,'We evaluate the use of program checking to ensure the correctness of compiler implementations. Our contributions in this paper are threefold: Firstly, we extend the classical notion of black-box program checking to program checking with certificates. Our checking approach with certificates relies on the observation that the correctness of solutions of <b>NP</b>-complete problems can be checked in polynomial time whereas their computation itself is believed to be much harder. Our second contribution is the application of program checking with certificates to optimizing compiler backends, in particular code generators, thus answering the open question of how program checking for such compiler backends can be achieved. In particular, we state a checking algorithm for code generation based on bottom-up rewrite systems from static single assignment representations. We have implemented this algorithm in a checker for a code generator used in an industrial project. Our last contribution in this paper is an integrated view on all compiler passes, in particular a comparison between frontend and backend phases, with respect to the applicable methods of program checking.'),
(246,''),
(247,'Alias analysis for Fortran is less complicated than for programming languages with pointers but many real Fortran programs violate the standard: a formal parameter or a common variable that is aliased with another formal parameter is modified. Compilers, assuming standard-conforming programs, consider that an assignment to one variable will not change the value of any other variable, allowing optimizations involving the aliased variables. Higher performance results but anything may happen: the program may appear to run normally, or produce incorrect answers, or behave unpredictably. The results may depend on the compiler and the optimization level.             <hr align=\"left\" width=\"50%\">            To guarantee the standard conformance of programs in order to make program analyses exact and program optimizations safe, precise alias information, i.e the determination of overlaps among arrays is studied in this paper. Static analyses and code instrumentation are used to find all violations of the aliasing rules in Fortran code. Alias violation tests are inserted only at places where it cannot be proved statically that they are useless in order to reduce the number of dynamic checks at run-time. A specific memory location naming technique is used to obtain compact representation and to enhance the precision of alias analysis. Modifications on the dependence graph created by aliasing are also studied to show the impact of aliases on the correctness of some program optimizing transformations. Experimental results on SPEC95 CFP benchmark are presented and some related issues are also discussed.'),
(248,'Optimizing programs by applying source-to-source transformations is a prevalent practice among programmers. Particularly so, while programming for high-performance and cost-effective embedded systems, where the initial program is subject to a series of transformations to optimize computation and communication. In the context of parallelization and custom memory design, such transformations are applied on the loop structures and index expressions of array variables in the program, more often manually than with a tool, leading to the non-trivial problem of checking their correctness. Applied transformations are <I>semantics preserving</I> if the transformed program is <I>functionally equivalent</I> to the initial program from the input-output point of view. In this work we present an automatic technique based on geometric modeling to formally check the functional equivalence of initial and transformed programs under loop and data reuse transformations. The verification is <I>transformation oblivious</I> needing no information either about the particular transformations that have been applied or the order in which they have been applied. Our technique also provides useful diagnostics to locate the detected errors.'),
(249,'There is a growing awareness, both in industry and academia, of the crucial role of formally verifying the translation from high-level source-code into low-level object code that is typically performed by an optimizing compiler. Formally verifying an optimizing compiler, as one would verify any other large program, is not feasible due to its size, ongoing evolution and modification, and, possibly, proprietary considerations. <i>Translation validation</i> is a novel approach that offers an alternative to the verification of translators in general and compilers in particular: Rather than verifying the compiler itself, one constructs a validation tool which, after every run of the compiler, formally confirms that the target code produced in the run is a correct translation of the source program. The paper presents voc, a methodology for the translation validation of optimizing compilers. We distinguish between structure preserving optimizations, for which we establish a simulation relation between the source and target code based on computational induction, and <i>structure</i> modifying optimizations, for which we develop specialized permutation rules. The paper also describes voc-64 - a prototype translation validator tool that automatically produces verification conditions for the global optimizations of the SGI Pro-64 compiler.'),
(250,'This bibliography offers citations for people who are interested in learning more about visual language, new types of communicating and archiving information with emphases on novel technologies and theoretical works in these multidisciplinary areas. This bibliography is considered in its broadest sense and covers references of research in humanities and social sciences as well as computer technology. Far from being exhaustive, it nevertheless covers essential resources in a selective way, so that the material can provide starting points for many different directions. What is not included here are references to visual programming languages.'),
(251,''),
(252,'Research in neurosciences, cognitive psychology and media sciences indicates that \"visual thinking\" carries a potential of the human mind that is generally still neglected today but could heavily be fostered by novel types of communicating and archiving information. Computer technology (information systems, telecommunication and visual tools) in turn promises to provide a wide range of highly effective tools to support visual, dynamic communication. MIRACLE treads new paths to address a crucial issue: In what way and to what extent can and should current and future systems support new ways of communicating and archiving information using dynamic, visual information?             <hr align=\"left\" width=\"50%\">            This paper gives a survey of the numerous attempts that have been made so far to overcome language barriers by introducing artificial languages (both on a spoken/text and on a visual basis). It also analyzes the general status of technology (computer hardware and software) to support such efforts as well as a number of specific projects. From this overview we draw the conclusion that computer-based systems designed to support communicating and archiving dynamic visual information should focus on the following features:      <br>      <ul>       <li>Support dynamic language elements on a structural level in addition to traditional animated icons,</li>      <li>Incorporate gestural language elements (inspired by natural sign languages) anticipating future ways of human-computer interaction,</li>      <li>Allow evolutionary development of the language in a group-dynamic and interactive process involving large international groups of participants.</li>      </ul>            In a final section we give a brief outline of the cluster of specific projects carried out under the heading of MIRACLE.'),
(253,'In this paper we argue that in about ten years time PCs as we now know them, will no longer exist. Their functionality will be totally integrated into mobile telephony devices, or putting it differently, in ten years time mobile phones will incorporate all functions one would expect from a powerful PC. These new devices, let us call them eAssistants, will be with us all the time and will change our lives enormously. In this paper we take a first look at both the technological and applied aspects of this prediction.'),
(254,'Systems supporting new forms of communication and archiving of dynamic visual information have a range of potential applications, some of which are described in this paper on a conceptual basis. We present a visual language for dynamic (historic) maps, applications of pictorial lexicons, concepts for interactive support systems for assembly and repair, and a platform for abstract movies.'),
(255,'The study of cellular automata (CA) on tilings of hyperbolic plane was initiated in [6]. Appropriate tools were developed which allow us to produce linear algorithms to implement cellular automata on the tiling of the hyperbolic plane with the regular rectangular pentagons, [8, 10]. In this paper we modify and improve these tools, generalise the algorithms and develop them for tilings of the hyperbolic plane with regular rectangular s-gons for s <img src=\"fibonacci_type_coding_for/images/img1.gif\"> 5. For this purpose a combinatorial structure of these tilings is studied.'),
(256,'This paper aims at investigating the integration of structured data into a semi-structured environment. In particular, it introduces the Cyclic Structure Converter (CSC) system that performs this task. In CSC, correspondence assertions and integration rules provide the adequate intelligence to reconcile the (possible) heteroge_ neous semantics relative to involved information sources. CSC has also the capability to filter and process only the relevant operational data. CSC s versatility in maneuvering with di#erent data models allows it to be applied into any field, such as engineering, insurance, medicine, space science and education, to mention a few.'),
(257,''),
(258,'In this paper, an integrated information system is presented that offers enhanced search and retrieval capabilities to users of heterogeneous digital audiovisual (a/v) archives. This innovative system exploits the advances in handlings a/v content and related metadata, as introduced by MPEG-4 and worked out by MPEG-7, to offer advanced services characterized by the tri-fold semantic phrasing of the request (query), unified handling and personalized response. The proposed system is targeting the intelligent extraction of semantic information from a/v and text related data taking into account the nature of the queries that users my issue, and the context determined by user profiles. It also provides a personalization process of the response in order to provide end_users with desired information. From a technical point of view, the FAETHON system plays the role of an intermediate access server residing between the end users and multiple heterogeneous audiovisual archives organized according to the new MPEG standards.'),
(259,'Automatic acquisition of information structures like Topic Maps or semantic networks from large document collections is an important issue in knowledge management. An inherent problem with automatic approaches is the treatment of multiword terms as single semantic entities. Taking company names as an example, we present a method for learning multiword terms from large text corpora exploiting their internal structure. Through the iteration of a search step and a verification step the single words typically forming company names are learnt. These name elements are used for recognizing compounds in order to use them for further processing. We give some evaluation of experiments on company name extraction and discuss some applications.'),
(260,'The goal of this article is to explore some of the main reasons that sustain a distributed approach to Knowledge Management, and this will be done, first, showing how, according to very different theoretical disciplines, knowledge diversity is proposed as the very source of organizational innovation and adaptability, second providing some evidence coming from major applicative domains, third proposing some considerations on the role of technology.'),
(261,'Companies are starting to recognise synergies between knowledge management, training and e-learning programs, but a closer look reveals that these integration ideas are rarely implemented in practice. The goal of this paper is to provide a starting point for collaboration between corporate KM and HR/learning teams by mapping existing practices of linking KM, training and e-learning efforts. We provide an overview of experiences and future ideas of collaboration derived from several studies, group them in three themes and then illustrate each theme with a scenario. The first theme gives examples of using HR and training instruments to support knowledge management. The second theme represents cases of using KM methods (namely a community of practice) to support HR learning management efforts. The last theme describes how KM and HR/learning teams could work on joint initiatives. Then we discuss the added value of the scenarios and propose further practical steps and research directions.'),
(262,'In the fast moving businesses the ability to be flexible and adaptive to change is crucial. When monitoring their operating environments for weak signals and for other disruptive information companies face filters that hinder the entry of the information to the company. We are discussing three filters: mentality filter, surveillance filter and power filter. Each filter has a logic of its own that hinders effective knowledge flow. We introduce a software tool that helps to overcome these filters especially in a strategy formulation process.'),
(263,'Algorithmic research is an established knowledge engineering process that has allowed researchers to identify new or significant problems, to better understand existing approaches and experimental results, and to obtain new, effective and efficient solutions. While algorithmic researchers regularly contribute to this knowledge base by proposing new problems and novel solutions, the processes currently used to share this knowledge are inefficient, resulting in unproductive overhead. Most of these publication-centred processes lack explicit high-level knowledge structures to support efficient knowledge management. The authors describe a problem-centred collaborative knowledge management architecture associated with Computational Problem Solving (CPS). Specifically we articulate the structure and flow of such knowledge by making in-depth analysis of the needs of algorithmic researchers, and then extract the ontology. We also propose a knowledge flow measurement methodology to provide human-centred evaluations of research activities within the knowledge structure. This measurement enables us to highlight active research topics and to identify influential researchers. The collaborative knowledge management architecture was realized by implementing an Open Computational Problem Solving (OpenCPS) Knowledge Portal, which is an open-source project accessible at http://www.opencps.org.'),
(264,'The development of a knowledge management system (KMS) is becoming increasingly important for the metal industry in Taiwan. The ontology design and knowledge search are two major activities of knowledge management. In this paper, we introduce a three-stage life cycle for the ontology design and propose a Java/XML-based scheme for automatically generating knowledge search components to reduce the overhead in developing a KMS. The resulting ontology is classified as information ontology and domain ontology so that the objective of semantic match for knowledge search can be realized. The system is built on the top of the component-based KAON development suite which makes it more flexible and robust. We conduct a case study by applying the system to Metal Industries Research  [and]  Development Centre (MIRDC), Taiwan to confirm its effectiveness and efficiency in dealing with KM activities. In addition, the proposed reusable scheme endorses the encouraging feasibility of wide applications to different domains.'),
(265,'Within a business environment, where the fast and reliable access to knowledge is a key success factor, an efficient handling of the organizational knowledge is crucial. Therefore the need for methods and techniques, which allow to structure and maintain complex knowledge bases according to the requirements emerging from the daily work have a high priority. This article provides a business process oriented approach to structure organizational knowledge and information bases. The approach was developed within applied research in the industrial, service and administrative sector. Following this approach, three different types of knowledge structures and their visualization have been developed by the Fraunhofer IPK and are currently applied and tested in organizations. Beside the approach itself, these three types of knowledge structure and the cases of application shall be introduced here.'),
(266,'In the paper the applications to the 2002 Knowledge Management Award are discussed in relation to measuring the benefits of KM. It is concluded that benefits of KM initiatives depend on the KM approach taken.'),
(267,'The objective of this paper is to describe a new post-Nonaka generation of Knowledge Management that, for the first time, has the potential to meet people\'s expectations. It is divided into the three categories:<br><ul><li>processes</li><li>organisation & culture</li><li>information technology</li></ul>  and builds on Frederik Taylor s idea of applying knowledge to work, though not on his Scientific Management model. Instead, it extends to Knowledge Workers and gives answers to the key question of Knowledge Management: How can the productivity of knowledge workers be increased?'),
(268,'Modern computational Problem Solving Environments (PSEs) become more and more complex and knowledge intensive in terms of their integrated toolsets, in particular for engineering design search and optimization. Whether these toolsets can be assembled effectively to produce satisfactory results depends heavily on using the best domain practice and following decisions made by skilled engineers in practical situations. In this paper, a knowledge based approach is used to acquire this knowledge from existing sources and model it in a maintainable fashion. Ontologies are used to develop the conceptualization of a knowledge base. In order to reuse this knowledge to provide guidance at knowledge intensive points, we propose a knowledge based advisor, which can give a context-aware critique to guide users through effective operations of building domain workflows. The concept of a state panel is proposed to collect system state information, which is then reasoned about together with various task models in the JESS (Java Expert System Shell) environment. Two reasoning strategies are designed for different advising styles. A multilayer and client-server style architecture is proposed to illustrate how this advisor can be deployed to make available its knowledge advising service to a real workflow construction PSE in a maintainable fashion. Throughout we use the example of these knowledge services in the context of design optimization in engineering.'),
(269,'The increasing complexity of both the environment in which companies operate and of their internal workings combined with an increasingly high pressure for innovation make knowledge and its efficient management central to business success today. The management of knowledge is more than directly managing knowledge as a resource. It is more concerned with providing knowledge-friendly environments in which knowledge can flourish and develop. The development of such environments can be addressed from many different perspectives, which makes knowledge management a very interdisciplinary field of research. It concerns human resources management, organizational development and information technology management to mention just the three most important fields.  This special issue on hot spots in knowledge management offers the reader a broad overview of the leading edge developments, technologies and applications in knowledge management. The issue serves two distinct purposes: (1) It may help shaping the reader s thinking in the way required for a successful implementation of knowledge management in an organization, (2) it may serve as a stimulus for the reader s research in knowledge management. The following 13 papers fall in four categories:  Knowledge Management in Business  Knowledge Mining  Knowledge Representation  Convergence of Knowledge Management with other Domains  The first category - knowledge management in business - addresses knowledge management from a more business-oriented perspective. In more detail, the papers of this group address the following topics:  Peter Schuett from IBM Stuttgart (Germany) presents in his paper The post-Nonaka Knowledge Management a new generation of knowledge management that can be divided in three categories (1) processes, (2) organization and culture, and (3) information technology. The objective of the paper is to provide solutions for increasing the productivity of knowledge workers through knowledge management.  He argues that in order to increase productivity we need to understand the work environment of knowledge workers. To provide guidance, P. Schuett identifies 11 factors which help understanding and improving a knowledge worker s environment. This factors fall in the following three groups work processes, organisation and culture, and information technology.  Klaus North and Tina Hornung from the University of Applied Sciences in Wiesbaden (Germany) entitled their paper The Benefits of Knowledge Management - Results from the German award _Knowledge Manager 2002_. Based on the evaluation of almost 40 companies the authors present which added-value and benefit knowledge management can generate. The benefits are grouped in the following five perspectives: learn and growth, business processes, customer satisfaction, financial results, and employee satisfaction. The results of the study revealed that knowledge management can generate the highest benefit in business processes (e.g. acceleration and higher transparency), customer satisfaction (e.g. better response times), and employee satisfaction (e.g. improved team work and increased motivation).  The paper Managing Operation Knowledge for the Metal Industry written by Sheng-Tun Li and Huang-Chih Hseih from the National Kaohsiung First University of Science Technology (Taiwan) presents a three-stage life cycle for the ontology design. The application of the resulting ontology in a metal industry company proves the effectiveness and efficiency of their approach.  In their paper Filters in the Strategy Formulation Process Leena Ilmola and Anna Kotsalo-Mustonen from Helsinki University of Technology (Finland) present a new software tool supporting strategy formulation processes. Based on three different types of filters that hinder effective knowledge flows in companies a software tool is introduced that helps overcome these filters.  Matteo Bonifacio and Alessandra Molani from University of Trento (Italy) are the authors of the paper T'),
(270,'In knowledge economy, companies and organisations build sustainable competitive advantages not only relying on their internal intellectual capital but also on the intellectual capital of other companies, organisations and institutions and specifically on those of the cluster [Porter, 1990], microcluster or territory where the company is located. This kind of intellectual capital, basically external and of a relational nature is one of the main constituents of the networked organisation and (will be called) from now on Social Capital [Nahapiet and Ghoshal, 1998] because it is embedded in the social fabric (texture) of the nearby environment.             <hr align=\"left\" width=\"50%\">            SCBS (Social Capital Benchmarking System) is both a new management method and a new management tool, that identifies, audits and benchmarks the resources and capabilities or the social capital, existing in alternative cluster locations that are necessary in order to develop the specific network organisation that each particular business model requires. The system has been successfully piloted in five European enterprises.'),
(271,'We present a novel approach of extracting a domain ontology from large-scale thesauri. Concepts are identified to be relevant for a domain based on their frequent occurrence in domain texts. The approach allows to bootstrap the ontology engineering process from given legacy thesauri and identifies an initial domain ontology that may easily be refined by experts in a later stage. We present a thorough evaluation of the results obtained in building a biosecurity ontology for the UN FAO AOS project.'),
(272,'The knowledge and the competence of the firm members are substantial success factors in the world-wide competition. For a \"Hidden Champion\" like the middle-sized manufacturer of Top-Class Concrete Pumps and Plastering Machines, Putzmeister, Inc./Germany, a systematic and anticipating Competence Development System is essential. The article describes a pilot project started in spring 2002 to gain more specific knowledge about the implementation of a strategic computer aided, employee orientated Skill Management System in the Company. The main success factors found are first, an acceptance strategy, which includes the participation of motivated groups of pilots, the integration of the workers council, the support of the management as well as much information and transparency about the objectives and the purpose of the system. Especially a good co-operation with the workers council is from great importance. Finally enough personnel and organizational resources must be given to the project.'),
(273,'This paper will study the influence of three components of human capital focusing on operative personnel under a dynamic perspective. It considers learning flows and the knowledge stocks that the employees of the organization generate because of the relationships that they maintain with their clients. The influence of individual knowledge in these learning flows will be examined. These being components such as: learning capacities, automatic and conscious knowledge, on the flows of the relational learning process including transfer, transformation and harvesting phases of knowledge. In order to study the relative importance of the individual knowledge components in each phase of the relational process, the scale established by [Kohli and Jaworski 1990] will be used in this research. The paper is structured in four parts. In the first, a theoretical reference on individual knowledge on the relational learning process will be established. In the second part, some hypothesis and the necessary methodology will be proposed. In the third part the results will be shown and finally, in the conclusions some interesting aspects on the role of individual knowledge in the process described will be shown. Conclusions are based on a study of eighty-four organizations. This investigation establishes important conclusions on the role of individual knowledge in the generation of the customer capital. Concretely, the explicit knowledge of the employees is the most meaningful in the relational learning process, although it is also true that the tacit knowledge and individual learning capacities have a special importance in the harvesting phase of knowledge.'),
(274,'Skills management has been recently acknowledged as one of the key factors to adequately face the increasing competitiveness between knowledge intensive companies.              <hr align=\"left\" width=\"50%\">            In this paper we present a formal approach to Ontology-Based Semantic Matchmaking between Skills demand and supply, devised as a virtual marketplace of knowledge. In such a knowledge market metaphor, skills are a peculiar kind of good that has distinguishing characteristics with respect to traditional assets. Buyers are entities that need the skills of people, such as projects, departments and organizations, sellers are workers that offer their own skills.             <hr align=\"left\" width=\"50%\">            The formal framework supports the semantic match of descriptions provided by demanders and sellers of skills. In particular our approach, based on Description Logics formalization and reasoning, overcomes simple subsumption matching and allows match ranking and categorization. The implementation of the approach in a prototype system, which embeds a NeoClassic reasoner, is also described.'),
(275,'Few studies attempt to measure organisational learning. Measurement is critical to evaluate relationships between initiatives to support learning and organisational performance. This paper proposes a theory-based tool kit for measurement of organisational learning. By tool kit we mean a collection of methods that each captures elements of the phenomenon  organisational learning . The paper clarifies the term and discusses requirements of theories and methods to be included in the tool kit. Some examples of theories with methods are given. Emphasis is placed on Kelly s Personal Construct Theory with the accompanying Role Construct Repertory Test to illustrate methodological requirements.'),
(276,'BEKO-SMS is a knowledge-based skill management system that combines project planning and human resource management. Application and system functions model specific skills and relationships used in a particular project. The definition of skills, skill trees, skill updating and other processes form the basis for the success factors of the SMS. We conclude that efficient project resource planning would not be possible without SMS and the skill manager.'),
(277,'Several approaches for formalising prerequisite structures on skills or competencies based on the psychological theory of knowledge space have been suggested and applied for adaptive eLearning. In this paper, we will discuss how these structures may be applied in skill management in a broader sense. After introducing some formal structures for prerequisite relationships between competencies, we will briefly present an example of an adaptive eLearning system based on this approach (APeLS). Several other aspects of the system which promise to be useful for advanced skill management are discussed. In the final part of this paper, we will discuss such broader applications of the model with respect to personal as well as to organisational skill management.'),
(278,'We present a formalisation for employee competencies which is based on a psychological framework separating the overt behavioural level from the underlying competence level. On the competence level, employees draw on action potentials (knowledge, skills and abilities) which in a given situation produce performance outcomes on the behavioural level. Our conception is based on the competence performance approach by [Korossy 1997] and [Korossy 1999] which uses mathematical structures to establish prerequisite relations on the competence and the performance level. From this framework, a methodology for assessing competencies in dynamic work domains is developed which utilises documents employees have created to assess the competencies they have been acquiring. By means of a case study, we show how the methodology and the resulting structures can be validated in an organisational setting. From the resulting structures, employee competency profiles can be derived and development planning can be supported. The structures also provide the means for making inferences within the competency assessment process which in turn facilitates continuous updating of competency profiles and maintenance of the structures.'),
(279,''),
(280,'The present state of research on competence management does not provide any suitable model that can be used in practice. Neither results from organizational nor from cognitive and social sciences meet the requirements for an application-oriented competence management completely as yet. An integrative competence management must be able to synchronise individual with organisational competencies. This linking is still neglected in research. A convenient solution has not been described yet. This article presents a model for an integrated competence management model, which gives approaches from both cognitive science and organizational science a practical framework of action.'),
(281,'Simulated learning environments provide an efficient means for improving individual skills in specific problem solving and learning situations. One crucial aspect of an optimal system for simulated training environments is its capability to keep track of the improvements of the user along the whole training process. In this paper we present a set-theoretical formal framework that can be applied for the efficient assessment of the skills of an individual in a simulated learning environment. The basic concept underlying our approach is that of a functional skill mapping of the simulated learning environment through <i>problem spaces</i>.'),
(282,'This paper analyses the Portuguese current economic situation, and points out to the decisive importance of management decisions, in order to explain the present state of affairs and to win over the massive challenges that lie ahead. The main conclusion is that, investments in training, education and skills can help Portugal to achieve success in trade. Evidence of that was obtained applying three simple economic models to statistical data on around 30 exporter economic sectors, related to the years from 1989 to 2001 (see section 4). That basic idea will be completed with three others that reinforce it: (1) the Portuguese future economic evolution will depend heavily on its external record (see section 3), (2) bad management caused Portugal to have traditionally low levels of education, skills and training (see section 3), (3) a new skills management attitude will be needed focusing in the importance of education, skills and training as real success factors (see section 5). The findings presented are somewhat preliminary, and may be extended and deepened by further and more detailed research (see section 7).'),
(283,'This paper presents an innovative approach to solve the problem of missing transparency over competencies within virtual organizations. We based our work on empirical studies on the problem to cope with the problem of competence finding in such distributed organizations. Former studies have shown that central storage of profiles is inappropriate due to missing flexibility and high costs of maintenance. The focus of our approach presented here is to support the peripheral awareness of competence-indicating events. Those events can be collected, stored and interpreted by the system without further work of the users. This idea is based on existing works on the awareness in computer-supported cooperative work scenarios.'),
(284,'As people transform data, information and experiences into shared corporate knowledge, the management of individual competencies has become increasingly important to knowledge intensive organisations (KIO). Knowledge gained during the normal execution of daily tasks is easily lost in the new and more dynamic business environment. The ability to find versatile employees and to be able to leverage their knowledge to meet differing corporate needs, is a matter of pivotal importance for KIOs. Employees  competencies, in the form of their technical and cognitive capabilities, are closely related to the ability of a company to exploit existing, and to create new, knowledge.             <hr align=\"left\" width=\"50%\">            The topic of this paper is an example of the design a particular instance of an organisational memory system: a group memory system for managing corporate competencies. The system described focuses on internal competencies, in particular human knowledge sources, their competencies, as well more straightforward project experiences and related heuristics. We will show an approach for representing and manipulating corporate competencies, and highlight the main features of ontology-driven organisational memories. This research work applies ontologies as a design approach to represent organisational knowledge and ultimately to create a consensual representation of corporate competencies.'),
(285,'Run-time monitoring of temporal properties and assertions is used for testing and as a component of execution-based model checking techniques. Traditional run-time monitoring however, is limited to observing sequences of pure Boolean propositions. This paper describes tools for observing temporal properties over time series, namely, sequences of propositions with constraints on data value changes over time. Using such Temporal Logic with time Series (TLS), it is possible to monitor important properties such as stability, monotonicity, temporal average and sum values, and temporal min/max values. The specification and monitoring of linear time temporal logic with real-time and time series constraints are supported by the Temporal Rover and the DBRover, which are in-process and remote run-time monitoring tools. The novel TLS extension described in this paper is based on practical experience and feedback provided by NASA engineers after using the DBRover to verify flight code. The paper also presents a novel hybrid approach to verify timing properties in rapid system prototyping that combines the traditional schedulability analysis of the design and the monitoring of timing constraint satisfaction during prototype execution based on a time-series temporal logic. The effectiveness of the approach is demonstrated with a prototype of the fish farm control system software.'),
(286,''),
(287,'Model-based development necessitates the transformation of models between different stages and tools of the design process. These transformations must be precisely, preferably formally, specified, such that end-to-end semantic interoperability is maintained. The paper introduces a graph-transformation-based technique for specifying these model transformations, gives a formal definition for the semantics of the transformation language, describes an implementation of the language, and illustrates its use through an example.'),
(288,'Rosetta is a systems level design language that allows algebraic specification of systems through facets. The usual approach to formally describe a specification is to define an algebra that satisfies the specification. Although it is possible to formally describe Rosetta facets with the use of algebras, we choose to use the dual of algebra, i.e. coalgebra, to do so. Coalgebras are particularly suited for describing state-based systems. This makes formally defining state-based Rosetta quite straightforward. For non-state-based Rosetta, the formalization is not as direct, but can still be done with coalgebras by focusing on the behaviors of systems specified. We use denotational semantics to map Rosetta syntactic constructs into a language understood by the coalgebras.'),
(289,'Meadows recently proposed a formal cost-based framework for the analysis of denial of service, showing how to formalize some existing principles used to make cryptographic protocols more resistant to denial of service by comparing the cost to the defender against the cost to the attacker. The firrst contribution of this paper is to introduce a new security property called <i>impassivity</i> designed to capture the abiity of a protocol to achieve these goals in the framework of a generic value-passing process algebra called <i>Security Process Algebra</i> (SPPA) extended with local function calls, cryptographic primitives and special semantic features in order to handle cryptographic protocols. Impassivity is defined as an information flow property founded on <i>bisimulation-based non-deterministic admissible interference</i>. A sound and complete proof method for impassivity is provided. The method extends previous results of the authors on bisimulation-based non-deterministic admissible interference and its application to the analysis of cryptographic protocols. It is illustrated by its application to the TCP/IP protocol. Key Words: Denial of service, Protocols, Ad'),
(290,'Two possibilities of automated CSP (Communicating Sequential Processes) support are introduced in [11] and [10] using either behavioral diagrams or application source code. While in the first approach a tool generates CSP specification from behavioral diagrams, based on UML Composite States diagram, in the second approach an application source code is translated directly into CSP specification using a compiler. This paper reviews tools related to both techniques.'),
(291,'This paper describes an ongoing implementation of an embedded hardware description language (HDL) using Haskell as a host language. Traditionally, functional HDL s are made using lazy lists to model signals, so circuits are functions from lists of input values to lists of output values. We use another known approach for embedded languages, in which circuits are data structures rather than functions. This style of implementation permits one to inspect the structure of the circuit, allowing one to perform different interpretations for the same description. The approach we present can also be applied to other domain-specific embedded languages. We provide an elegant implementation of memories and a set of new signal types.'),
(292,'This paper introduces a calculus of state_based software components modelled as concrete coalgebras for some Set endofunctors, with specified initial conditions. The calculus is parametrized by a notion of <i>behaviour</i>, introduced as a strong (usually commutative) monad. The proposed component model and calculus are illustrated through the characterisation of a particular class of components, classified as <i>separable</i>, which includes the ones arising in the so-called <i>model oriented</i> approach to systems  design.'),
(293,'Action Semantics is a framework for the formal specification of programming languages. Two different, recently proposed approaches provide modularity to the framework, allowing for specification reusability and extension. In this work, we analyze the previous approaches, and introduce <i>Object-Oriented Action Semantics</i>, a new form of modular organization of Action Semantics descriptions. Object-oriented Action Semantics does not modify the syntax in which actions are written, the addition of object-oriented features (like classes and objects) is done as an upper layer to the semantic entities and functions. A simple Pascal-like, imperative programming language is described using the formalism. The extension and reuse capabilities of Object_Oriented Action Semantics are demonstrated by adding new features to the description. The semantics of the object-oriented action notation is also presented.'),
(294,'This paper evaluates the use of AspectJ, a general-purpose aspect-oriented extension to Java, to provide adaptive behavior for J2ME applications in a modularized way. Our evaluation is based on the development of a simple but non-trivial dictionary application where new adaptive behavior was incrementally implemented using AspectJ. Our main contribution is to show that the AspectJ language can be used to implement several adaptive concerns, which allow the application to have di#erent behaviors according to changes in its environment. We also compare our implementation with corresponding pure Java alternatives, identify disadvantages of using AspectJ and propose some possible patterns.'),
(295,'We describe a programming language for distributed computations that supports mobile resources and is based on a process calculus. The syntax, semantics and implementation of the language are presented with a focus on the novel model of computation.'),
(296,''),
(297,'We present a new <i>constant propagation</i> (CP) algorithm for <i>predicated code</i>, for which classical CP-techniques are inadequate. The new algorithm works for arbitrary control flow, detects constancy of terms, whose operands are not constant themselves, and is <i>optimal</i> for acyclic code such as <i>hyperblocks</i>, the central \"compilation units\" for instruction scheduling of predicated code. The new algorithm operates on the <i>predicated value graph</i>, an extension of the well-known <i>value graph</i> of Alpern et al. [Alpern et al., 1988], which is tailored for predicated code and constructed on top of the <i>predicate-sensitive</i> SSA-form, which has been introduced by Carter et al. [Carter et al., 1999]. As an additional benefit, the new algorithm identifies off-predicated instructions in predicated code. They can simply be eliminated thereby further increasing the performance and simplifying later compilation phases such as instruction scheduling.'),
(298,'With the goal of assessing the use of the tuple space model in the context of event-driven applications, we developed a reactive tuple space in the Lua programming language. This system, which we called LuaTS, extends the original Linda model with a more powerful associative mechanism for retrieving tuples, supports code mobility and includes a reactive layer through which the programmer can modify the behavior of the basic system calls. In this paper we describe the implementation of LuaTS and illustrate its main features with a few examples.'),
(299,'Reference counting is a widely employed memory management technique, in which garbage collection operations are interleaved with computation. Standard reference counting has the major drawback of being unable to handle cyclic structures. This paper presents an important optimisation to a recently published algorithm for cyclic reference counting. Proofs of the correctness of the original and lazy algorithms are provided, together with performance figures.'),
(300,'We propose a model to support aspect-oriented programming in object-oriented languages, expressing general purpose aspects. To apply this model, the developer should implement the abstraction and composition mechanisms as well as one or more strategies defined in it. It could be applied to regular OO languages.'),
(301,'In this paper, we present XOCL, an XML-based language to represent OCL (Object Constraint Language) constraints in UML models. XOCL was designed in two steps from the UML meta-model and OCL EBNF grammar published by OMG: (1) construction of a simple OCL meta-model and (2) derivation of an XML Schema for this meta-model. XOCL applications include full interoperability among UML modelling tools as well as finely grained structured input for automatic behavioral code generation and model checking.'),
(302,'Java RMI is the computational model used to develop distributed systems in the Java language. Although widely used in the construction of distributed systems, the use of Java RMI is limited because this middleware does not allow asynchronous method invocations. This paper presents FlexRMI, a Java based system that supports asynchronous invocations of remote methods. FlexRMI is completely implemented in Java, making use of the reflection and dynamic proxy facilities of this language. The implementation is also compatible with standard Java RMI distributed systems.'),
(303,'This paper describes a practical type inference algorithm for typing poly-morphic and possibly mutually recursive definitions, using Haskell to provide a high-level implementation of the algorithm.'),
(304,'The mechanism for declaring datatypes to model data structures in programming languages such as Standard ML and Haskell can offer both convenience in programming and clarity in code. With the introduction of dependent datatypes in DML, the programmer can model data structures with more accuracy, thus capturing more program invariants. In this paper, we study some practical aspects of dependent datatypes that affect both type-checking and compiling pattern matching. The results, which have already been tested, demonstrate that dependent datatype can not only o#er various programming benefits but also lead to performance gains, yielding a concrete case where safer programs run faster.'),
(305,'This paper presents the final result of the designing of a new specification for the Haskell# Language, including new features to increase its expressiveness, but without losing either efficiency or obedience to its original premisses.'),
(306,'This article addresses the relations between ontology-based knowledge management implemented by logic-oriented knowledge representation/retrieval approaches and knowledge management using case-based reasoning. We argue that knowledge management with CBR does not only very much resemble but indeed is a kind of ontology-based knowledge management since it is based on closely related ideas and a similar development methodology, although the reasoning paradigms are different. Therefore, we conclude by proposing to merge logic-oriented and case-based retrieval and also to extend the current view of the semantic web architecture respectively.'),
(307,'Knowledge culture is one aspect in corporate culture. It describes, how knowledge is identified, acquired, developed, distributed, used and retained. There are tree levels with which the culture can be described: basic underlying assumptions, norms and values, artifacts. Based on this description it is possible to analyse the current culture and define measures to change it towards a more knowledge oriented culture. A survey on the wm03 had shown, that in most organization still exist an overlap or an ambivalence which is characterized by non-knowledge-oriented culture elements. For the change of culture the tools that are developed for cultural change must be adapted for the specific needs of knowledge cultural change.'),
(308,'Knowledge engineering emerged as a very promising area to help improve software engineering practice. One of its possible applications would be to help in solving the numerous problems that affect the software maintenance activity. Maintainers of legacy systems developed years ago with obsolete techniques and tools, and not documented, need all kinds of knowledge (application domain, programming skills, software engineering techniques, etc.) It is generally assumed that formalizing all this knowledge and recording it would be a worthwhile effort. However, research is still in a early stage and numerous questions need to be answered: What knowledge should be targeted first? Where to find this knowledge? etc. To answer these questions, one needs a precise understanding of what knowledge is at stake here. We, therefore, propose an ontology of the knowledge needed to perform software maintenance. This ontology would be most useful as a framework for future research in knowledge engineering for software maintenance.'),
(309,'Quality and process improvement programs usually require organizations to run a repository such as an experience base. However, setting up the schema of an experience base requires expert knowledge. But schema experts are not always available to support the setup of a new experience base. One promising solution is to capture their knowledge in patterns or building blocks. An initial collection of such building blocks is systematically documented in the PLEASERS (<i>P</i>roduct <i>L</i>ine <i>A</i>pproach for <i>S</i>oftware <i>E</i>ngineering <i>R</i>epositories) library. In this article we describe the underlying conceptual model of the PLEASERS schema building blocks. Schema experts can use the introduced model to create sets of schema building blocks representing their knowledge.'),
(310,'We present a software tool for examplet reuse. We define examplets to be goal-directed snippets of source code, often written for tutorial purposes, that show how to use program library facilities to achieve some task. Our tool allows users to specify both their goal (in free text) and their `situation  (the source code on which they are working). The system combines text retrieval and spreading activation through a semantic net representation of the source code.'),
(311,'A cornerstone in the integration of Knowledge Management (KM) in the business is the extension of the business strategy with a knowledge strategy. The Knowledge Strategy Process (KSP) at Siemens follows a top-down approach and helps the management to integrate knowledge strategy effectively in their business strategy. Furthermore, it brings the decision makers of a business unit on one table to draw up an action plan for their respective business unit. In six consequent steps, this action plan is generated to improve the way of working and learning by focusing on knowledge areas with highest impact on the major business ambitions. With a knowledge strategy, the pressure on impact measurements for KM is released, since sense and need for the KM program is understood and it is driven by the management. Only very reasonable cost-benefit checks will be required for larger investment plans by the business owner. An overview on diagnostics and mesurements for knowledge and KM as well as a list of open KM research issues is given for the full integration of KM into the business.'),
(312,'In our study we set the goal to consider culture as a crucial factor of learning system design. This culture oriented approach is put in concrete terms by comparing US-American and German learning programs on four different levels: layout, interaction and navigation, presentation of content, and the didactic approach. The results of a questioning on culturally specific approaches to computers complete this investigation.'),
(313,'A knowledge oriented culture is often considered as a basic infrastructure for successful Knowledge Management. Decentralized knowledge activities at Bosch follow the given company culture and do not try to affect cultural aspects at short notice. The internal analysis of knowledge culture bases on a Bosch-specific three-level model. This model is used for research activities on Bosch internal  knowledge markets . As result, a specific  Knowledge Culture Index  can be defined. This Index helps to show the potentials of knowledge management solutions in one single organizational context.'),
(314,'During the analysis of knowledge processes in enterprises it often turns out that simple access to existing enterprise knowledge which is covered in documents is not possible. To enable access to a companys  document and data stocks Information Retrieval (IR) technologies play a central role. In the following we describe the underlying theory of the SemanticMiner system, including methods and technologies as well as continuing approaches to obtain Knowledge Retrieval (KR) by dint of semantic technologies.'),
(315,''),
(316,'In this paper, we present an ontology-based approach for the improvement of searching in an information portal. The approach is based on incremental refinement of user\'s queries, according to the ambiguity of a query\'s interpretation. The so-called Librarian Agent plays the role of the human librarian in the traditional library - it uses information, about the domain vocabulary, the capacity of the knowledge repository and the behaviour of previous users in order to help users find the resources they are interested in. Moreover, the agent analyses the users  requests off-line and compares the users\' interests with the capacity of the information repository, in order to find which new topics should be introduced or which topics users are no more interested in. We partially implemented the approach in the Web Portal of our Institute and some initial evaluation results are shown.'),
(317,'Risk planning requires an organization global view, as it is strongly centered in the experience and knowledge acquired in former projects. The larger the experience of the project manager the better will be his ability in identifying risks, estimating their occurrence likelihood and impact, and defining the mitigation and contingency plans. However, project manager risk knowledge cannot stay in an individual level, but it must be made available to the organization. This paper describes an approach to risk planning in software projects based on the organizational risk knowledge reuse. A risk management process focused on the capture and utilization of organizational knowledge together with a support case tool make part of this approach. An experimental study of the relations between risk-causing facts and risks of software projects was accomplished and its results used to define such a tool.'),
(318,'The drawbacks of programming coordination activities directly within the applications software that needs them are briefly reviewed. Coordination programming helps to separate concerns, making complex coordination protocols into standalone entities, permitting separate development, verification, maintenance, and reuse. The IWIM coordination model is described, and a formal automata theoretic version of the model is developed, capturing the essentials of the framework in a fibration based approach. Specifically, families of worker automata have their communication governed by a state of a manager automaton, whose transitions correspond to reconfigurations. To capture the generality of processes in IWIM systems, the construction is generalised so that process automata can display both manager and worker traits. The relationship with other formalisations of the IWIM conception of the coordination principle is explored.'),
(319,'The operators min?, max?, and #? translate classes of the polynomial-time hierarchy to function classes. Although the inclusion relationships between these function classes have been studied in depth, some questions concerning separations remained open.             <hr align=\"left\" width=\"50%\">              We provide oracle constructions that answer most of these open questions in the relativized case. As a typical instance for the type of results of this paper, we construct a relativized world where min_P <img src=\"relativizing_function_classes/images/img1.gif\"> #?NP, thus giving evidence for the hardness of proving min?P <img src=\"relativizing_function_classes/images/img1.gif\"> #?NP in the unrelativized case.                  <hr align=\"left\" width=\"50%\">                The strongest results, proved in the paper, are the constructions of oracles D and E, such that min_coNPD <img src=\"relativizing_function_classes/images/img2.gif\"> #?PD <img src=\"relativizing_function_classes/images/img3.gif\"> NPD <img src=\"relativizing_function_classes/images/img4.gif\"> coNPD and UPE = NPE <img src=\"relativizing_function_classes/images/img3.gif\"> min?PE <img src=\"relativizing_function_classes/images/img1.gif\"> #?PE.'),
(320,'This paper presents the research work towards improving human computer interaction by providing intelligent assistance to users. This has been approached by incorporating principles of a cognitive theory in a Graphical User Interface (GUI), that deals with file manipulation and is called IFM. The cognitive theory is called Human Plausible Reasoning (HPR) and has been used to simulate users\'  reasoning in the user model of the system so that the GUI may provide spontaneous assistance to users\'  errors. Such a goal is difficult to achieve and depends heavily on the development process. However, there is a shortage of reports on the software engineering process of intelligent assistants. Moreover, in the literature of intelligent assistants there is evidence that some important phases of their development process may have been omitted and thus the understanding of delicate issues has not improved significantly. Therefore, the focus of this paper is on presenting and discussing the software engineering process of the intelligent assistant developed. Special emphasis has been put on the description of the experimental studies, which were conducted prior and after the development of the system. Theses studies were used for the specification and refinement of the overall design as well as the adaptation of HPR in it. The experimental results have shown that the intelligent assistant may follow the users\'  reasoning and provide helpful advice to a satisfactory extent as compared to human advisors.'),
(321,''),
(322,'This paper describes cognitive mechanisms that interpret elliptical instructions used in navigation. We introduce action vectors, which are defined as an agent s previous positions on routes. In addition, a new perspective system, an action-oriented perspective system is presented. In this system, the action vectors are designated to reference objects. Using theory of the action vectors and the action-oriented system, we demonstrate specific spatial configurations between objects and the action vectors, which arise in cognitive process of interpreting elliptical instructions.'),
(323,'This paper considers temporal constraints that can impose a minimum and maximum time distance between the occurrences of two events by specifying the minimum and maximum values in terms of a time granularity. When several constraints using different time granularities are part of the specification of a single problem, a reasonable question is how to convert the constraints in terms of a single granularity in order to apply standard temporal constraint algorithms. This paper investigates the problem of converting a distance constraint expressed in terms of a granularity into another one in terms of a different time granularity. An expressive formal model for time granularities is assumed including common granularities like hours and days as well as user-defined granularities like business days and academic semesters.'),
(324,'This paper proposes a general discussion of the handling of imprecise and uncertain information in temporal reasoning in the framework of fuzzy sets and possibility theory. The introduction of fuzzy features in temporal reasoning can be related to different issues. First, it can be motivated by the need of a gradual, linguistic-like description of temporal relations even in the face of complete information. An extension of Allen relational calculus is proposed, based on fuzzy comparators expressing linguistic tolerance. Fuzzy Allen relations are defined from a fuzzy partition made by three possible fuzzy relations between dates (<i>approximately equal, clearly smaller,</i> and <i>clearly greater</i>). Second, the handling of fuzzy or incomplete information leads to pervade classical Allen relations, and more generally fuzzy Allen relations, with uncertainty. The paper provides a detailed presentation of the calculus of fuzzy Allen relations (including the composition table of these relations). Moreover, the paper discusses the patterns for propagating uncertainty about (fuzzy) Allen relations in a possibilistic way.'),
(325,'This paper describes a method for constructing an abstract representation of a shape from a classical polyhedral 3D representation of an object. This framework is suitable for qualitative reasoning. As an application we use this abstract representation to compute the structural symmetries of a 3D polyhedron. The starting point of the computation is a classical polyhedral 3D representation of the object. From the Medial Axis Transform (MAT) of this object we propose a more abstract representation based on a set of spheres extracted from the MAT and structured as one or several graphs. This framework can be used for several purposes. Here we focus on the problem of finding structural symmetries of the object. We use the automorphisms group of the computed graphs. Then we propose a method to compute the automorphisms that have a geometrical sense among the set of all automorphisms. We compare the brute force algorithm with a branch and bound strategy based on the orbits partition of the vertices.'),
(326,'Logics for time intervals provide a natural framework for dealing with time in various areas of computer science and artificial intelligence, such as planning, natural language processing, temporal databases, and formal specification. In this paper we focus our attention on propositional interval temporal logics with temporal modalities for neighboring intervals over linear orders. We study the class of propositional neigh-borhood logics (<i>PNL</i>) over two natural semantics, respectively admitting and excluding point-intervals. First, we introduce interval neighborhood frames and we provide representation theorems for them, then, we develop complete axiomatic systems and semantic tableaux for logics in <i>PNL</i>.'),
(327,''),
(328,'In this paper, we present the knowledgebased system rosa working on spatial and functional organizations in agriculture. The reasoning in rosa combines hierarchical classification, case-based reasoning, and qualitative spatial reasoning. The goal of the system is twofold: formalizing and building a case base holding on farm spatial and functional organizations, and helping the analysis of new cases. Domain knowledge and cases are modeled with the help of the so-called <i>spatial organization graphs</i> (sogs), and represented within a description logic system. Hierarchical case-based reasoning, involving classification and qualitative spatial reasoning, is used to compare and explain farm spatial structures modeled by sogs. An example of case retrieval is proposed, followed by a global discussion on casebased reasoning in the rosa system and related work.'),
(329,'A qualitative representational model and the corresponding reasoning process for integrating time and topological information is developed in this paper. In the calculus presented, topological information in function of the point of the time in which it is true is represented as an instance of the Constraint Satisfaction Problem. The resulting method can be applied to qualitative navigation of autonomous agents. The model presented in this paper will help us during the path planning task by describing the sequence of topological situations that the agent should find during its way to the target objective. A preliminary result of that application has been obtained by using qualitative representation of such spatial aspects for the autonomous simulated navigation of a Nomad-200 robot, on a structured environment of an easy corridor in a building.'),
(330,'Recent attempts to perform formal knowledge representation and reasoning in cell biology have presented new challenges to spatial reasoning. In this paper we formalize two distinct notions of containment that were so motivated and which are relevant to reasoning about physical systems, a notion of being inside and a notion of being restricted. We develop a formal vocabulary for purposes of representing and reasoning about restrictive containment and formalize three kinds of accessibility that are each salient to attempts to reason about the possibility of interaction between pairs of objects in a system. We also consider the relation of this calculus to the well known Region Connection Calculus and related calculi for reasoning about containment. Finally, we discuss methods for implementing in a context of uncertainty, within a planning system and discuss an application to some simple representation and reasoning tasks in virology.'),
(331,'We discuss how a property of some region is propagated to other regions. We propose a system called <i>SRCC</i> that enables the integration of spatial and semantic data. <i>SRCC</i> can represent the relative positions of regions, properties that hold in some regions, semantic relation between regions, and so on. We define the model and describe an algorithm that checks for the existence of a model for a given set of formulas based on this model. We prove the soundness and completeness of the algorithm and apply it to an example that inspects the causality of contamination in 2D space.'),
(332,'For almost 350 years it was known that 1729 is the smallest integer which can be expressed as the sum of two positive cubes in two different ways. Motivated by a famous story involving Hardy and Ramanujan, a class of numbers called Taxicab Numbers has been defined: <i>Taxicab(k, j, n)</i> is the smallest number which can be expressed as the sum of <i>j k</i>th powers in n different ways. So, <i>Taxicab</i>(3, 2, 2) = 1729, <i>Taxicab</i>(4, 2, 2) = 635318657. Computing <i>Taxicab</i> Numbers is challenging and interesting, both from mathematical and programming points of view. The exact value of <i>Taxicab</i>(6) = <i>Taxicab</i>(3, 2, 6) is not known, however, recent results announced by Rathbun [R2002] show that <i>Taxicab</i>(6) is in the interval [10 18 , 24153319581254312065344]. In this note we show that with probability greater than 99%, <i>Taxicab</i>(6) = 24153319581254312065344.'),
(333,'Given a finite set of patterns, i.e., subsets of  <img src=\"on_identification_in_zz2/images/img1.gif\">. What is the best way to place translates of them in such a way that every point belongs to at least one translate and no two points belong to the same set of translates? We give some general results, and investigate the particular case when there is only a single pattern and that pattern is a square or has size at most four.'),
(334,'Internet services can now be used from mobile terminals. The main standard supporting this technology, WAP, will enable new services since it is compatible with network technologies like IP and UMTS. In parallel, powerful methods must be proposed to validate the underlying protocols in order to guaratee reliability and interoperability of new products. Our work, based on formal methods, contributes to WAP testing efforts by proposing an approach to the development of interoperability tests. We illustrate this approach with the design of tests suites for the WSP-protocol operating over a WAP transaction service.'),
(335,'Most informed educators agree that e-Learning should create a paradigm shift away from traditional teaching models, yet in practice this is extremely difficult to achieve. Typically, teachers use computer networks (internet or intranets) mainly for email, dissemination of information that frequently just mirrors traditional book material, assignments, and perhaps a discussion forum. In this paper, we examine reasons why there has been so little departure away from conventional teaching paradigms. We look beyond Virtual Learning Environments to Managed Learning Environments. We look at ways to make this transition a desirable option for both teachers and students. We suggest that when teachers and learners are properly supported within a Managed Learning Environment the workload of teachers is not increased and they enjoy teaching more, also, students learn better (i.e. more efficiently) and with higher motivation.'),
(336,''),
(337,'The research belonging to the Abstract State Machines approach to system design and analysis is surveyed and documented in an annotated ASM bibliography. The survey covers the period from 1984, when the idea for the concept of ASMs (under the name dynamic or evolving algebras or structures) appears for the first time in a foundational context, to the year 2001 where a mathematically well-founded, practical system development method based upon the notion of ASMs is in place and ready to be industrially deployed. Some lessons for the future of ASMs are drawn.'),
(338,'<P><B>Abstract:</B> We define a language over an algebra of words by means of a version of <I>predicative recursion</I>, and we prove that it represents a resource-free characterization of the computations performed by a register machine in time <I>O</I>(<I>n<Sup>k</sup></I>), for each denite <I>k</I>; starting from this result, and by means of a restricted form of composition, we give a characterization of the computations of a register machine with a polynomial bound simultaneously imposed over time and space complexity.</P>'),
(339,'In this paper we present the Maurer - Tochtermann Model for Knowledge Management (KM) and present strong evidence that this model has powerful ramification. First, it shows clearly that KM is not just \"old wine in new bottles\" but an important and new area of research and applications, second, it shows clearly where KM differs from classical distributed information systems or data bases, third, it is shown to embrace a number of pragmatic problems that have often been considered the heart of KM, and fourth, it gives a clear indication of the areas that will be of increasing importance for KM in the future. We claim that the model can and should be the basis of future efforts in IT-oriented KM.'),
(340,'Two of today\'s most used buzz-words in the context of software development are the terms <I>Componentware</I> and <I>Distributed Object-System</I>. The combination of both ideas is then called a <I>Distributed Component-System</I>, meaning a componentware approach where the components are distributed across the network. Today\'s approaches fulfill the application developers\' needs only partly. Also, most are more or less cumbersome to use. I want to call such part-solutions like e.g. Corba, Enterprise JavaBeans and others <I>first generation distributed component systems</I>. In fact, Corba has a different origin, but for the moment let me consider it to be a first generation componentware system, too.</P>  <P>In this paper I want to identify the requirements that have to be fulfilled to design and implement a second generation distributed component system. One main aspect is behind all of the ideas of second generation systems: <I>a good distributed component system is one that the application programmers don\'t notice</I>.</P>  <P>The open-source project <I>Dinopolis</I> is currently in its early implementation phase and can be considered the first second-generation distributed component system according to the requirements that are identified in the following. Therefore the very basic cornerstones of Dinopolis are discussed at the end of this paper.</P> </P>'),
(341,''),
(342,'We consider a uniform way of treating objects and rules in P systems: we start with multisets of rules, which are consumed when they are applied, but the application of a rule may also produce rules, to be applied at subsequent steps. We find that this natural and simple feature is surprisingly powerful: systems with only one membrane can characterize the recursively enumerable languages, both in the case of rewriting and of splicing rules, the same result is obtained in the case of symbol-objects, for the recursively enumerable sets of vectors of natural numbers.'),
(343,'A metric defined by Fine induces a topology on the unit interval which is strictly stronger than the ordinary Euclidean topology and which has some interesting applications in Walsh analysis. We investigate computability properties of a corresponding Fine representation of the real numbers and we construct a structure which characterizes this representation. Moreover, we introduce a general class of Fine computable functions and we compare this class with the class of locally uniformly Fine computable functions defined by Mori. Both classes of functions include all ordinary computable functions and, additionally, some important functions which are discontinuous with respect to the usual Euclidean metric. Finally, we prove that the integration operator on the space of Fine continuous functions is lower semi-computable.'),
(344,'Concepts of democracy have been developed and refined since Aristotle\'s time. However it is not until the new millennium that a unique set of circumstances, social, technical, and economic, have enabled a realistic plan of an e-democratic world to emerge. It is the lot of the internet generation to carry this vision through a maze of regulatory and commercial obstacles. The most important development on the internet in terms of its impact is e-commerce and the flow of support from governments and the market. It may appear that social and political aspirations for the internet have been eclipsed. However, on the contrary, global online commerce is providing opportunities for e-democracy to meet ethical and strategic challenges. The ultimate goal is to create peaceful change in key areas of global governance. We investigate some of those aspects in this paper.'),
(345,''),
(346,'<p>People have always been mobile. In the Middle Ages, masses moved because of pilgrimages. Today, these masses are called holiday travellers.</P>  <P>People have never been more mobile than today. Because of increasing prosperity and the abolishment of many borders in Europe, \"freedom\" has become a new symbol. Freedom causes movement. We have yet to overcome the borders of the countries behind the former Iron Curtain.</P>  <P>Our economy needed the division of labour, and therefore more mobility for goods and managers.</P>  <P>Moreover, liberalised markets brought about the \"global village\". In the 21st century, the global economy will be dominated by three key industries:</P>  <UL> <LI>Telecommunications</LI> <LI>Information Technology and</LI> <LI>Tourism</LI> </UL>  <P>Changes in technologies and generations are common developments. Old styles are replaced by new ones. New technologies are replacing old ones. Telecommunications and Information Technology have propelled us into what we call the Information Society. More than 50 percent of employees in developed countries are working with information.</P>  <P>The Information Society did not only bring about change. Its instruments also helped us to become more mobile. Mobility is nothing new for people. We can now lead nomadic lives without being accountable to the state.</P>'),
(347,'On the basis of an example gained from the perspective of a person reading Intellectual Capital (IC) reports this paper explains the method of BibTechMonTM which is based on an analysis of the co-occurrence of different terms within databases and the algorithm to visualise the results [Kopcsa, A., Schiebel, E. (1998b)]. The application of this method for the IC report is currently a major step in improving the IC reporting system within ARC Seibersdorf research GmbH. In this paper the advantages and potentials of using BibTechMonTM in the context of IC reporting will be demonstrated by means of the 2001 IC report of ARC Seibersdorf research GmbH.'),
(348,'This paper reports on a study examining attorneys  and law librarians  use of their memory and information they record externally in searching for, using, and sharing legal information. The paper suggests automatically and manually recording search histories and basing user interface tools on this information to support mental model building and knowledge sharing in the legal information domain. The research described is part of the author s dissertation research [1] that examined the use of search histories in legal information seeking and use, and proposed interface design recommendations for information systems. While searching for and using information, attorneys learn about legal topics and use this knowledge in their work. They create mental models and share their new knowledge with colleagues. Computers can automatically record human-computer interaction events. This information can help searchers represent and share new knowledge. The recorded information can be provided back to the user through the user interface to support searching for and using information, learning about the subject matter and sharing this knowledge with others. In this study, attorneys and law librarians were interviewed and observed to assess their use of their memory and external memory aids while searching for and using legal information. The results reported here focus on the role of interaction histories and history-based interface tools in supporting mental model development of legal information seekers of a topical area and sharing this information with other users.'),
(349,'The process of knowledge and intellectual capital management aims to improve organisational performance and efficiency. Knowledge is a distinct capability that contributes to the improvement of this efficiency. Learning is an integral part of the knowledge system and can be identified by deconstructing available organisational knowledge. This paper offers an interpretative perspective of knowledge and intellectual capital development, it also examines previously fractured contextual approaches to organisational management research, which often fail to include learning as a significant factor for both absorbing and recognising the knowledge capabilities of a firm. Based on the results from a study conducted across 140 companies as well as selected case studies, this paper investigates learning mechanisms and their role in building a firm s knowledge capabilities. This paper argues that learning is an integral part of the knowledge process in which learning acts as an endogenous factor for the development, absorption and utilisation of knowledge. The search continues for an appropriate epistemological framework in the area of management research under which organisational learning theories can be analysed while simultaneously remaining relevant and useful to the pragmatics of organisational knowledge development.'),
(350,'This article outlines new technologies in the areas of automated expertise finding, expert network discover, virtual place-based collaboration, and automated question answering. We illustrate each of these areas with implemented and in some cases empirically evaluated systems. Collectively, these illustrate new methods for automatic discovery of knowledge, experts, and communities in an effective and efficient manner.'),
(351,'This paper reports on long-term research work of recycling networks in Germany and Austria from a knowledge-based perspective. Using data from expert interviews, we discuss the key determinants of inter-organizational knowledge transfer within networks. In particular, we highlight the factor of mutual trust as important determinant of knowledge transfer in company recycling networks. One important goal of our empirical research is the institutionalization of knowledge transfer through the implementation of a central recycling agency in order to build core capabilities and to create intellectual capital.'),
(352,'Establishing electronically accessible repositories of people s capabilities, experiences, and key knowledge areas is key in setting up Enterprise Knowledge Management. A skills repository can be used for e.g. finding people, staffing, skills gap analysis, and professional development. The ontology based skills management system developed at Swiss Life uses RDF schema for storing ontologies. Its query interface is based on a combined RQL and HTML query engine.'),
(353,'The loss of an employee - voluntarily or involuntarily - represents a great risk of losing information and know how as well as breaks the natural knowledge flow. We developed the Knowledge Transfer Meeting Methodology in order to reduce the \"brain drain\" through a systematic hand-over. The Knowledge Transfer Meeting consists of five modules that support the retrieval and sharing of knowledge systematically and explicitly. The approach promotes a mentorship or partnership philosophy, motivating the leaving employee to share his or her knowledge and experience with a successor. For the implementation of the Knowledge Transfer Meeting Methodology in the company, we identify and train so-called \"facilitators\" who lead the participants through the process and hence support and spread the methodology within the company.'),
(354,'The generation of technical knowledge abounds while the underusage of existing knowledge potential remains a problem in business as well as in society. Generally speaking value can be extracted from knowledge in three ways:<br><ul><li>by exclusive use</li><li>by faster access</li><li>by better translation of public knowledge into products that yield private profit</li></ul>Each way requires different approaches to KM. But in all cases the problem of how to deal with abundance arises: It arises at the individual as well as at the level of interface design in a knowledge dividing society. First ideas to solve that problem refer to the individual rather than the interface design level:<br><ul><li>technical solutions</li><li>psychological solutions</li><li>neurological solutions</li></ul>deal with the growing gap between abundant potential knowledge and scarce human attention on the one hand and with restricted human capacity to process information on the other. For the time being a clear focus on good old virtues, such as will (focus), modesty (less is more) and courage (to decide under conditions of incomplete information and uncertainty) seem as trivial intellectually as hard to implement in practice.'),
(355,''),
(356,'Managing image data in a database system using metadata has been practiced since the last two decades. However, describing an image fully and adequately with metadata is practically not possible. The other alternative is describing image content by its low-level features such as color, texture, shape, etc. and using the same for similarity-based image retrieval. However, practice has shown that using only the low-level features can not as well be complete. Hence, systems need to integrate both low-level and metadata descriptions for an efficient image data management. However, due to lack of adequate image data model, absence of a formal algebra for content-based image operations, and lack of precision of the existing image processing and retrieval techniques, no much work is done to integrate the use of low-level and metadata description and retrieval methods. In this paper, we first present a global image data model that supports both metadata and low-level descriptions of images and their salient objects. This allows to make multi-criteria image retrieval (context-, semantic-, and content-based queries). Furthermore, we present an image data repository model that captures all data described in the model and permits to integrate heterogeneous operations in a DBMS. In particular, similarity-based operations (similarity-based join and selection) in combination with traditional ones can be carried out. Finally, we present an image DBMS architecture that we use to develop a prototype in order to support both content-based and metadata retrieval.'),
(357,'Starting from text corpus analysis with linguistic and statistical analysis algorithms, an infrastructure for text mining is described which uses collocation analysis as a central tool. This text mining method may be applied to different domains as well as languages. Some examples taken form large reference databases motivate the applicability to knowledge management using declarative standards of information structuring and description. The ISO/IEC Topic Map standard is introduced as a candidate for rich metadata description of information resources and it is shown how text mining can be used for automatic topic map generation.'),
(358,'In this paper, we criticise the objectivistic approach that underlies most current systems for Knowledge Management. We show that such an approach is incompatible with the very nature of what is to be managed (i.e., knowledge), and we argue that this may partially explain why most knowledge management systems are deserted by users. We propose a different approach - called distributed knowledge management - in which subjective and social (in a word, contextual) aspects of knowledge are seriously taken into account. Finally, we present a general technological architecture in which these ideas are implemented by introducing the concept of knowledge node.'),
(359,'The main goal of this research is to improve Information Retrieval Systems by enabling them to generate search outcomes that are relevant and customized to each specific user. Our proposal advocates the use of Instance Based Reasoning during the information retrieval process.<br>  <P>When conducting a search, the system retrieves a previous similar search experience and traces back previous human reasoning and behavior and then replicates it in the current situation. Thus, user information retrieval experiences or instances are saved to be reused in future similar cases. The resulting cooperative memory is used for user query expansion.<br>  <P>In order to improve the information retrieval experience, we propose to conceptualize and model both the user profile, and the information retrieval process. This leads us to define some similarity functions between user profiles and information retrieval situations. The reuse of past experiences serves to enrich the initial user query by words from documents found in similar cases. Unlike the classical <I>Rocchio</I> method, these documents are those already judged as valid by users with similar profile and in similar search situation. The value this method brings to the user is an icreasing relevance of the search outcomes while reducing user interaction with the system.<br>  <P>This method has been implemented in the COSYDOR (Cooperative System for Document Retrieval) prototype based on <I>Intermedia</I> (Oracle 8i). Tests and evaluations have been performed on the COSYDOR prototype using the test corpus of TREC (Text Retrieval Converence) and its standard procedures for performance analysis and benchmarking. The results of these analyses show a significant improvement of performance in the first search iterations compared to the <I>Intermedia</I> benchmark.'),
(360,'In IT-supported knowledge management (KM), the software user interface is at the boundary between persons and the knowledge management system (KMS). It plays a central role because seen from the users point of view, the user interface is the system. This paper presents a case study in which a particular User Interface Design methodology was employed to design a prototype KMS user interface for an inbound call center. In this example, we combine knowledge re-use and expert location.'),
(361,'There is an immense number of information resources on the Internet that can be utilized free of charge. So many knowledge workers try to make use of this information in their daily tasks. Nevertheless, it is very hard to find the relevant information in the Internet by using the full-text retrieval techniques which are offered by most existing search engines. <br><br> This paper demonstrates that Thesauri, which have been used in established online retrieval systems for a long time, also open up new methods for the automated search for information in the Internet. In addition, thesaurus-like structures known as Gazetteers allow handling geographical references of information resources in a very effective way. The knowledge represented in thesauri and gazetteers can be used to process a variety of thematic and geographical queries and to retrieve the information of interest from the Internet. Comfortable ways of specifying queries can be offered to the users, e.g., by navigating in a hierarchical tree of descriptors, by using synonymous, related or foreign-language terms rather than fixed elements of a controlled vocabulary, or by indicating a geographical region of interest on a cartographic map. <br><br> In addition to the general principles, examples of powerful query processors and advanced user interfaces are presented which demonstrate the effective usage of the knowledge stored in thesauri and gazetteers. The implemented solutions turn out to be considerably more comfortable than the \"black box search\" offered by most existing library catalogs and Internet search engines.'),
(362,'<p>Software quality management and quality assurance are disciplines that require substantial knowledge of the methods and techniques to be applied. More important than a solid knowledge of methodology, however, is the ability to judge feasibility of approaches, and to tailor activities to the business unit culture and constraints. Software quality activities must be carefully integrated into an existing company or business culture. Making informed decisions requires more than knowledge - it calls for experience of what works and what does not work in a given environment. Experienced quality agents are a scarce resource. Exploiting a scarce resource - like experiences in software quality - more effectively is a straight-forward concept.</P>  <P>Five years ago, DaimlerChrysler set up a large research project with business units, called SEC (Software Experience Center). Its purpose was to explore opportunities for learning from experiences within and across different business units. Unlike more general approaches of knowledge management, SEC was entirely devoted to software processes: software development, software acquisition, and in particular software quality in both development and acquisition settings.</P>  <P>However, not all expectations that are often related to experience exploitation are realistic. In SEC, some of our initial expectations were met, others were not. This talk reports and reflects on our attempts to capture, engineer, and reuse experiences in the realm of software quality and software process improvement.</P> </P>'),
(363,'New wireless protocols like W-LAN and Bluetooth allow establishing spontaneous networks and peer-to-peer exchange of information. At the same time standards like Semantic Web and Topic Maps gain acceptance that add semantics to information. This paper introduces Shark. Shark is an acronym and stands for Mobile Shared Knowledge?. Shark organizes knowledge with help of Topic Maps, synchronizes knowledge inside closed user groups but also enables a peer-to-peer exchange of knowledge by means of Bluetooth. This paper gives an overview of the system and its communication protocols.'),
(364,'Community web sites exhibit the property that multiple content providers exist. Of course, any portal is only as useful as the quality and amount of its content. Developing original content is time consuming and expensive. To offset the cost, we present a novel framework, viz. SEAL (SEmantic portAL), that builds on Semantic Web standards. We illustrate our approach with examples from the OntoWeb community portal. Community web sites exhibit two dominating properties: They often need to integrate many different information sources and they require an adequate web site management system. SEAL exploits ontologies for fulfilling the requirements set forth by these two properties. Ontologies provide a high level of sophistication for web information integration as well as for web site management.'),
(365,''),
(366,'Vast collections of video and audio recordings which have captured events of the last century remain a largely untapped resource of historical and scientific value. The Informedia Digital Video Library has pioneered techniques for automated video and audio indexing, navigation, visualization, search and retrieval and embedded them in a system for use in education and information mining. In recent work we introduce new paradigms for knowledge discovery by aggregating and integrating video content on-demand to enable summarization and visualization in response to queries in a useful broader context, starting with historic and geographic perspectives.'),
(367,'This paper presents a framework for representing formal semantics of a subset of the Unified Modeling Language (UML) notation in a higher-order logic, more specifically semantics of UML sequence diagrams is encoded into the Prototype Verification System (PVS). The primary objective of our work is to make UML models amenable to rigorous analysis by providing their precise semantics. This approach paves a way for formal development of systems through a systematic transformation of UML models. This work is a part of a long-term vision to explore how the PVS tool set can be used to underpin practical tools for analyzing UML models. It contributes to the ongoing effort to provide mathematical foundation to UML notations, with the aim of clarifying the semantics of the language as well as supporting the development of semantically-based tools.'),
(368,'ATM virtual path (VP) contains bundles of virtual channels (VCs). A VP layer network can be used as a server layer network of VC layer networks and each VC layer network can be a client layer. Therefore the effective provision of VC services can be achieved by a better routing scheme of the VP layer network. However, the traditional hierarchical routing scheme of PNNI signaling protocol does not provide the globally optimal route in hierarchical transport network due to its successive network partitioning and topology abstraction. We propose a new VP routing scheme suitable for a nation-wide hierarchical transport network and a network model suitable for the scalable VP network management system. The routing algorithm can provide the globally optimal route in the hierarchical network environment from the perspectives of maximization of network resource utilization and satisfaction of the end user s QoS requirement. In addition, we describe the implementation model of the ATM virtual path network management system (VP-NMS). Lastly, we show the routing performance evaluated in the High Speed Information Network (HSIN) of Korea Telecom.'),
(369,'By introducing a new operation, the exponentiation of formal languages, we can define Heyting algebras of formal languages. It turns out that some well known families of languages are closed under this exponentiation, e. g., the families of regular and of context-sensitive languages.'),
(370,''),
(371,'Major companies, especially banks, invest in interactive distance learning replacing face-to-face training. Research has shown learning gains are mostly due to a shift in instruction. In this study, a WBT about currency management of a major German bank was examined. The communicational features of the WBT comprise a discussion forum, note taking, and automatic messaging of questions and answers between experts and students. The experimental design compared a face-to-face seminar with WBT learning. The results show that WBT participants learned as much as the seminar participants, but in about 70% of the seminar s study time. Young seminar participants performed better than older ones, while WBT learning did not produce an age effect. The results of the study demonstrate that the learners in the bank tend to choose traditional learning strategies, they do not cope optimally with co-operative and selective learning strategies, and they tend to appreciate audio-visual media. Experts did not voluntarily play an active role in the discussion processes. Communicational features, however, were used quite frequently. The users who were experienced in using a CBT and showed high self esteem gained most from WBT learning.'),
(372,'Many believe that today s Web has not yet reached the full potential which globally distributed systems may achieve in terms of information access and use. Realizing this potential may indeed turn the Web into a vast knowledge and service space. We discuss some of the issues involved and present a number of activities initiated and supported by the European Commission that are likely to make significant contributions towards attaining this goal.'),
(373,''),
(374,'Global optimization methods in connection with interval arithmetic permit to determine an accurate enclosure of the global optimum, and of all the corresponding <i>optimizers</i>. One of the main features of these algorithms consists in the construction of an interval function which produces an enclosure of the range of the studied function over a box (right parallelepiped). <br>      We use here affine arithmetic in global optimization algorithms, in order to elaborate new inclusion functions. These techniques are implemented and then discussed. Three new affine and quadratic forms are introduced. On some polynomial examples, we show that these new tools often yield more efficient <i>lower bounds</i> (and <i>upper bounds</i>) compared to several well-known classical inclusion functions. The three new methods, presented in this paper, are integrated into various Branch and Bound algorithms. This leads to improve the convergence of these algorithms by attenuating some negative effects due to the use of interval analysis and standard affne arithmetic.'),
(375,'JPlag is a web service that finds pairs of similar programs among a given set of programs. It has successfully been used in practice for detecting plagiarisms among student Java program submissions. Support for the languages C, C++ and Scheme is also available. We describe JPlag\'s architecture and its comparsion algorithm, which is based on a known one called Greedy String Tiling. Then, the contribution of this paper is threefold: First, an evaluation of JPlag\'s performance on several rather different sets of Java programs shows that JPlag is very hard to deceive. More than 90 percent of the 77 plagiarisms within our various benchmark program sets are reliably detected and a majority of the others at least raise suspicion. The run time is just a few seconds for submissions of 100 programs of several hundred lines each. Second, a parameter study shows that the approach is fairly robust with respect to its configuration parameters. Third, we study the kinds of attempts used for disguising plagiarisms, their frequency, and their success.'),
(376,''),
(377,'Nielsen, Rozenberg, Salomaa and Skyum have shown that HD0L languages are CPDF0L languages. We will generalize this result for formal power series. We will also give a new proof of the result of Nielsen, Rozenberg, Salomaa and Skyum.'),
(378,'WordNet (WN) is a lexical knowledge base, first developed for English and then adopted for several Western European languages, which was created as a machine-readable dictionary based on psycholinguistic principles. Our paper is an attempt to discuss the semiautomatic generation of WNs for languages other than English, a topic of great interest since the existence of such WNs will create the appropriate infrastructure for advanced Information Technology systems. Extending the algorithmic approach proposed in [Nikolov and Petrova, 01] we introduce a semiautomatic method based on heuristics for generating noun and adjective synsets and clusters. This choice of involved parts of speech is determined by the fact that nouns and adjectives have completely different organizations in WN: the hierarchy and the N-dimensional hyper-space respectively. Our approach to WN generation relies on so-called \"class methods\", namely it uses as knowledge sources individual entries coming from bilingual dictio naries and WN synsets, but at the same time demonstrates the need to combine such methods with structural ones.'),
(379,''),
(380,'This paper describes the architecture of a route finding system that computes an optimal route between two given locations efficiently and that considers user preferences when doing so. The basis of the system is an A* algorithm that applies heuristics such as the air distance heuristic or the Manhattan heuristic to compute the shortest path between the two locations. Since A* is not tractable in general, island search is used to divide the problem into smaller problems, which can be solved more easily. In addition to that, search control rules are introduced to express user preferences about the routes to be considered during the search.'),
(381,''),
(382,'Our work is concerned with the design of a knowledge-based system for recognizing agricultural landscape models on land-use maps. Landscape models are defined as sets of spatial structures and spatial relations. This paper focuses on the representation of topological relations inside an object-based representation system. In this system, relations are represented by objects with their own properties. We propose to define two types of properties: the first ones are concerned with relations as concepts while the second are concerned with relations as links between concepts. In order to represent the second type of properties, we have defined facets that are inspired from the constructors of description logics. We describe these facets and how they are used for classifying spatial structures and relations on land-use maps. The paper ends with a discussion on the present work and related work in qualitative spatial reasoning.'),
(383,'Suppose a group of mobile agents situated in some Euclidean space does not have any idea on where they are exactly located within that space. However, they do have some notion about their relative positions with respect to each other. This problem may be formulated as a multi-dimensional point-based qualitative reasoning problem with disjunctive constraints. In this article we have developed a set of incremental algorithms for finding feasible positions of a new agent relative to the other existing agents (located in 1D, 2D and the generalized d-D dimensional space for d_=1), given some qualitative spatial constraints between the new one and the other agents. Our approach is a <i>domain-theoretic</i> one, similar to that used in the traditional constraint-based reasoning works (CSP). This approach differs from the algebraic approach - that is traditionally deployed in the spatio-temporal reasoning areas. We have also obtained some tractability results here for the full binary constraint satisfaction problem (rather than the incremental problem, which is polynomial) based on a notion of strong pre-convexity. The article also hints toward many future directions for this work.'),
(384,'The formalizations of periods of time inside a  linear model of Time are usually based on the notion of intervals, that may contain or may not their endpoints. This is not enough when the periods are written in terms of coarse granularities with respect to the event taken into account. For instance, how to express the inter-war period in terms of a years interval? This paper presents a new type of intervals, neither open, nor closed or open-closed and the extension of operations on intervals of this new type, in order to reduce the gap between the discourse related to temporal relationship and its translation into a discretized model of Time.'),
(385,'This paper presents a complete system for  scheduling transportation orders to a fleet of autonomous mobile robots in service environments. It consists of the autonomous mobile robots, a user friendly interface to acquire the orders for the robots via internet and to store them in a database, a general language for modeling multistorey buildings with XML and the scheduling algorithms. The model description of the buildings is used to plan the paths for the robots and to estimate the cost and times for the orders. One challenging key problem - the multi robot cooperation - is solved by the scheduling algorithms and by giving <i>autonomy</i> to the service robots.'),
(386,'To use hypertext/hypermedia elements in teaching at universities an author not only needs knowledge of the technological possibilities. In addition he/she has to renew a here so called mental model of the relevant aspects of the material to be transferred. For the author, this last mentioned problem seems to be a central and often very complicated and time consuming one.'),
(387,'Different models and methodologies for the development of hypermedia systems and applications have emerged in the recent years. Software-technical methods and principles enriched with ideas mainly driven from the applications  needs are often sponsor to those models and methodologies. Hence, they deal with very specific problems occurring in the hypermedia domain, thereby extending design notations like UML or State Charts and adapting them to modeling this domain. In the present paper, we propose a very usual software-technical approach to the development of hyperlink structures which form the basis for navigation in hyperdocuments. Our approach uses standard UML, algebraic specification and object-oriented implementation to cover the construction of hyperlink structures, from design through to specification and realization. We thereby equate the development of hypermedia documents with usual software development. Instead of adopting software-engineering and notations to hypermedial concerns, we adopt the latter to the former and show the advantages of this approach.'),
(388,'Being lost in space and overloaded with information are two key problems users are confronted with, when searching for appropriate information. Trails built from information about the users\' browsing paths and activities, are an established approach to assist users in navigating vast information spaces. However, existing trail-based systems are focusing on browsers only and therefore do not fully exploit the notion of trails. The <i>TrailTRECer</i> framework addresses these issues by being open to any application and any activity. The usability of the framework and the concept of user trails were tested by building a navigation support system with different trail-enabled clients.'),
(389,'Structural computing is one of the most recent  research threads to emerge in the field of hypermedia. Though a relatively new line of study, research results have already started to emerge in the structural computing field. This paper examines a number of structural computing research projects to provide an overview of the current state of the field as well as a look at the direction of ongoing projects. It also briefly discusses additional areas of research in structural computing that will be important to consider as research in the field continues.'),
(390,'The evolution of the Web is not only accompanied by an increasing diversity of multimedia but by new requirements towards intelligent research capabilities, user specific assistance, intuitive user interfaces and platform independent information presentation. To reach these and further upcoming requirements new standardized Web technologies and XML based description languages are used. The Web Information Space has transformed into a Knowledge marketplace where worldwide located participants take part into the creation, annotation and consumption of knowledge. This paper points out the design of semantic retrieval frameworks and a prototype implementation for audio and video annotation, storage and retrieval using the MPEG-7 standard and semantic web reference implementations. MPEG-7 plays an important role towards the standardized enrichment of multimedia with semantics on higher abstraction levels and a related improvement of query results.'),
(391,'Design for All is an important challenge for hypermedia engineering. We analyze this challenge and show that it is necessary to find a way of describing partially designed hypermedia documents that can then be transformed into different hypermedia applications according to user needs and call this concept \"semi-documents\". We sketch similarities and differences to existing formalisms and conclude that there are three areas in which functional languages can make a contribution: the development of an embedded special-purpose language for describing semi-documents, the building of generators which produce hypermedia applications from semi-document, and the realization of support tools for the development of semi-documents.'),
(392,'On the background of rising Intranet applications the automatic generation of adaptable, context-sensitive hypertexts becomes more and more important [El-Beltagy et al., 2001]. This observation contradicts the literature on hypertext authoring, where Information Retrieval techniques prevail, which disregard any linguistic and context-theoretical underpinning. As a consequence, resulting hypertexts do not manifest those schematic structures, which are constitutive for the emergence of text types and the context-mediated understanding of their instances, i.e. natural language texts. This paper utilizes <i>Systemic Functional Linguistics</i> (SFL) and its context model as a theoretical basis of hypertext authoring. So called <i>Systemic Functional Hypertexts</i> (SFHT) are proposed, which refer to a stratified context layer as the proper source of text linkage. The purpose of this paper is twofold: First, hypertexts are reconstructed from a linguistic point of view as a kind of supersign, whose constituents are natural language texts and whose structuring is due to intra- and intertextual coherence relations and their context-sensitive interpretation. Second, the paper prepares a formal notion of SFHTs as a first step towards operationalization of fundamental text linguistic concepts. On this background, SFHTs serve to overcome the theoretical poverty of many approaches to link generation.'),
(393,'This paper discusses some experiences with the development of features based on natural language processing for a multimedia encyclopedia.'),
(394,''),
(395,'Document structures are a crucial mechanism for the creation and the usability of complex hypermedia documents. They form a possibility to deal with the inherent complexity of such documents and with document structures it is also possible to support the reuse of parts of hypermedia documents. In several theoretical approaches different kinds of document structures have been proposed. For example in the Dexter Hypertext Model or in the hypermedia model developed by Klaus Tochtermann. <br><br> In the creation process of such hypermedia documents, which is strongly influenced by the offered functionality of the existing editors and tools, only simple kinds of structures could presently be used. Furthermore the use of hypermedia documents is often somehow connected to special system requirements, which makes it difficult to use these documents in a network. Especially the use of such hypermedia documents in the internet with all its different platforms and operating systems still cause many difficulties. The profit of hypermedia documents could obviously be increased, if broad forms of structuring could be used to build hypermedia documents and when these documents fulfill at the time the demands of interoperability and platform independency. <br><br> This papers presents a contribution to this topic by introducing techniques for the implementation of structured hypermedia documents, which fulfill the demands of system and platform independency. These techniques are consequently based on the Extensible Markup Language. To form the basis for an XML-based implementation of structured hypermedia documents, the concepts of the Tochtermann model were transformed into a XML document type definition. Because we understand the process of creating a hypermedia document as an integrative process, not only the document type definition itself is described, but also the aspects of displaying such a XML-based hypermedia document. Due to the continuous use of XML conform techniques the developed HMDoc hypermedia documents are platform and system independent and can therefore be easily used in networks like the internet.'),
(396,'In this paper we propose several significant advances in online employment services. We address issues such as privacy, interaction, and scalability to worldwide services. The details of each user are managed in completely relevant-to-service formats and both jobseekers & employers have considerable control over just how many of their own personal details are visible at any time. Perhaps most interestingly, virtual connections are maintained that support the various stages of negotiation in iterative, computer-supported cycles.'),
(397,'<p>Observation is a fundamental interaction pattern in today\'s computer-based systems. Adopting observation as the main modelling criterion, computer-based systems can be represented as composed by three class of entities: <i>observers, observables</I> (or <i>sources</I>), and <i>coordinators</I>, that is, the entities managing the observer/source interaction.</P>  <P>Also, <i>agents</I> and agent <i>societies</I> are fundamental abstractions in modelling today\'s complex systems. When exploiting observation in the context of agent-based systems, the most natural interpretation for agents is to see them as either observers or coordinators. However, their situatedness and autonomy, their peculiar perception and representation of the environment, and their typical ability to infer new knowledge - in short, their <i>individual viewpoint</I> over the world -, make agents suitable for an interpretation as observable sources.</P>  <P>Accordingly, this paper discusses the implications of using observation to model agent systems, and focuses on the interpretation of agents as observables. A formal framework is developed where multiagent systems are modelled as the composition of agents interacting by observing each other and by mutually affecting their observable behaviour.</P> </P>'),
(398,'[unknown]'),
(399,'We introduce counter synchronized contextfree grammars and investigate their generative power. It turns out that the family of counter synchronized contextfree languages is a proper superset of the family of contextfree languages and is strictly contained in the family of synchronized contextfree languages. Moreover, we establish the space and time complexity of the fixed membership, the general membership, and the nonemptiness problem for synchronized and counter synchronized contextfree languages and solve the mentioned complexity questions in terms of completeness results for complexity classes. In this way we present new complete problems for LOG(<B>CF</B>), <B>NP</B>, and <B>PSpace</B>. It is worth to mention that the main theorem on the <B>PSpacecompleteness</B> of the general membership problem of synchronized contextfree grammars relies on a remarkable normal form for these grammars, namely for every synchronized contextfree grammar one can effectively construct and equivalent grammar of same type without nonsynchronizing nonterminals, except the axiom.  </P> <hr align=\"left\" width=\"50%\"> 1.) C. S. Calude, K. Salomaa, S. Yu (eds.). Advances and Trends in Automata and Formal Languages. A Collection of Papers in Honour of the 60th Birthday of Helmut Jrgensen.'),
(400,'We study additive distances and quasi-distances  between words. We show that every additive distance is finite. We then prove that every additive quasi-distance is regularity-preserving, that is, the neighborhood of any radius of a regular language with respect to an additive quasi-distance is regular. Finally, similar results will be proven for context-free, computable and computably enumerable languages.      <hr align=\"left\" width=\"50%\">      1.) C. S. Calude, K. Salomaa, S. Yu (eds.). Advances and Trends in Automata and Formal Languages. A Collection of Papers in Honour of the 60th Birthday of Helmut Jrgensen.'),
(401,''),
(402,'In a database with categorical attributes, each attribute  defines a partition whose classes can be regarded as natural clusters of rows. In this paper we focus on finding a partition of the rows of a given database, that is as close as possible to the partitions associated to each attribute. We evaluate the closeness of two partitions by using a generalization of the classical conditional entropy. From this perspective, we wish to construct a partition (referred to as the median partition) such that the sum of the dissimilarities between this partition and all the partitions determined by the attributes of the database is minimal. Then, the problem of finding the median partition is an optimization problem, over the space of all partitions of the rows of the database, for which we give an approximative solution. To search more e#ciently the large space of possible partitions we use a genetic algorithm where the partitions are represented by chromosomes. Our genetic algorithm obtains better clustering results than the classical k-means algorithm.      <hr align=\"left\" width=\"50%\">      1.) C. S. Calude, K. Salomaa, S. Yu (eds.). Advances and Trends in Automata and Formal Languages. A Collection of Papers in Honour of the 60th Birthday of Helmut Jrgensen.'),
(403,'In this paper we introduce the concept of the quasi-product of tree automata. In a quasi-product the inputs of the component tree automata are operational symbols in which permutation and unification of variables are allowed. It is shown that in sets of tree automata which are homomorphically complete with respect to the quasi-product the essentially unary operations play the basic role among all operations with nonzero ranks. Furthermore, we give a characterization of homomorphically complete sets which is similar to the classical one.      <hr align=\"left\" width=\"50%\">      1.) C. S. Calude, K. Salomaa, S. Yu (eds.). Advances and Trends in Automata and Formal Languages. A Collection of Papers in Honour of the 60th Birthday of Helmut Jrgensen.'),
(404,'Over the last 30 years or so many results have appeared on the descriptional complexity of machines with limited resources. Since these results have appeared in a variety of different contexts, our goal here is to provide a survey of these results. Particular emphasis is put on limiting resources (e.g., nondeterminism, ambiguity, lookahead, etc.) for various types of finite state machines, pushdown automata, parsers and cellular automata and on the effect it has on their descriptional complexity. We also address the question of how descriptional complexity might help in the future to solve practical issues, such as software reliability.      <hr align=\"left\" width=\"50%\">      1.) C. S. Calude, K. Salomaa, S. Yu (eds.). Advances and Trends in Automata and Formal Languages. A Collection of Papers in Honour of the 60th Birthday of Helmut Jrgensen.'),
(405,'Let b  2 be an integer. A real number is called simply normal to base b if in its representation to base b every digit appears with the same asymptotic frequency. We answer the following question for arbitrary integers a, b  2:if a real number is simply normal to base a, does this imply that it is also simply normal to base b? It turns out that the answer is different from the wellknown answers to the corresponding questions for the related properties normality?, disjunctiveness?, and randomness?.      <hr align=\"left\" width=\"50%\">      1.) C. S. Calude, K. Salomaa, S. Yu (eds.). Advances and Trends in Automata and Formal Languages. A Collection of Papers in Honour of the 60th Birthday of Helmut Jrgensen.'),
(406,'Based on the general operation <img src=\"word_operation_closure_and/images/img1.gif\"> of words, called bw_operation, the notions of <img src=\"word_operation_closure_and/images/img1.gif\">primitive words, <img src=\"word_operation_closure_and/images/img1.gif\">closed languages, <img src=\"word_operation_closure_and/images/img1.gif\">bases of languages and operation_left_quotient_closed languages are defined and investigated. These notions turn out to be generalizations of the classical notions of primitive words, plus_closed (star_closed) languages, minimal generating sets and deletion_closed languages. Properties of the set of all <img src=\"word_operation_closure_and/images/img1.gif\">primitive words, the <img src=\"word_operation_closure_and/images/img1.gif\">bases of non_empty languages, right <img src=\"word_operation_closure_and/images/img1.gif\">residuals and operation_left_quotient closed languages are studied under the general concept of word operation. Properties of bi_catenation and related languages are discussed as examples and also by their own interests.      <hr align=\"left\" width=\"50%\">      1.) C. S. Calude, K. Salomaa, S. Yu (eds.). Advances and Trends in Automata and Formal Languages. A Collection of Papers in Honour of the 60th Birthday of Helmut Jrgensen.'),
(407,'Let A <img src=\"shuffle_decomposition_of_regular/images/img1.gif\">  X* be a regular language. In the paper, we will provide an algorithm to decide whether there exist a nontrivial language B <img src=\"shuffle_decomposition_of_regular/images/img2.gif\"> <img src=\"shuffle_decomposition_of_regular/images/img4.gif\">(n, X) and a nontrivial regular language C <img src=\"shuffle_decomposition_of_regular/images/img1.gif\"> X* such that A = B <img src=\"shuffle_decomposition_of_regular/images/img3.gif\"> C      <hr align=\"left\" width=\"50%\">      1.) C. S. Calude, K. Salomaa, S. Yu (eds.). Advances and Trends in Automata and Formal Languages. A Collection of Papers in Honour of the 60th Birthday of Helmut Jrgensen.'),
(408,'With any Petri net we associated its CPN language which consists of all sequences of transitions which reach a marking with an empty place whereas all proper prefixes of the sequence lead to positive markings.  We prove that any CPN language can be accepted by a partially blind multicounter machine, and that any partially blind multicounter language is the morphic image of some CPN language. As a corollary we obtain the decidability of membership, emptiness and finiteness problem for CPN languages. We characterize the very strictly bounded regular languages, which are CPN languages, and give a condition for a Petri net, which ensures that its generated language is regular. We give a dense CPN language and prove that no dense regular language is a CPN language.      <hr align=\"left\" width=\"50%\">      1.) C. S. Calude, K. Salomaa, S. Yu (eds.). Advances and Trends in Automata and Formal Languages. A Collection of Papers in Honour of the 60th Birthday of Helmut Jrgensen.'),
(409,'<p>Let <I>G</I> = (<I>V</I>, <I>E</I>) be a strongly connected and aperiodic directed graph of uniform out-degree <I>k</I>. A deterministic finite automaton is obtained if the edges are colored with <I>k</I> colors in such a way that each vertex has one edge of each color leaving it. The automaton is called synchronized if there exists an input word that maps all vertices into the same fixed vertex. The road coloring conjecture asks whether there always exists a coloring such that the resulting automaton is synchronized. The conjecture has been proved for various types of graphs but the general problem remains open. In this work we investigate a related concept of stability, using techniques of linear algebra. We have proved in our earlier papers that the road coloring conjecture is equivalent to the conjecture that each strongly connected and aperiodic graph has a coloring where at least one pair of states is stable. In the present work we prove that stable pairs of states exist in all automata that are almost balanced in the sense that there is at most one state for each color where synchronization can take place. </P>      <hr align=\"left\" width=\"50%\">      1.) C. S. Calude, K. Salomaa, S. Yu (eds.). Advances and Trends in Automata and Formal Languages. A Collection of Papers in Honour of the 60th Birthday of Helmut Jrgensen.'),
(410,'When the words of a language are communicated via a noisy channel, the language property of error-detection ensures that no word of the language can be transformed to another word of the language. On the other hand, the property of error-correction ensures that the channel cannot transform two different words of the language to the same word. In this work we use transducers to model noisy channels and consider a few simple transducer operations that can be used to reduce the language properties of error-detection and error-correction to the transducer property of functionality. As a consequence, we obtain simple polynomial-time algorithms for deciding these properties for regular languages. On the other hand the properties are not decidable for context-free languages. In addition we show that, in a certain sense, the class of rational channels can be used to model various error combinations. Using the same tools, we also obtain simple polynomial-time algorithms for deciding whether a given regular language is thin and whether a given regular code has decoding delay d, for given d, and for computing the minimum decoding delay of a given regular code.      <hr align=\"left\" width=\"50%\">      1.) C. S. Calude, K. Salomaa, S. Yu (eds.). Advances and Trends in Automata and Formal Languages. A Collection of Papers in Honour of the 60th Birthday of Helmut Jrgensen.'),
(411,'Infinite words on a finite non-empty alphabet have been investigated in various respects. We will consider here two important strategies in approaching such words; one of them proceeds from particular to general, while the other proceeds from general to particular. As we shall see, the respective hierarchies dont interfer. There is between them an empty space waiting for investigation.      <hr align=\"left\" width=\"50%\">      1.) C. S. Calude, K. Salomaa, S. Yu (eds.). Advances and Trends in Automata and Formal Languages. A Collection of Papers in Honour of the 60th Birthday of Helmut Jrgensen.'),
(412,'In this paper, we study the number of tilings of the hyperbolic plane that can be constructed, starting from a single pentagonal tile, the only permitted transformations on the basic tile being the replication by displacement along the lines of the pentagrid. We obtain that there is no such tiling with five colours, that there are exactly two of them with four colours and a single trivial tiling with one colour. For three colours, the number of solutions depends of the assortment of the colours. For half of them, there is a continuous number of such tilings, for one of them there are four solutions, for the two other ones, there is no such tiling. For two colours, there is always a continuous number of such tilings.      <br><br>      By contrast, there is no such analog in the euclidean plane with the similar constraints.      <hr align=\"left\" width=\"50%\">      1.) C. S. Calude, K. Salomaa, S. Yu (eds.). Advances and Trends in Automata and Formal Languages. A Collection of Papers in Honour of the 60th Birthday of Helmut Jrgensen.'),
(413,'A purely communicative variant of P systems was considered recently, based on the trans-membrane transport of couples of chemicals. When using both symport rules (the chemicals pass together in the same direction) and antiport rules (one chemical enters and the other exits a membrane), one obtains the computational completeness, and the question was formulated what happens when only symport rules are considered. We address here this question. First, we surprisingly find that \"generalized\" symport rules are sufficient: if more than two chemicals pass together through membranes, then we get again the power of Turing machines. Three results of this type are obtained, with a trade-off between the number of chemicals which move together (at least three in the best case) and the number of membranes used. The same result is obtained for standard symport rules (couples of chemicals), if the passing through membranes is conditioned by some permitting contexts (certain chemicals should be present in the membrane). In this case, four membranes suffice. The study of other variants of P systems with symport rules (for instance, with forbidding contexts) is formulated as an open problem.      <hr align=\"left\" width=\"50%\">      1.) C. S. Calude, K. Salomaa, S. Yu (eds.). Advances and Trends in Automata and Formal Languages. A Collection of Papers in Honour of the 60th Birthday of Helmut Jrgensen.'),
(414,'<p>The problem about the <I>synchronization</I> of a finite deterministic automaton is not yet properly understood. The present paper investigates this and related problems within the general framework of a composition theory for functions over a finite domain <I>N</I> with <I>n</I> elements. The notion of <I>depth</I> introduced in this connection is a good indication of the <I>complexity</I> of a given function, namely, the complexity with respect to the length of composition sequences in terms of functions belonging to a basic set. The depth may vary considerably with the target function. Not much is known about the reachability of some target functions, notably constants. <I>Synchronizability</I> of a finite automaton amounts to the representability of some constant as a composition of the functions defined by the input letters. Properties of <I>n</I> such as primality or being a power of 2 turn out to be important, independently of the semantic interpretation. We present some necessary, as well as some sufficient, conditions for synchronizability. We also discuss a famous conjecture about the length of the shortest synchronizing word, and present some results about <I>universal</I> synchronizing words.  </P>      <hr align=\"left\" width=\"50%\">      1.) C. S. Calude, K. Salomaa, S. Yu (eds.). Advances and Trends in Automata and Formal Languages. A Collection of Papers in Honour of the 60th Birthday of Helmut Jrgensen.'),
(415,'We consider disjunctive sequences, that is, infinite sequences <img src=\"how_large_is_the/images/img1.gif\">-words) having all finite words as infixes. It is shown that the set of all disjunctive sequences can be described in an easy way using recursive languages and, besides being a set of measure one, is a residual set in Cantor space.  Moreover, we consider the subword complexity of sequences: here disjunctive sequences are shown to be sequences of maximal complexity.  Along with disjunctive sequences we consider the set of real numbers having disjunctive expansions with respect to some bases and to all bases. The latter are called absolutely disjunctive real numbers. We show that the set of absolutely disjunctive reals is also a residual set and has representations in terms of recursive languages similar to the ones in case of disjunctive sequences. To this end we derive some fundamental properties of the functions translating a base r-expansion of a real <img src=\"how_large_is_the/images/img2.gif\"> [0, 1] into <img src=\"how_large_is_the/images/img3.gif\">.      <hr align=\"left\" width=\"50%\">      1.) C. S. Calude, K. Salomaa, S. Yu (eds.). Advances and Trends in Automata and Formal Languages. A Collection of Papers in Honour of the 60th Birthday of Helmut Jrgensen.'),
(416,'In this paper, we present a fast and simple algorithm for constructing a minimal acyclic deterministic finite automaton from a denite set of words. Such automata are useful in a wide variety of applications, including computer virus detection, computational linguistics and computational genetics. There are several known algorithms that solve the same problem, though most of the alternative algorithms are considerably more difficult to present, understand and implement than the one given here. Preliminary benchmarking indicates that the algorithm presented here is competitive with the other known algorithms.      <hr align=\"left\" width=\"50%\">      1.) C. S. Calude, K. Salomaa, S. Yu (eds.). Advances and Trends in Automata and Formal Languages. A Collection of Papers in Honour of the 60th Birthday of Helmut Jrgensen.'),
(417,'We define rationally additive semirings that are a  generalization of ()-complete and ()-continuous semirings. We prove that every rationally additive semiring is an iteration semiring. Moreover, we characterize the semirings of rational power series with coefficients in <img src=\"rationally_additive_semirings/images/img3.gif\">, the semiring of natural numbers equipped with a top element, as the free rationally additive semirings      <hr align=\"left\" width=\"50%\">      1.) C. S. Calude, K. Salomaa, S. Yu (eds.). Advances and Trends in Automata and Formal Languages. A Collection of Papers in Honour of the 60th Birthday of Helmut Jrgensen.'),
(418,'Formal methods have been considered one possible solution to the so-called software crisis. Tools are valuable companions to formal methods: they assist in analysis and understanding of formal specifications and enable the use of rigorous techniques in industrial projects. In this paper, an overview of the new DisCo toolset is given. DisCo is a formal specification method for reactive and distributed systems. It focuses on collective behaviour of objects and provides a refinement mechanism that preserves safety properties. The toolset currently includes a compiler, a graphical animation tool, and a scenario tool for representing execution traces as Message Sequence Charts. A prototype verification back-end based on the PVS theorem prover also exists, and a model checking back-end based on Kronos as well as code generation facilities have been planned. In this paper, the operation of the DisCo toolset is illustrated by applying it to an example specification describing a simple cash-point service system.'),
(419,'This paper presents an environment to support the use of specification for mixed systems, i.e. systems with both dynamic (behaviour, communication, concurrency) and static (data type) aspects. We provide an open and extensible environment based on the KORRIGAN specification model. This model uses a hierarchy of view concepts to specify data types, behaviours and compositions in a uniform way. The key notion behind a view is the symbolic transition system. A good environment supporting such a model needs to interface with existing languages and tools. At the core of our environment is the CLIS library which is devoted to the representation of our view concepts and existing specification languages. Our environment is implemented using the object-oriented language PYTHON. It provides an integration process for new tools, a specification library, a parser library, LOTOS generation and object-oriented code generation for KORRIGAN specifications.'),
(420,'<p>In this paper we describe our system for automatically extracting \"correct\" programs from proofs using a development of the Curry-Howard process. </P>  <P>Although program extraction has been developed by many authors (see, for example, [HN88], [Con97] and [HKPM97]), our system has a number of novel features designed to make it very easy to use and as close as possible to ordinary mathematical terminology and practice. These features include 1. the use of Henkin\'s technique [Hen50] to reduce higher-order logic to many-sorted (first-order) logic; 2. the free use of new rules for induction subject to certain conditions; 3. the extensive use of previously programmed (total, recursive) functions; 4. the use of <I>templates</I> to make the reasoning much closer to normal mathematical proofs and 5. a conceptual distinction between the computational type theory (for representing programs) and the logical type theory (for reasoning about programs).</P>  <P>As an example of our system we give a constructive proof of the well known theorem that every graph of even parity, which is non-trivial in the sense that it does not consist of isolated vertices, has a cycle. Given such a graph as input, the extracted program produces a cycle as promised.  </P>'),
(421,'We show how the declarative spirit of attribute grammars can be employed to define an attribution mechanism for <i>term graphs</I>, where the non-uniqueness of inherited attributes demands an appropriately generalised treatment.  <br><br>  Since term graphs are a useful data structure for symbolic computation systems such as theorem provers or program transformation systems, this mechanism provides a powerful means to generate concrete programs (and other relevant text or data structures) from their abstract term graph representations.  <br><br>  We have implemented this declarative term graph attribution mechanism in the interactive term graph program transformation system HOPS and show a few simple examples of its use.'),
(422,''),
(423,'In this paper we present the real-time verification and analysis tool RAVEN. RAVEN is developed for verifying timed systems on various levels of abstraction. It integrates a real-time model checker for real-time specifications, it offers algorithms for analyzing critical delay times, for inspecting data values and event occurrences and for detecting dead_locks and live-locks. The counter example generator provides helpful information for error recovering by printing system execution paths (failing a given specification) to the integrated wave_form browser. All included algorithms are based on a common data structure enabling a compact representation and possibilities for acceleration. By some examples we show that our approach outperforms some state-of-the-art verification tools.'),
(424,'The paper presents a tool architecture which supports the formal verification of logic controllers for processing systems. The tool\'s main intention is to provide a front-end for modelling the controller as well as the processing systems. The models are automatically transformed into representations which can be analysed by existing model checking algorithms. While the first part of the paper gives an overview of the complete architecture, the second part introduces a newly developed modelling interface: Process Control Event Diagrams (PCEDs) are formally defined as a suitable means to represent the flow of information in controlled processes. The transformation of PCEDs into verifiable code is described, and the whole procedure of modelling, model transformation and verification is illustrated with a simple processing system.'),
(425,'In this paper, we propose a generic mechanism for extending decision procedures by means of a lemma speculation mechanism. This problem is important in order to widen the scope of decision procedures incorporated in state-of-the-art verification systems. Soundness and termination of the extension schema are formally stated and proved. As a case study, we consider extensions of a decision procedure for the quantifier-free fragment of Presburger Arithmetic to significant fragments of non-linear arithmetic.'),
(426,'<p>Recently there has been much interest in the automatic and semi-automatic verification of parameterized networks, i.e., verification of a family of systems <IMG src=\"verification_of_parameterized_protocols/images/img1.gif\"> <IMG src=\"verification_of_parameterized_protocols/images/img2.gif\">, where each <IMG src=\"verification_of_parameterized_protocols/images/img3.gif\"> is a network consisting of i processes. </P>  <P>In this paper, we present a method for the verification of so-called <I>universal</I> properties of fair parameterized networks of similar processes, that is, properties of the form <IMG src=\"verification_of_parameterized_protocols/images/img4.gif\"> where <IMG src=\"verification_of_parameterized_protocols/images/img5.gif\"> is a quantifier-free LTL formula and the <I>p<SUB>i</sub></I> refer to processes.  To prove an universal property of a parameterized network, we first model the infinite family of networks by a single fair WS1S transition system, that is, a transition system whose variables are set (2nd-order) variables and whose transitions are described in WS1S. Then, we abstract the WS1S system into a finite state system that can be model-checked. We present a generic abstraction relation for verifying universal properties as well as an algorithm for computing an abstract system.</P>  <P>However, the abstract system may contain infinite computations that have no corresponding fair computations at the concrete level, and hence, in case the property of interest is a progress property, verification may fail because of this. Therefore, we present methods that allow to synthesize fairness conditions from the parameterized network and discuss under which conditions and how to lift fairness conditions of this network to fairness conditions on the abstract system. We implemented our methods in a tool, called <tt>PAX</TT>, and applied it to several examples.  </P>'),
(427,'We use a relational characterization of binary direct sums to model sequences within the relation-algebraic manipulation and prototyping system <tt>RelView</tt> in a simple way. As an application we formally derive a <tt>RelView</tt> program for computing equivalence classes of an equivalence relation, where we combine relation-algebraic calculations with the so-called Dijkstra-Gries program development method. Also a refinement of the simple modeling is presented, which leads to the classical datatype of stacks, and a further application is sketched.'),
(428,'We define a class of predicate diagrams that represent abstractions of - possibly infinite-state - reactive systems. Our diagrams support the verification of safety as well as liveness properties. Non-temporal proof obligations establish the correspondence between the original specification, whereas model checking can be used to verify behavioral properties. We define a notion of refinement between diagrams that is intended to justify the top_down development of systems within the framework of diagrams. The method is illustrated by a number of mutual-exclusion algorithms.'),
(429,'In this paper we describe the formal specification and verification of an efficient algorithm based on bitvectors for real-time model checking with the KIV system.            <br><br>             We demonstrate that the verification captures the essentials of the C++ algorithm as implemented in the RAVEN model checker. Verification revealed several possibilities to reduce the size of the code and to improve its efficiency.'),
(430,''),
(431,'<p>This paper presents the <I>Coalgebraic Class Specification Language</I> <tt><TT>CCSL</TT></tt> that is developed within the loop project on formal methods for object-oriented languages. <tt>CCSL</tt> allows the (coalgebraic) specification of behavioral types and classes of object-oriented languages. It uses higher-order logic with universal modal operators to restrict the behavior of objects. A front-end to the theorem provers pvs [ORR + 96] and <tt>ISABELLE</tt> [Pau94] compiles <tt>CCSL</tt> specifications into the logic of these theorem provers and allows to mechanically reason about the specifications.  </P>'),
(432,'This paper describes an analysis of the effectiveness of an in-service training system developed by a project sponsored by the foundation of the Information-technology Promotion Agency, Japan (IPA). We developed and carried out a 10 days training course for 65 teachers in three different locations. The three main elements of this course were (1) training curriculum, (2) CD_ROM materials, and (3) Web-based support system. The participants of this course were hoping to become Information Technology (IT) leaders in their schools. An analysis was conducted to investigate the factors influencing the effectiveness of the training. Based on our analysis, we were able to draw the following conclusions: (1) Web-based training support system and CD-ROM materials were very effective to improve teachers  knowledge and skills, regardless of prior knowledge and skills (2) Traditional instructional style (i.e. one-way instructor-centered style) was generally an ineffective training method. (3) CD-ROM materials significantly enhanced the effectiveness of teachers  creating materials (such as a Web page), especially when the CD-ROM was used for self-study. These findings will be useful for educators and educational designers who plan and conduct in-service training programs.'),
(433,'Currently, the web-based learning support systems  are one of interesting and hot topics in points of the utilization of Internet and the application of computers to education. In particular, the web-based collaboration is very applicable means to make unfamiliar students, who are unknown with each other, discuss together in the same virtual interaction space. However, there are some problems derived from the gap between the real world and virtual environment: coordination for discussions, cooperative reactions, comprehension of learning progress, etc. These problems may be dependent on the fact that the actions of students cannot be influenced from the behaviors of others directly. In this paper, we address a coordination mechanism to promote cooperative actions/reactions for progressive discussions. Our idea is to apply an agent-oriented framework to this coordination mechanism and introduce two different types of agents. One is a coordinator and the other is a learner. The coordinator monitors the learning progress of group and promotes the discussion, if necessary, so as to reach their common goal successfully. The learners are assigned to individual students, and act as interaction mediators among students in place of the corresponding students. Of course, the coordinator is a passive entity and learners are active entities in our collaborative learning space.'),
(434,'In this paper, we describe the design and implementation of a synchronous EFL computer assisted English writing environment. In addition to supporting the basic writing function requirements, two novel mechanisms, namely, (1) synchronous text co-editing, and (2) voice delivery, have been designed to provide fundamental capabilities over the Internet. As a result, the designed system exploits the integration of computers and networking capabilities with linguistic and pedagogical principles crucial to web-based language learning. The system integrates CMC tools with database technology for the specific purpose of archiving the communications between tutors and students. Since the platform offers a bank of comments that are frequently used in these online tutorials, the system can store and tabulate each token instance when a comment is used. This database then offers instant cumulative profiles into tutor-learner interaction and into the common language errors or difficulties uncovered in the tutorials. Such an archive supports research into language learning difficulties and into patterns of tutor-learner interaction. This data is valuable in the assessment of pedagogical effectiveness and in the development of online tutorial materials that meet the attested needs of learners.'),
(435,'The importance of collaborative and social learning processes is well established, as is the utility of external representations in supporting learners\'  active expression, examination and manipulation of their own emerging knowledge. However, research on how computer-based representational tools may support collaborative learning is in its infancy. This paper motivates such a line of research, sketches a theoretical analysis of the roles of constraint and salience in the representational guidance of collaborative learning discourse, and reports on an initial study that compared textual, graphical, and matrix representations. Differences in the predicted direction were observed in the amount of talk about evidential relations and the use of epistemological categories.'),
(436,'Abstract: The purpose of this paper is to describe  one module in a highly integrated language learning environment. The module described is an asynchronous interactive online environment for EFL writing which integrates the potential of computers, Internet, and linguistic analysis to address the highly specific needs of second language composition classes. The system accommodates learners, teachers, and researchers. A crucial consequence of the interactive nature of this system is that users actually create information through their use, and this information enables the system to improve with use. In addition to the tools provided for teachers to mark essays and automatically track the feedback they have given each learner, the system supports the automated capture of a learner corpus of written English in the process. The essays written by users and the comments given by teachers are archived in a searchable online database. Learners can retrieve this information to examine their own recurring problems in the target language. Teachers can do the same in order to discover these problem areas for individual learners and for a class as a whole. The modular system provides interfaces with functions to facilitate an array of user tasks such as teachers\'  correction of essays and learners\'  writing and revision processes. Error analysis of learner essays has led to content creation for automated online help. One sort of help feature can detect certain errors automatically and offer appropriate help pages. Another type of help feature can track the number of times a teacher has marked the same error type in one learner\'s writing and, when this number reaches a threshold, automatically offer help on this error to this learner.'),
(437,''),
(438,'The Message Minimizing Load Redistribution Problem is described which arises from the need to redistribute work when performing load balancing in a parallel computing environment. We consider a global perspective and seek a redistribution plan that minimizes the overall processing time. We define the cost associated with a solution to be the number of packets needed to balance out the workload. The impact of the interconnection network is ignored. This problem can arise in many applications. One such example being the U.K. Meteorological Office\'s operational weather forecasting and climate prediction models.                   <br><br>            This problem is equivalent to the Pure Unit-Cost Transportation Problem. A simple proof of <img src=\"the_message_minimizing_load/images/img1.gif\">-completeness is given, and various heuristics and approximation issues are investigated. Several theoretical results are shown that may impact the design of an algorithm. Simulation results are presented.'),
(439,'<p>Computational complexity of the subtasks in the symmetry reduction method for Place/Transition-nets is studied. The task of finding the automorphisms (symmetries) of a net is shown to be polynomial time many-one equivalent to the problem of finding the automorphisms of a graph. Deciding whether two markings are symmetric is shown to be a problem equivalent to the graph isomorphism problem. This remains to be the case even if a generator set for the automorphism group of the net is known. The problem of constructing the lexicographically greatest marking symmetric to a given marking (a canonical representative for the marking) is classified to belong to the lower levels of the polynomial hierarchy, namely to be <b>FP<sup>NP</sup></B>[log <i>n</i>] - hard but in <b>FP<sup>NP</sup></B>. It is also discussed how the self-symmetries of a marking can be exploited. Calculation of such symmetries is classified to be as hard as computing graph automorphism groups. Furthermore, the coverability version of testing marking symmetricity is shown to be an <b>NP</b>-complete problem. It is proven that canonical representative markings and the symmetric coverability problem cannot be combined in a straightforward way. </p>'),
(440,'At first sight, knowledge management for  poverty-stricken countries appears to be a contradiction in terms. It sounds \"high-tech\" and not very applicable for \"third world\" countries that may not possess the necessary infrastructure. However, the aim of the paper is to show that this is not only false but that Knowledge Management (KM) has a big role to play. We begin by giving an introduction to KM systems in general before considering how they may be applied in poverty and crisis situations. We then consider specific functions of these systems before looking at some problems and possible solutions of implementing such a system.'),
(441,''),
(442,'We address three basic questions in computational geometry which can be phrased in simple terms but have only recently received (more or less) satisfactory answers: point set enumeration, optimum triangulation, and polygon decomposition.'),
(443,'Trends in telecommunications networks including network convergence, requirements for QoS and service level agreements, and open service architectures are impacting the service mangement systems and processes. New results in three areas of IP service management are described. The architecture of a new platform for service management is presented. This is the first reported service assurance platform to use ASP technology as its infrastructure. A new performance mangement suite is described. This suite currently supports measurement and reporting of web and stream servers and VoIP softswitches. Finally, a recent result in customer care automation for processing large volumes of email sent to a customer care center is reviewed.'),
(444,'This paper discusses necessary changes to the computer science curriculum at universities for the future. The alterations are grouped into the following five areas: content and body of knowledge, pedagogy, audience, training-on-the-job, and professional skills. The paper argues that extending the scope of knowledge beyond the narrow borders of primary computer science topics (breadth in computer science) will lay a solid foundation for building the necessary skills for the future work force.'),
(445,'The 180 models collected in this paper are produced by sampling and wrapping point sets on tubes. The surfaces are represented as triangulated 2-manifolds and available as stl-files from the author s web site at www.cs.duke.edu/~edels. Each tube is obtained by thickening a circle or a smooth torus knot, and for some we use the degrees of freedom in the thickening process to encode meaningful information, such as curvature or torsion.'),
(446,'Working with the ubiquitous  Web  we immediately realize its limitations when it comes to the delivery or exchange of non-textual, particularly graphical, information. Graphical information is still predominantly represented by raster images, either in a fairly low resolution to warrant acceptable transmission times or in high resolutions to please the reader s perception thereby challenging his or her patience (as these large data sets take their time to travel over congested internet highways).             <br><br>            Comparing the current situation with efforts and developments of the past, e.g. the Videotex systems developed in the time period from 1977 to 1985, we see that a proper integration of graphics from the very beginning has, once again, been overlooked.             <br><br>            The situation is even worse going from two-dimensional images to three-dimensional models or scenes. VRML, originally designed to address this very demand has failed to establish itself as a reliable tool for the time window given and recent advances in graphics technology as well as digital library technology demand new approaches which VRML, at least in its current form, won t be able to deliver.               <br><br>            After summarizing the situation for 2D graphics in digital documents or digital libraries this paper concentrates on the 3D graphics aspects of recent digital library developments and tries to identify the future challenges the community needs to master.'),
(447,'We consider the state of the art in compiler construction and where to go from here. Main topics are improved exploitation of present (and future) hardware features, the interaction between compiling techniques and processor design, and the use of compiling techniques in application areas such as component-based software engineering and software reengineering.'),
(448,''),
(449,'Animation is commonly seen as an ideal tool for teaching dynamic phenomena. While there have been very few studies testing this hypothesis, animations are used extensively in teaching, particularly in the field of algorithms. We highlight features that we consider important for animation systems, describe the development of algorithm animation by examples, and present a new Java-based system supporting annotation and recording of animations. We also outline a way to annotate animations and movies given in the MPEG video format. By listing several case studies we describe new ways and possibilities of how animation systems may be used in the future.'),
(450,'The last twenty years have seen enormous progress in  the design of algorithms, but little of it has been put into practice. Because many recently developed algorithms are hard to characterize theoretically and have large running_time coefficients, the gap between theory and practice has widened over these years. Experimentation is indispensable in the assessment of heuristics for hard problems, in the characterization of asymptotic behavior of complex algorithms, and in the comparison of competing designs for tractable problems.              <br><br>             Implementation, although perhaps not rigorous experimentation, was characteristic of early work in algorithms and data structures. Donald Knuth has throughout insisted on testing every algorithm and conducting analyses that can predict behavior on actual data, more recently, Jon Bentley has vividly illustrated the difficulty of implementation and the value of testing. Numerical analysts have long understood the need for standardized test suites to ensure robustness, precision and efficiency of numerical libraries. It is only recently, however, that the algorithms community has shown signs of returning to implementation and testing as an integral part of algorithm development. The emerging disciplines of experimental algorithmics and algorithm engineering have revived and are extending many of the approaches used by computing pioneers such as Floyd and Knuth and are placing on a formal basis many of Bentley\'s observations.                <br><br>               We reflect on these issues, looking back at the last thirty years of algorithm development and forward to new challenges: designing cache_aware algorithms, algorithms for mixed models of computation, algorithms for external memory, and algorithms for scientific research.'),
(451,'The users of the Internet in general have not developed a perception of where what security is crucial and beneficial for their applications. At present the average user is provided very few information independent of what is transported over the service and how this is done. What is needed for a secure Internet, is that security is answered on a system level or on an application level and that an appropriate level of security is reached and still is accepted by the user? These questions are primarily questions on a technical level but have a great dimension of awareness which has to be kept in mind. However, the main question is not how to secure the Internet in place but how to develop mechanisms and tools for the Internet that can seamlessly improve an ever changing media which opens up new dimensions of security risks with every new protocol system and application. Security will remain a race where comfort is often seen as a competitor.'),
(452,'The utilization of new emerging standards such as MPEG-7 is expected to be a major breakthrough for content-based multimedia data retrieval. The main features of the MPEG standards series and of related standards, formats and protocols are presented. It is discussed, how they, despite their partially early and immature stage, can best be utilized to yield effective results in the context of a knowledge management environment. Complementary to that, the current status and state of the art in content-based retrieval for images, video and audio content is briefly presented. In the context of the KNOW-Center we are developing a prototype platform to implement a user friendly and highly informative access to audiovisual content as a potential component for a future knowledge management system. The technical requirements and the system architecture for the prototype platform are described.'),
(453,'Humans are not able to cope with the exponential growth of information and the increasing speed of information and business processes fostered by information and communication technologies. Technical support not only for information storage and retrieval but also for information selection, process planning, and decision support is needed. Most of the ICT investments, however, do not foster innovation or productivity. Recent studies show that ICT-based training is the main instrument of knowledge management. On-line media and self-directed learning environments are among the most effective training solutions in terms of cost, time and logistics. In the last few years, the percentage of employees participating in training courses increased. At the same time, there has been a decline of training budgets. E-Learning is able to deliver more valuable training for less money only if it is part of an integrated knowledge and skills management system. Two case studies of knowledge and meta data management systems are discussed.'),
(454,'The rapid emergence of a global knowledge economy both shortens the timetable for progress on sustainable development and also offers a potential \"win-win\" alternative to the traditional trade-off between growth and environmental sustainability.            <br><br>             The Lisbon Strategy and e-Europe initiative to accelerate development of the knowledge economy in Europe already addresses several aspects of social and economic sustainability. However, the trends in most resource-use and environmental impact indicators are still worsening, and much more needs to be done to realise the potential benefits of structural change in business and employment, notably in the service sector.              <br><br>               The Stockholm and Gteborg EU Summits, and the subsequent Rio+10 conference give a timely and unique opportunity to establish European coherence and leadership in seeking sustainable development in the knowledge economy.                 <br><br>                However, we also need a new clarification of individual and business-level responsibilities for lifestyle and business organisation changes, and a much wider take-up of innovative \"win-win\" solutions for growth with reductions in resource use and impacts.'),
(455,'\"New\" essential resources and success factors keep being invested and provide fertile grounds, not only in the consultancy industry, for ever more glossy brochures to create success. The production factor of knowledge is currently at the focus of many theories and numerous publications. It remains to be seen whether we are seeing real innovations. Knowledge has always been prerequisite to creating products or services, an essential input, a \"silent production factor\".             <br><br>            The modern, complex environment has also made products and processes more complex and extensive. The ability to adapt to changing conditions increasingly determines success or failure. All aspects of enterprises are affected, even the \"smallest units\", the human element. In this context, it is becoming increasingly important to be able to share knowledge with colleagues. Knowledge transfer is basically characterised by a question-and-answer principle. The focus is on the incalculable human factor. This causes more or less distinct transfer barriers.             <br><br>            Prejudices, fear of criticism, lack of confidence, constant time pressures and other factors are some barriers to transfer caused by the individual. Besides organisations may create barriers, too, through rigid hierarchies, red tape, and outdated procedures.            <br><br>              By means of the barrier matrix and the barrier cube we have presented eight different constellations from the scientist\'s view. At a very theoretical level we have also touched briefly on how to solve these problems.                  <br><br>                Knowledge management does not yet seem to attach enough importance to the issue of communication, particularly to internal communication. In addition to individual and organisational transfer barriers, communication media can also contribute to problems and barriers in knowledge transfer.'),
(456,'This paper deals with a general approach to knowledge management in companies and organizations. It strongly builds on insights concerning knowledge processing in superorganisms and reflects years of FAW experiences in applications. The paper in particular shows how (1) human resources, (2) issues of organization and (3) new IT systems interact in achieving a higher level of competence and competitiveness. In this context, dealing with non-explicit sources of knowledge is a major issue, too.'),
(457,'This paper describes our vision for the near future in digital content analysis as it relates to the creation, verification, and presentation of knowledge. We focus on how visualization enables humans to make discoveries and gain knowledge. Visualization, in this context, is not just the picture representing the data but also a two-way interaction between humans and their information resources for the purposes of knowledge discovery, verification, and the sharing of knowledge with others. We present visual interaction and analysis examples to demonstrate how one current visualization tool analyzes large, diverse collections of text. This is followed by lessons learned and the presentation of a core concept for a new human information discourse.'),
(458,''),
(459,'Knowledge management systems provide three basic services: information capture, storage and organization, and access. This paper argues that collaborative virtual environments (CVEs) provide features that make them uniquely suited as an integral part of information capture. After introducing CVEs, we present our work in this area and outline our future plans.'),
(460,'<p>In order to be able to (re-)use digital content, interested users must be able to identify and locate relevant documents. This requires descriptive data, nowadays generally referred to as <i>metadata</I>. Technical <i>standards</i> for a scaleable deployment on a global scale are required if we want to achieve a critical mass of resources. In this paper, we present the current status of ongoing work in this area, with a particular emphasis on the IEEE LTSC Learning Object Metadata standard [IEEE, 2001] and related developments in the context of the ISSS Learning Technologies Workshop [ISSS, 2001].  </P>'),
(461,'The efficient management of knowledge has become imperative for almost all types of organizations. Many approaches exist for dealing with knowledge management at a corporate level. But there is also a need to support knowledge management also at an individual level, a level which takes the specific needs, experiences and skills of knowledge workers into account. While largely unexplored within the field of knowledge management, in the field of digital libraries advanced personalization and customization concepts exist. Within this context, this paper examines these concepts and how they can be exploited to address the challenges which are typical for knowledge management. As the paper will show, many synergies exist, if knowledge management at an individual level is dealt with in combination with personal digital libraries.'),
(462,'Recently, the idea of semantic portals on the Web or on the intranet has gained popularity. Their key concern is to allow a community of users to present and share knowledge in a particular (set of) domain(s) via semantic methods. Thus, semantic portals aim at creating high-quality access - in contrast to methods like information retrieval or document clustering that do not exploit any semantic background knowledge at all. However, by way of this construction semantic portals may easily suffer from a typical knowledge management problem. Their initial value is low, because only little richly structured knowledge is available. Hence the motivation of its potential users to extend the knowledge pool is small, too.            <br><br>             We here present SEAL-II, a methodology for semantic portals that extends its previous version, by providing a range of ontology-based means for hitting the soft spot between unstructured knowledge, which virtually comes for free, but which is of little use, and richly structured knowledge, which is expensive to gain, but of tremendous possible value. Thus, we give the portal builder tools and techniques in an overall framework to start the knowledge process at a semantic portal. SEAL-II takes advantage of the ontology in order to initiate the portal with knowledge, which is more usable than unstructured knowledge, but cheaper than richly structured knowledge.'),
(463,'This case study illustrates the knowledge management framework that was designed during the introduction of knowledge management instruments at Siemens Business Services GmbH & Co. or SBS, as it is known. The knowledge management framework will give the reader an understanding of the holistic approach to knowledge management and the different stages of implementation. It also introduces the key learning processes experienced by Siemens Business Services (SBS) during the various implementation phases. The knowledge management (KM) requirements, challenges and solutions within the service business are highlighted. The case study also shows the challenges and objectives of knowledge management (KM) programs, in general, and at Siemens Business Services (SBS), in particular. Based on the experience of the implementation of KM at SBS, the case study closes with critical success factors for other KM implementations, both within and outside Siemens.'),
(464,'New Learning in analogy to New Economy means a new paradigm of learning. Old Learning was learning with a continuous learning history in mind. New Learning means, that the continuity of a learning history is stored in a computer memory and can be quickly accessed. The external storage generates a better and more precise continuity of individual historical learning experiences and shifts the focus of cognitive energy to cognitive creativity. If knowledge is managable as the new discipline knowledge management offers, this new approach will make sense.'),
(465,'Interested readers find a lot of ideas, concepts and implementation attempts for the modern subject \"knowledge management\". A midsize consulting company now faces the problem of finding the answer to: \"what do we need to implement to stay in touch with knowledge and where does the cost/profit relationship just put a stop our possibilities?\" The biggest problem for these companies is their size: they are too big to exchange information and knowledge during coffee or lunch breaks. On the other hand the extensive, company wide systems of the corporates are too expensive and usually not hitting the target. Against this background I\'d like to present a possible solution for day-to-day knowledge management using the hands-on experience of Gosch Consulting GmbH, a midsize IT-consulting company. Looking at our company from the knowledge point of view we realized early on that certain standards have been partly implemented within the company even before the knowledge management hype started. This motivated us to take a closer look at the practicability of our tools and to look into and introduce some of the new concepts and ideas. The objective was to examine their efficiency and effectiveness for our own company first and then to find the balance between \"must\" and \"nice to have\". Equally important was the fact that the instru-ments had to enhance the quality and value of the company and also of the individual employee.'),
(466,'Knowledge management has become an organizational imperative for all types of corporate and governmental organizations. A key objective is to apply knowledge which resides within an organization to achieve the organization\'s goals most efficiently and cost-effectively. To implement knowledge management in organizations, different aspects from different disciplines have to be taken into account. Organizational aspects are required to define which knowledge should be captured and the way it is captured. A new knowledge-friendly culture has to be developed to support knowledge sharing and creation and to overcome the employees\' fear that sharing of knowledge means loosing power. Finally, information technologies play a key role as enabling technology for knowledge management.'),
(467,''),
(468,'<p>This article discusses the calculation of test-cases for interactive systems. A novel approach is presented that treats the problem of test-case synthesis as a formal abstraction problem. It is shown that test-cases can be viewed as formal contracts and that such test-cases are in fact abstractions of requirements specifications. The refinement calculus of Back and von Wright is used to formulate abstraction rules for calculating correct test-cases from a formal specification. The advantage of this abstraction approach is that simple input-output test-cases, as well as testing scenarios can be handled. Furthermore, different testing strategies like partition testing and mutation testing can be formulated in one theory.  </P>'),
(469,'We present the didactic bases for a different kind of text book on Software Engineering - one that is based on semiotics, proper description principles, informal narrations and formal specifications, on phase, stage and stepwise development from developing understandings of the domain, via requirements to software design. Each of the concepts: Semiotics, description, documents, abstraction & modelling, domains, requirements and software design, are covered systematically while enunciating a number of method principles for selecting and applying techniques and tools for the effcient construction of efficient software. The proposed textbook presents many, what are believed to be novel development concepts: Domain engineering with its emphasis on domain attributes, stake{holder perspectives and domain facets (intrinsics, support technologies, management & organization, rules & regulation, human behaviour, etc.), requirements engineering with its decomposition into domain requirements (featuring such techniques as projection, instantiation, extension and initialization), interface requirements and machine requirements, etc.'),
(470,'<p>This paper describes (one person\'s view of) how the <I>Vienna Development Method</I> grew out of the earlier work on the <I>Vienna Definition Language</I>. Both of these activities were undertaken at the IBM Laboratory Vienna during the 1960s and 70s.  </P>'),
(471,'<p>This article provides a historical overview of a decade of the development of the IFAD VDM Toolboxes commonly referred to as <B>VDMTools</B>. All along, the existing tools have been used in the development of new major components. This kind of \"bootstrapping \" approach where a CASE tool is developed by taking \"\"its own medicine\" is seldom used. However, we believe that this approach is important to be able to better understand what the most important improvements are for the users in practice. This article also describes how the different components have been maintained by a changing development team. We feel that the decisions we have made regarding the parts of the tool which have been formally specified and the parts which have been developed conventionally may provide valuable input for others considering the use of formal specification. The overall organisation of the development environment may also be interesting for other developers. </P>'),
(472,'In this paper I celebrate the evolution of the  Vienna Development Method (VDM) along its Irish branch and attempt to tell the story that Peter Lucas played in it.              <br><br>             There are two parts to the paper. In the first part I tell my story of the early day of the origins of the Irish School of the VDM (<img src=\"an_eclectic_view_of/images/img3.gif\">), beginning with prehistory in 1978 up until the radical decisions of 1995 which led to the Irish School of Constructive Mathematics <img src=\"an_eclectic_view_of/images/img1.gif\">.                <br><br>               In 1995 the School committed itself to the development of the modelling of (computing) systems in full generality. This was achieved by embracing Category Theory and by exploring a geometry of formal methods using techniques of fiber bundles. From fiber bundles to sheaves was a natural step. Concurrently, the School moved from the algebra of monoids to categories, and from categories to topoi (alt. toposes). The second part of the paper illustrates, with simple examples, how I introduce topos logic into modelling in 2001.'),
(473,'<p>This paper sketches a reverse engineering     discipline which combines formal and semi-formal methods. Central to the     former is denotational semantics, expressed in the ISO/IEC 13817-1 standard     specification language (VDMSL). This is strengthened with <I>algebra of     programming</I>, which is applied in \"reverse order\" so as to     reconstruct formal specifications from legacy code. The latter include     <I>code slicing</I>, a \"shortcut\" which trims down the     complexity of handling the formal semantics of all program variables at the     same time.</P>   <P>A key point of the approach is its constructive style. Reverse     calculations go as far as absorbing auxiliary variables, introducing mutual     recursion (if applicable) and reversing semantic denotations into standard     generic programming schemata such as cata/paramorphisms.</P>   <P>The approach is illustrated for a small piece of code already studied     in the code-slicing literature: Kernighan and Richtie\'s <I>word     count</I> C programming     \"bagatelle\".</P>'),
(474,'High performance computing (HPC) architectures are specialized machines which can reach their peak performance only if they are programmed in a way which exploits the idiosyncrasies of the architecture. An important feature of most such architectures is a physically distributed memory, resulting in the requirement to take data locality into account independent of the memory model offered to the user. In this paper we discuss various ways for managing data distribution in a program, comparing in particular the low-level message-passing approach to that in High Performance Fortran (HPF) and other high performance languages. The main part of the paper outlines a method for the specification of data distribution semantics for distributed-memory architectures and clusters of SMPs. The paper concludes with a discussion of open issues and references to future work.'),
(475,'<p>We discuss a possible framework for virtualizing the delivery of university courses. With the advent of new technological innovations like high speed computer networks and multimedia computers, there is an increasing awareness that direct face to face teaching is not the only possible mode of teaching in a university system.  There is a demand for preparing high quality multimedia course materials across all disciplines which can be used by learners who either cannot attend live lectures or prefer to study in an off-line mode. Our group at the University of Freiburg has developed a robust tool called <I>Authoring on the Fly</I> (AOF) for recording live classroom sessions as multimedia documents and the synchronous playback of the diffeerent media streams in such a document in an off-line setting. In this paper we discuss the facilities this tool provides for virtualizing university courses as well as improving the offering of courses in a traditional university setting. We discuss the progress we have made in high quality delivery of lectures through multimedia documents and its implications for both off-line and classroom teaching. Further, we discuss our current attempts in virtualizing the assignment submission and correction process as a follow up of the virtual delivery of courses. We also discuss the possible implications of virtual delivery of courses and creation of a virtual university from the point of view of university students, professors and administrators.</p>'),
(476,'We discuss three variants of the DT0L sequence equivalence problem. One of the variants generalizes the <img src=\"three_variants_of_the/images/img1.gif\">sequence equivalence problem of D0L systems for DT0L systems.'),
(477,'Computer-mediated Communication (CMC) has been commonly compared to face-to-face (FtF) communication in recent CMC literature. Research comparisons suggested depersonalizing effects of CMC. However, this experimental study indicates that CMC is a potentially viable mode of social-emotion-oriented communication. In this study, the effects of frequency and duration of messaging on impression development in CMC were investigated. Undergraduate participants were randomly assigned to each of the four experimental groups. For a period of two weeks, participants monitored discussion lists that differed in relation to the frequency and duration of messaging in asynchronous CMC environments. ANOVA results indicated that duration and frequency had significant main effects on impression development in asynchronous CMC environments. No interaction effects were found. The results of this study not only theoretically support the social-emotion-oriented model in CMC, but also lay foundations for further research in many popular types of interactive CMC environments, including e-learning, e-commerce, and e-health.'),
(478,''),
(479,''),
(480,'<p>In an attempt to capture the fundamental features that are common to neural networks, we define a parameterized <I>Neural Abstract Machine</I> (<I>NAM</I>) in such a way that the major neural networks in the literature can be described as natural extensions or refinements of the <I>NAM</I>. We illustrate the refinement for feedforward networks with back-propagation training. The <I>NAM</I> provides a platform and programming language independent basis for a comparative mathematical and experimental analysis and evaluation of different implementations of neural networks. We concentrate our attention here on the computational core (<I>Neural Kernel NK</I>) and provide abstract interfaces for the other <I>NAM</I> components.  </P>'),
(481,'In November 1999, the current version of SDL (Specification and Description Language), commonly referred to as SDL-2000, has passed ITU-T, an international standardization body for telecommunication. The importance and acceptance of SDL in the telecommunication industry surpasses that of UML, which can be seen as the major competitor. A crucial difference between SDL and UML is the existence of a formal SDL semantics as part of the international standard, which has a positive impact on the quality of the entire language definition. In this paper, we treat fundamental questions concerning practicability, adequacy and maintainability of the formalization approach, provide insights into the formal semantics definition and point out several effects on the SDL standard.'),
(482,'This paper tackles some aspects concerning the exploitation of Abstract State Machines (ASMs) for testing purposes. We define for ASM specifications a set of adequacy criteria measuring the coverage achieved by a test suite, and determining whether sufficient testing has been performed. We introduce a method to automatically generate from ASM specifications test sequences which accomplish a desired coverage. This method exploits the counter example generation of the model checker SMV. We use ASMs as test oracles to predict the expected outputs of units under test.'),
(483,'The partial update problem for parallel abstract state machines has manifested itself in the cases of counters, sets and maps. We propose a solution of the problem that lends itself to an efficient implementation and covers the three cases mentioned above. There are other cases of the problem that require a more general framework.'),
(484,'This paper describes a generic proof method for the correctness of refinements of Abstract State Machines based on commuting diagrams. The method generalizes forward simulations from the refinement of I/O automata by allowing arbitrary m:n diagrams, and by combining it with the refinement of data structures.'),
(485,'Abstract State Machines (ASMs) have been widely used to specify soft-ware and hardware systems. Only a few of these specifications are executable, although there are several interpreters and some compilers. This paper introduces a compilation scheme to transform an ASM specification in the syntax of the ASM-Workbench into C++. In particular, we transform algebraic types, pattern matching, functional expressions, dynamic functions, and simultaneous updates to C++ code. The main aim of this compilation scheme is to preserve the specification structure in the generated code without generating inefficient code. The implemented compiler was used successfully in the industrial FALKO application at Siemens Corporate Technology.'),
(486,'We introduce a logic for non distributed, deterministic Abstract State Machines with parallel function updates. Unlike other logics for ASMs which are based on dynamic logic, our logic is based on an atomic predicate for function updates and on a definedness predicate for the termination of the evaluation of transition rules. We do not assume that the transition rules of ASMs are in normal form, for example, that they concern distinct cases. Instead we allow structuring concepts of ASM rules including sequential composition and possibly recursive submachine calls. We show that several axioms that have been proposed for reasoning about ASMs are derivable in our system. We provide also an extension of the logic with explicit step information which allows to eliminate modal operators in certain cases. The main technical result is that the logic is complete for hierarchical (non-recursive) ASMs. We show that, for hierarchical ASMs, the logic is a definitional extension of first-order predicate logic.'),
(487,'<p>We use the <I>Abstract State Machine</I> methodology to give formal operational semantics for the <I>Location Consistency</I> memory model and cache protocol. With these formal models, we prove that the cache protocol satisfies the memory model, but in a way that is strictly stronger than necessary, disallowing certain behavior allowed by the memory model.  </P>'),
(488,'<p>In this note, a direct proof is given of the NP-completeness of a variant of <tt>GRAPH COLORING</tt>, i.e., a generic proof similar to the proof of Cook of the NP-completeness of <tt>SATISFIABILITY</TT>. Then, transformations from this variant of <tt>GRAPH COLORING</TT> to <TT>INDEPENDENT SET</TT> and to <TT>SATISFIABILITY</TT> are shown.</P>  <P>These proofs could be useful in an educational setting, where basics of the theory of NP-completeness must be explained to students whose background in combinatorial optimisation and/or graph theory is stronger than their background in logic. In addition, I believe that the proof given here is slightly easier than older generic proofs of NP-completeness.  </P>'),
(489,'When quoting some part of a document authors usually cut and paste the relevant content into the new document. Thereby the connection between this selected part and the original document is lost. Transclusions - first mentioned in 1960 by Ted Nelson - address this problem of  lost context . With transclusions it is possible to store information about the original document and the exact position of the quote in the newly created document and provide the reader with additional navigational features. Document formats and information systems matured over the last 40 years. This paper gives an overview of some document formats available today in the WWW environment and points to some requirements for server systems providing transclusions. Thereafter we present some ideas on how to implement transclusions based on a Hyperwave Information Server (HIS).'),
(490,''),
(491,'We introduce a couple of families of codifiable languages and investigate properties of these families as well as interrelationships between diffeerent families. We also develop an algorithm based on the Earley algorithm to compute the values of the inverse of the Parikh matrix mapping over a codifiable context-free language. Finally, an attributed grammar that computes the values of the Parikh matrix mapping is defined.'),
(492,'We study the problem of efficient identification of particular classes of p-time languages, called uniform. We require the learner to identify each language of such a class by constantly guessing, after a small number of examples, the same index for it. We present three identification paradigms based on different kind of examples: identification on informant (positive and negative information), measure identification (positive information in a probabilistic setting), identification with probability (positive and negative information in a probabilistic setting). In each case we introduce two efficient identification paradigms, called efficient and very efficient identification respectively. We characterize efficient identification on informant and with probability and, as a corollary, we show that the two identification paradigms are equivalent. A necessary condition is shown for very efficient identification on informant, which becomes sufficient if and only if P = NP. The same condition is sufficient for very efficient identiffication with probability if and only if NP=RP. We show that (very) efficient identification on informant and with probability are strictly stronger than (very) efficient measure identiffication.'),
(493,'Toda proved a remarkable connection between the polynomial hierarchy and the counting classes. Tarui improved Toda s result to show the connection to a weak form of counting and provided an elegant proof. This paper shows that a key step in Tarui s proof can be done uniformly using the depth-first traversal and provides the algorithm that generalizes Toda s result to arbitrary alternating Turing machines (ATMs). Tarui s proof is carefully dissected to obtain an interesting relationship between the running time of the constructed counting machine and the diffeerent parameters of the original ATM: the number of alternation blocks, the number of non-deterministic steps, and the number of deterministic steps.'),
(494,'Tailorability (or adaptability) of software becomes more important with the increasing use of off-the-shelf-software. On the other hand, computers support the work of many groups which in turn have to tailor a commonly used software to support individual as well as group needs. This includes not only groupware, i. e., software that directly supports collaborative work, but also single user software. Research has shown that often adaptations to single user software are distributed among colleagues, thus leading to a systematization in a group\'s adaptations. Based on this observation an empirical field-study on the collaborative tailoring habits of users of a particular word processor was carried out. Based on these and literature research an add-on to this word processor was developed which provides a public and a private repository for adaptations as well as a mailing function for users to exchange adaptations. Some notification and annotation mechanisms are also provided. Results of two forms of evaluation indicate that users of different levels of qualification are able to handle the tool and consider it a relevant alternative to existing mailing mechanisms.'),
(495,'A simultaneous multithreaded (SMT) processor is  able to issue and execute instructions from several threads simultaneously. A SMT processor reaches its highest performance, when all issue slots are utilized by non-speculative instructions provided that the workload is sufficient. SMT processors are able to utilize their resources by executing instructions of multiple threads simultaneously, whereas single-threaded processors fill their resources with highly speculative instructions that must frequently be discarded due to misspeculations. Consequently, we explore speculation control in SMT processor models with the target to increase performance by restricting the number of in-flight speculative instructions. We vary the sizes of internal buffers, the instruction fetch bandwidth, the instruction selection strategies and branch prediction models with the target to increase performance of the simulated SMT processor models. Our results show (1) that retirement buffer sizes of 16 or 32 entries increase performance compared to smaller and to larger buffer sizes, (2) that the instruction fetch bandwidth can be decreased to two times four instructions per cycle without performance loss even for eight threaded eight issue processor models, (3) that an instruction selection strategy that discriminates speculative instructions in the fetch, decode, issue, and dispatch stages may increase overall performance, and (4) that a highly multithreaded processor with a sufficient workload may do without a branch prediction.'),
(496,''),
(497,'<p>In this paper we examine the problem of heap construction on a rooted tree <i>T</i> from a packet routing perspective. Each node of <i>T</i> initially contains a packet which has a key-value associated with it. The aim of the heap construction algorithm is to route the packets along the edges of the tree so that, at the end of the routing, the tree is heap ordered with respect to the key values associated with the packets. We consider the case where the routing is performed according to the matching model and we present and analyse an off-line algorithm that heap orders the tree within 2<i>h</i>(<i>T</i>) routing steps, where <i>h</i>(<i>T</i> ) is the height of tree <i>T</i>. The main contribution of the paper is the novel analysis of the algorithm based on potential functions. It is our belief that potential functions will be the main vehicle in analysing fast non-recursive routing algorithms.  </P>'),
(498,'Studying the architectural evolution of mainstream  field programmable gate arrays (FPGAs) leads to the following remark: in these circuits, the proportion of silicon devoted to reconfigurable routing is increasing, reducing the proportion of silicon available for computation resources. A quantitative analysis shows that this trend, if pursued, will lead to a widening gap between FPGA performance and VLSI performance. Some prospective solutions to this problem are discussed.'),
(499,''),
(500,'We are all very conscious of living through a revolution - one in which the industrial society is being superseded by the information society. Every day brings new evidence of the breakneck pace of the changes that are currently underway. But while broad awareness may be unavoidable, understanding is not so easy. Both the pace of the revolution and its multi-faceted nature make it difficult to gain a clear perspective. But here the new science of complexity can perhaps help. It provides a coherent theory that is directly applicable to the emerging society, potentially providing new insights and new understanding. This paper examines several facets of the current revolution from a complexity perspective, and suggests that the relationship between the emerging science and the emerging society will be a rich one.'),
(501,'<p>This introduction briefly summarizes the six articles that makeup this special issue on IT and process reengineering, and places them against a backdrop of the role of IT within the 21<SUP>st</SUP>century organization. Maintaining high-quality information technology (IT) is essential as organizations move toward a \"system of systems\" and a \"team of team.\" Added to this milieu of managed change are emerging new requirements for leadership and challenging new roles for knowledge workers in the next decade. This article examines three goals for IT in organizations: increased productivity, mediated change, and empowered workforce. Four enablers are identified as the means through which IT can accomplish modernization: (1) using next-generation IT as cognitive tools, (2) understanding the dynamics of organizational culture in order to purposefully change culture, (3) nurturing human performance as a source of yet unrealized gains, (4) leading people as well as managing resources.  </P>'),
(502,'Today, grant proposals submitted to the German Research Foundation (DFG) are paper documents. They are received by ordinary mail, manually entered into a proprietary software system and, finally, information relevant to the specific task is extracted manually and sent to other departments involved in the reviewing/approval process. Of course, all these activities are purely hard-copy based. This paper gives a first report on a research project _GoldenGate_ which focuses on the development of a prototype system for a complete electronic workflow including submission, managing and approval of applications for research funding at the DFG. Typically one would use one of the available Information/Workflow Management Systems, but after careful consideration we made the decision to use a set of standard software tools and formats (i.e. Hyperwave Information Server, MS Office 97, XML) as the key components of our new system and combine them with minimal but flexible interfaces. These ideas, the situation at the DFG, technical details of our present implementation and preliminary results are presented in this paper.'),
(503,'This paper describes the integrated use of electronic structure computations in the undergraduate chemistry curriculum including organic, physical, and analytical chemistry courses. This computational tool is used to enhance student learning and understanding of chemical principles along with exposing students to a modern research tool in chemistry. The electronic structure computations are used for homework, classroom activities (including examinations), and laboratory experiments (both computational and wet-lab). Some examples of these uses of electronic structure computations in organic, analytical, and physical chemistry courses are discussed.'),
(504,'This study investigated the self-reported effects of Internet use on faculty at small Christian colleges and universities by age, years of Internet use, academic field, and on faculty communication style, teaching style, personal productivity, fulfillment of the organization s mission, social networks, research, and professional development. Findings: Faculty believed that their communication had changed and that they can communicate with others more quickly, get faster replies to questions, and obtain more relevant data. Faculty disagreed that their teaching style had changed and that they had changed the way they conduct a class. Faculty believe that their productivity has changed. Most faculty disagreed that the Internet has made them more comfortable sharing their feelings about God. Faculty believe that the Internet has changed the type of jobs and the way students look for jobs, that there are fewer barriers to joining an electronic group, and that the volume of people they keep in frequent touch with has increased. Most faculty agreed that the way they do research has changed and that the Internet makes it easier to get information about advances in their fields. Faculty disagreed that the Internet could be substituted for conferences and that the Internet has made it possible for them to serve on boards. Key Words: Internet, faculty, communication, professional development, personal productivity, research, social networks, teaching, mission.'),
(505,'Continuous improvement of hard technology (software, electronic, mechanical, chemical, biological, etc.) systems and institutional (mixed human and technology based) systems is examined from a system perspective, applying system engineering and assessment methodologies and tools. Class and containment hierarchies are used to simplify the modeling of complex systems and their dynamic processes, particularly system families with both shared standardized content and necessary diversity, resolving addressing an historical tension. The engineering concept of _embedded system_ is formalized as modeled patterns of embedding management intelligence in both hard technology systems and human institutions. Embedded intelligence models describe intelligent performance, human learning, technical system life cycle improvement, and institutional improvement of all systems. The resulting models describe situationally aware, conscious systems, whether adaptive man_made systems or continuously improving institutions. Models include system requirements, design, verification, and change management. Assessment of system performance against goals determines priority for continuing system improvement. After treating human and hard technology systems on a unified basis, their significant differences are recognized through knowledge worker educational processes, personal reflection on performance, and use of electronic portfolios exhibiting best work. Tools supporting these methodologies are Intranet infrastructure providing computer support of the collaborative work of specifying institutional and technical system requirements, design, assessment, and improvement change management. This approach originates from integrating methodologies and tools of a collegiate educational institution and a commercial engineering enterprise, applied to educational and industrial client systems, environments, technologies, and markets. The resulting approach creates a unified framework for continuous improvement of systems.'),
(506,'Design is a hard problem: ill defined and  open-ended. Schoen [Schn 83] characterized the process of designing an artifact as a successive refinement of reflection and redesign. Critiquing - the communication of reasoned opinion about an artifact - plays a central role in the design process. A computational critiquing mechanism provides an effective form of human-computer interaction to support these important aspects of design [Fischer 91]. Systems which realize such a computational critiquing mechanism are called Critiquing Systems. These systems provide context sensitive advice and rationale for an artifact designed by a user. This is realized by delivering so-called critiques, which contain relevant information for the user to the task at hand and are some kind of rule of thumb. But design experts are not programmers and programmers are not designers. So we need a module which supports design experts in stating their knowledge in form of critiques. The basis for this module is a a visual critiquing language (here called visual CiLa), completed by a knowledge construction supporting component. Furthermore a single design expert normally does not have all existing design knowledge. So the necessary information for building a complete design system is distributed among different stakeholders. Therefore we additionally need concepts and algorithms to combine and structure the critiquing knowledge of different design experts to construct a trustful, consistent and wise codesigner. This aspect is done by a module constructing the knowledge base and a module for constructing the virtual codesigner. These two aspects - design knowledge construction and presentation - are realized in a tool called Coffein. This article deals with the way Coffein works and how it influences the design process.'),
(507,'This paper presents a new model for the specification of communicating X-machine systems (CXMS). In previous papers, systems of X-machines have been implemented in two ways: using an unique X-machine which simulates the concurrent behaviour of several processes [1], or using several X-machines which communicate through asynchronous channels [2].             <br><br>            This article introduces an X-machine system for which the communication between components is done through synchronous channels. The model supposes that each X-machine has a local memory, an input and an output tape. The X-machines act simultaneously. The states of each component of the system are partitioned into processing and communicating states. Passing messages between the X-machines involves only communicating states. It is shown that, taking advantage of the behaviour of X-machines, communication using channels may be implemented, thus providing a synchronous message passing.'),
(508,'Existing interfaces to large-scale hypermedia such as the world wide web have poor conceptual models and poor rendering of navigational and contextual information. New technologies that make it cheaper to use three-dimensional representations suggest the use of richer conceptual models. We discuss criteria for assessing more powerful conceptual models and design decisions that have to be made to exploit richer interfaces. The Treeworld model is suggested as one attractive example of such a model.'),
(509,'The objective of this paper is to identify synergy fields and relationships between knowledge management and environmental informatics. From the perspective of knowledge management many sophisticated techniques, concepts, and methodologies developed in the domain of environmental informatics can build the starting point for finding answers to open questions in knowledge management. For example, meta-knowledge management can capitalise on existing results gained in the area of metadata management, which plays a key role in environmental informatics. Up to now, many tools for knowledge processing have been applied in the domain of environmental informatics to help solve environmental problems. New knowledge management tools can improve this situation which in turn contributes directly or indirectly to a significant improvement of the protection of our environment. In order to achieve its objective, the paper introduces knowledge management with a strong focus on information technology. This introduction is followed by a literature survey on knowledge processing in environmental applications. Thereafter, several environmental information systems are analysed in the light of knowledge management. A special emphasis is placed on how geographical information systems can be used for knowledge management. Finally, the paper closes with suggestions of further areas of research in the synergy field of knowledge management and environmental informatics.'),
(510,''),
(511,'The WWW is currently considered as the most  promising and rapidly evolving software platform for the deployment of applications in wide area networks as well as enterprise intranets. Interfacing legacy systems like RDBMS to the WWW has become a very important issue to the computing industry. We discuss the efficiency of RDBMS gateways throughout periods of increased workload. We present a client/server architecture aiming to diminish overheads encountered in conventional gateways. The performance gain is assessed through a series of measurements. Alternative architectures were subject to the same measurements to assess the performance achieved by technologies like ODBC, JDBC, Dynamic SQL, ISAPI, NSAPI and CORBA.'),
(512,'Crystal lattices are infinite periodic graphs that occur naturally in a variety of geometries and which are of fundamental importance in polymer science. Discrete models of protein folding use crystal lattices to define the space of protein conformations. Because various crystal lattices provide discretizations of the same physical phenomenon, it is reasonable to expect that there will exist \"invariants\" across lattices related to fundamental properties of the protein folding process. This paper considers whether performance-guaranteed approximability is such an invariant for HP lattice models. We define a master approximation algorithm that has provable performance guarantees provided that a specific sublattice exists within a given lattice. We describe a broad class of crystal lattices that are approximable, which further suggests that approximability is a general property of HP lattice models.            <hr align=\"left\" width=\"50%\">                 1 C.S.Calude and G.Stefanescu (eds.). <i>Automata, Logic, and Computability. Special issue dedicated to Professor Sergiu Rudeanu Festschrift</i>.'),
(513,''),
(514,'We show how to capture informally stated  requirements by an ASM (Abstract State Machine) model. The model removes the inconsistencies, ambiguities and incomplete parts in the informal description without adding details which belong to the subsequent software design. Such models are formulated using application-domain-oriented terminology and standard software engineering notation and bridge the gap between the application-domain and the system design views of the underlying problem in a reliable and practical way, avoiding any formal overhead. The basic model architecture reflects the three main system parts, namely for the manual and automatic light control and for handling failures and services. We refine the ground model into a version that is executable by AsmGofer and can be used for high-level simulation, test and debugging purposes.'),
(515,''),
(516,'In this synopsis, we classify the solutions to the LCCS contained in this Special Issue according to a number of criteria. Furthermore, we provide brief descriptions of the focus of each solution, the major achievements and possible shortcomings. We leave it to the reader to establish a ranking of the different approaches, taking into account that the objectives of the contributions differ from each other and influence in particular the choice of languages, methods, and tools. Therefore, the synopsis is mainly based on information received from the authors, it does not present an in_depth analysis of the solutions. Nevertheless, we hope that this synopsis supports the reader in finding the right access to this Special Issue.'),
(517,'To date, the SCR (Software Cost Reduction)  requirements method has been used in industrial environments to specify the requirements of many practical systems, including control systems for nuclear power plants and avionics systems. This paper describes the use of the SCR method to specify the requirements of the Light Control System (LCS), the subject of a case study at the Dagstuhl Seminar on Requirements Capture, Documentation, and Validation in June 1999. It introduces a systematic process for constructing the LCS requirements specification, presents the specification of the LCS in the SCR tabular notation, discusses the tools that we applied to the LCS specification, and concludes with a discussion of a number of issues that arose in developing the specification.'),
(518,'Forest is a requirements engineering approach designed to support the creation of precise and intelligible problem specifications of reactive systems. It integrates a product model, a process model, and an editing tool. In this paper, we present the results of applying the Forest approach to the Light Control Case Study. This includes the presentation of excerpts of the resulting problem specification, as well as the discussion of the strengths and shortcomings of the Forest approach.'),
(519,'This document contains a range of needs and requirements concerning the construction of a light control system for a floor of a university building. A description of the building architecture and of some pre-installed (light-)hardware is included. This problem description was the common input for all participants of the requirements engineering case study Light Control.'),
(520,'System requirements frequently change while the system is still under development. Usually this means going back and revising the requirements specification and redoing those development steps already completed. In this article we show how formal requirements can be allowed to evolve while system development is in progress, without the need for costly redevelopment. This is done via a formalism which allows requirements engineering steps to be interleaved with formal development steps in a manageable way. The approach is demonstrated by a significant case study, the Light Control System.'),
(521,'<p>Evaluations of methods and tools applied to a reference problem are useful when comparing various techniques. In this paper, we present a solution to the challenge of capturing the requirements for the Light Control System case study, which was proposed before the Dagstuhl Seminar on <i>Requirements Capture, Documentation, and Validation</I> in June of 1999.  </P>  <P>The paper focuses primarily on how the requirements were specified: what techniques were used, and what the results were. The language used to capture the requirements is RSML<sup>-<i>e</I></sup>; a state-based specification language with a fully specified formal denotational semantics. In addition, the Nimbus environment - a toolset supporting RSML<sup>-<i>e</I></sup> - is used to visualize and execute the high-level requirements.  </P>'),
(522,'The interactive theorem prover PVS is used to formalize the user needs of the Light Control system. First the system is modeled at a high level of abstraction, in terms of properties the user can observe. After resolving ambiguities and conflicts, a refinement is defined, using dimmable light actuators. Correctness of the refinement has been proved in PVS, under the assumption that there are no internal delays. Next these internal delays are taken into account, leading to a new notion of delay-refinement which allows abstraction from delays such that systems with delays can be seen as an approximation of an undelayed specification.'),
(523,'<p>We consider the problem of identifying a class of ptime functions in efficient time. We restrict our attention to particular classes of p-time functions, called uniform and we try to identify each function of such a class by guessing, after a small number of examples, some index for it or its next value. In both cases we introduce two efficient identification paradigms, called <i>efficient</i> and <i>very efficient identification</i> respectively. We find a characterization for efficient identification and, as a corollary, we show that the entire class <i>P</i> is not efficiently identifiable. A necessary condition is shown for very efficient identification, which becomes sufficient if and only if <i>P</i> = <i>NP</i>. We give some examples of well-known uniform classes which are very efficiently identifiable in both identification paradigms.  </P>'),
(524,'<p>This article introduces the idea that <I>information compression by multiple alignment, unification and search</I> (ICMAUS) provides a framework within which natural language syntax may be represented in a simple format and the parsing and production of natural language may be performed in a transparent manner. </P>  <P>In this context, <I>multiple alignment</I> has a meaning which is similar to its meaning in bio&shy;informatics but with significant differences, while <I>unification</I> means a simple merging of matching patterns, a meaning which is related to but simpler than the meaning of that term in logic. The concept of <I>search</I> in the present context means search for alignments which are `good\' in terms of information compression, using heuristic methods or arbitrary <I>constraints</I> (or both) to restrict the size of the search space.</P>  <P>These concepts are embodied in a software model, SP61. The organisation and operation of the model are described and a simple example is presented showing how the model can achieve parsing of natural language. </P>  <P>Notwithstanding the apparent paradox of `decompression by compression\', the ICMAUS framework, without any modification, can produce a sentence by decoding a compressed code for the sentence. This is illustrated with output from the SP61 model. The article includes four other examples &shy; one of the parsing of a sentence in French and three from the domain of English auxiliary verbs. These examples show how the ICMAUS framework and the SP61 model can accommodate `context sensitive\' features of syntax in a relatively simple and direct manner. </P>  <P>An important motivation for this research is the possibility of developing the ICMAUS framework as a unifying framework for diverse aspects of computing in addition to those described in this article. Other aspects which appear to fall within the scope of the ICMAUS framework but which are outside the scope of this article, include the representation of natural language semantics, best&shy;match pattern recognition and information retrieval, deductive and probabilistic reasoning, planning and problem solving, and unsupervised inductive learning. </P>'),
(525,'Nonhomomorphicity is a new nonlinearity criterion  of a mapping or S-box used in a private key encryption algorithm. An important advantage of nonhomomorphicity over other nonlinearity criteria is that the value of nonhomomorphicity is easy to estimate by the use of a fast statistical method. Due to the Law of Large Numbers, such a statistical method is highly reliable. Major contributions of this paper are (1) to explicitly express the nonhomomorphicity by other nonlinear characteristics, (2) to identify tight upper and lower bounds on nonhomomorphicity, and (3) to find the mean of nonhomomorphicity over all the S-boxes with the same size. It is hoped that these results on nonhomomorphicity facilitate the analysis and design of S-boxes.'),
(526,''),
(527,'Since the era of vector and pipelined computing, the computational speed is limited by the memory access time. Faster caches and more cache levels are used to bridge the growing gap between the memory and processor speeds. With the advent of multithreaded processors, it becomes feasible to concurrently fetch data and compute in two cooperating threads. A technique is presented to generate these threads at compile time, taking into account the characteristics of both the program and the underlying architecture. The results have been evaluated for an explicitly parallel processor. With a number of common programs the data-fetch thread allows to continue the computation without cache miss stalls.'),
(528,'A dynamic speculative multithreaded processor automatically extracts thread level parallelism from sequential binary applications without software support. The hardware is responsible for partitioning the program into threads and managing inter-thread dependencies. Current published dynamic thread partitioning algorithms work by detecting loops, procedures, or partitioning at fixed intervals. Research has thus far examined these algorithms in isolation from one another. This paper makes two contributions. First, it quantitatively compares different dynamic partitioning algorithms in the context of a fixed microarchitecture. The architecture is a single-chip shared memory multiprocessor enhanced to allow thread and value speculation. Second, this paper presents a new dynamic partitioning algorithm called MEM-slicing. Insights into the development and operation of this algorithm are presented. The technique is particularly suited to irregular, non-numeric programs, and greatly outperforms other algorithms in this domain. MEM-slicing is shown to be an important tool to enable the automatic parallelization of irregular binary applications. Over SPECint95, an average speedup of 3.4 is achieved on 8 processors.'),
(529,'This paper presents the Data Driven Network Of Workstations (D<sup%gt;2</SUP>NOW), a multithreaded architecture that is based on the Decoupled Data Driven model of execution. This model decouples the synchronization from the computation portions of a program and allows them to execute asynchronously. At compile time a Multithreaded program is created with a Data-Driven thread synchronization graph superimposed on it.            <br><br>              D<sup%gt;2</SUP>NOW is built using commodity control-flow microprocessors. The support for the data driven synchronization of threads, is provided by the Thread Synchronization Unit (TSU). The TSU is attached in the COAST (Cache On A STick) L2 Cache slot of Pentium workstations and thus it has an implicit interface, using snooping, to the Pentium microprocessor. Workstations are connected via a Telegraphos interconnection network, which is a high throughput ATM-like switch. Telegraphos uses short packets and guarantees no packet-drop, which is a must for fine grain data-driven computation. D<sup%gt;2</SUP>NOW exhibits the tolerance to long memory and communication latencies, of the data-driven model, with very little overhead and also exploits short-term optimal cache placement and replacement policies. In our prototype implementation the TSU is implemented using FPGAs and it has very low hardware overhead.'),
(530,'Block multithreaded architectures tolerate large memory and synchronization latencies by switching contexts on every remote-memory-access or on a failed synchronization request. We study the performance of a waiting mechanism called switch-blocking where waiting threads are disabled (but not unloaded) and signalled at the completion of the wait in comparison with switch_spinning where waiting threads poll and execute in a round-robin fashion. We present an implementation of switch-blocking on a cycle-by-cycle simulator for Alewife (a block multithreaded machine) for both remote memory accesses and synchronization operations and discuss results from the simulator. Our results indicate that while switch-blocking almost always has better performance than switch-spinning, its performance is similar to switch-spinning under heavy lock contention. Support for switch-blocking for remote memory accesses may be appropriate in the future due to their strong interactions with synchronization operations.'),
(531,'<p>This paper presents an evaluation of our Scheduled Dataflow (SDF) Processor. Recent focus in the field of new processor architectures is mainly on VLIW (e.g. IA-64), superscalar and superspeculative architectures. This trend allows for better performance at the expense of an increased hardware complexity and a brute-force solution to the memory-wall problem. Our research substantially deviates from this trend by exploring a simpler, yet powerful execution paradigm that is based on dataflow concepts. A program is partitioned into functional execution threads, which are perfectly suited for our non-blocking multithreaded architecture. In addition, all memory accesses are decoupled from the thread\'s execution. Data is <I>pre-loaded</I> into the thread\'s context (registers), and all results are <I>post-stored</I> after the completion of the thread\'s execution. The decoupling of memory accesses from thread execution requires a separate unit to perform the necessary pre-loads and post-stores, and to control the allocation of hardware thread contexts to enabled threads.</P>  <P>The analytical analysis of our architecture showed that we could achieve a better performance than other classical dataflow architectures (i.e., ETS), hybrid models (e.g., EARTH) and decoupled multithreaded architectures (e.g., Rhamma processor). This paper analyzes the architecture using an instruction set level simulator for a variety of benchmark programs. We compared the execution cycles required for programs on SDF with the execution cycles required by the programs on DLX (or MIPS). Then we investigated the expected cache-memory performance by collecting address traces from programs and using a trace-driven cache simulator (Dinero-IV). We present these results in this paper.</P>'),
(532,''),
(533,'This paper considers the thread scheduling problem. The thread scheduling problem abstracts the problem of minimizing memory latency, using a directed data dependency graph generated form a compiler, to improve run time effciency. Two thread scheduling problems are formulated and shown to be strongly NP-complete. New methods and algorithms for analyzing a data dependency graph in order to compute the theoretical best runtime (lower bound of the finishing time) and to estimate the required minimum number of PEs needed to achieve certain finishing time are presented. The new methods and algorithms improve upon some of the existing analysis and transformation techniques.'),
(534,'This paper presents the use of multithreaded  processors in real-time architectures. In particular we will handle real-time applications with hard timing constraints. In our approach, events (e.g. timer interrupts, signals from the environment, etc) are distinguished into three classes according to the reaction times that have to be met. Since two of these classes are well known in real-time systems, we will focus on the new class, for which the special features of a multithreaded processor together with a real-time scheduler realized in hardware are employed. Doing so enables us to realize the handling of events from this new class in software while still meeting the demands on reaction time. Additionally, the predictability of the application and the ease of implementing them are increased. The processor, named MSparc, which we developed to support these features, is based on block multithreading and is outlined in this paper, too. We then present an architecture, designed for rapid prototyping of embedded systems, to show the feasibility of this approach. Finally, a case study shows the potential of multithreading for embedded systems.'),
(535,'<p>The investigation on Boolean operations on the stop conditions of derivation modes for cooperating distributed grammar systems is continued by considering the logical negation of such conditions. The focus is on the negation of the <i>t</i>-mode of derivation, where such non-<i>t</i>-components may stop rewriting only if they still have a production applicable to the current sentential form. In many cases, hybrid cooperating distributed grammar systems with non-<i>t</i>-components turn out to give new characterizations of the class of programmed context-free languages or recurrent programmed context-free languages, where the latter class coincides with the biologically motivated family of languages generated by ET0L systems with random context. Thus, the results presented in this paper can shed new light on some longstanding open problems in the theory of regulated rewriting.  </p>'),
(536,'A class LT 0 of functions computable in a proper sub_class of Lintime is defined, and formalized in a system LT0 of monadic and atomic (quantifier-free) logic. In spite of its poor computational complexity power and logical apparatus, this system has enough power to describe its own proof-predicate. Therefore it might qualify as smallest known system in which Gdel-like diagonalization can be applied. A proof is given that the identically true functions of LT 0 are productive. Hence this incompleteness phenomenon doesn t depend on the technicalities adopted to show it.'),
(537,'In this paper we present the notion of active documents. The basic idea is that in the future, users of documents in any networked system should not just be able to communicate with other users, but also with documents. To put it differently, we believe that communication in networks should be understood in a more general sense than it usually is. Although our notion will, at first glance, almost look like science fiction, we will show that good approximations can indeed be implemented. We conclude this short paper by pointing out a number of important applications of our new concept and mention cases where it has been applied already, successfully.'),
(538,'This article investigates the issue of structuring Z specifications. It uses examples from a large specification (the production cell) to examine both conventions for using Z and notational extensions, including Object-Z. Because of the importance of good structure within a specification, specifiers need to be aware of a range of structuring techniques and understand where each is applicable.'),
(539,'In this paper, we introduce a new technique in order to deal with cellular automata in the hyperbolic plane. The subject was introduced in [7] which gave an important application of the new possibility opened by the first part of that paper. At the same time, we recall the results that were already obtained in previous papers.              <br><br>            Here we go further in these techniques that we opened, and we give new ones that should give better tools to develop the matter.'),
(540,''),
(541,'We extend the restrictions which induce unique parsability in Chomsky grammars to accepting grammar systems. It is shown that the accepting power of global RC-uniquely parsable accepting grammar systems equals the computational power of deterministic pushdown automata. More computational power, keeping the parsability without backtracking, is observed for local accepting grammar systems satisfying the prefix condition. We discuss a simple recognition algorithm for these systems.'),
(542,'When trying to solve quantified constraints (i.e., first-order formulas over the real numbers) exactly, one faces the following problems: First, constants coming from measurements are often only approximately given. Second, solving such constraints is in general undecidable and for special cases highly complex. Third, exact solutions are often extremely complicated symbolic expressions. In this paper we study how to do approximate computation instead of working on approximate inputs and producing approximate output. For this we show how quantiffied constraints can be viewed as expressions in heterogeneous algebra and study how to do uncertainty propagation there. Since set theory is a very fundamental approach for representing uncertainty, also here we represent uncertainty by sets. Our considerations result in a general framework for approximate computation that can be applied in various different domains.'),
(543,'A dynamical systems based model of computation is studied. We demonstrate the computational capability of a class of dynamical systems called switching map systems. There exists a switching map system with two types of baker s map to emulate any Turing machines. The baker s maps are corresponding to the elementary operations of Turing machines such as left/right head-moving and read/write symbols. A connection between the generalized shifts by C. Moore [Moore 91] and the input-output mappings by L. Blum et al. [Blum, Cucker, Shub and Smale 98] is shown with our model. We present four concrete examples of switching map systems corresponding to the Chomsky hierarchy. Taking non-hyperbolic mappings as elementary operations, it is expected that the switching map systems shows a new model of computation with nonlinearity as an oracle.'),
(544,''),
(545,'In this paper we highlight three main qualities for a processing model: processing abstraction, dynamic behavior and graphical representation. We define a model closely related to high-level Petri Nets. Dynamic Relations Nets (DRN) allow the specification of data, processing, events and constraints within a unique graphical representation. Annotations of the net use a set based abstract language. Constraints arise from three levels: from places (related to the notion of abstract type), from markings (we can then express global constraints between places), and from transitions (in order to specify processing as state transformations). The DRN formalism has been successfully applied to a number of case studies. In this paper, we develop the standard `IFIP case , which has been handled with a lot of modeling methods. A DRN specification has a well defined operational semantics. Therefore a DRN can also be viewed as an executable specification of information systems. We briefly introduce a tool designed to operate an application developed with DRNs, namely NetSpec, based on the use of an active database management system. This tool allows an automated code generation (C/SQL) from a DRN specification.'),
(546,'E-LOTOS, a new version of the ISO standard specification language LOTOS, is currently being developed. We describe how it can be used to give a formal meaning to, and to discover inconsistencies in, UML models. As part of this work, we give mappings from UML constructs to E-LOTOS. Emphasis is placed on dealing with UML use case, class and interaction diagrams as these play the dominant part in the development of a UML analysis or high-level design model. Requirements are usually inconsistent and incomplete and we deal with how this can be modelled in a formal language.'),
(547,''),
(548,'Embedded Computer-based Systems are becoming  highly complex and hard to implement because of the large number of concerns the designers have to address. These systems are tightly coupled to their environments and this requires an integrated view that encompasses both the information system and its physical surroundings. Therefore, mathematical analysis of these systems necessitates formal modeling of both sides and their interaction. There exist a number of suitable modeling techniques for describing the information system component and the physical environment, but the best choice changes from domain to domain. In this paper, we propose a two-level approach to modeling that introduces a meta-level representation. Meta-level models define modeling languages, but they can also be used to capture subtle interactions between domain level models. We will show how the two-level approach can be supported with computational tools, and what kind of novel capabilities are offered.'),
(549,'We present a survey of formal specification techniques applied to the Tree Identify Protocol of the IEEE 1394 High Performance Serial Bus 1 . Specifications written in a variety of formalisms are compared with regard to a number of criteria including expressiveness, readability, standardisation, and level of analysis.'),
(550,'In this article we address the issue of confidentiality of information in the context of downgrading systems i.e. systems admitting information flow between secrecy levels only through a downgrader. Inspired by the intuition underlying the usual definition of admissible information flow, we propose an analogue based on trace equivalence as developed in the context of concurrency theory and on a modification of the usual definition of purge function. We also provide unwinding conditions to guarantee a consistent and complete proof method in terms of communicating transition systems. We take advantage of this framework to investigate its compositionality issues w.r.t. the main operators over communicating transition systems. We conclude the article with a short presentation of this work s most promising aspects in the perspective of future developments.'),
(551,'The current UML standard provides definitions for the semantics of its components. These definitions focus mainly on the static structure of UML, but they don t include an execution semantics. These definitions include several semantic variation points leaving out the door open for multiple interpretations of the concepts involved. This situation can be handled by formalizing the semantic concepts involved. In this paper we present an approach for the formalization of one of the multiple diagrams of UML, namely statechart diagrams. That is achieved by using the PVS Specification Language as formal semantics domain. We present also how the approach can be used to conduct a formal analysis using the PVS model-checker.'),
(552,'A formal specification of an algorithm is a very  rich mathematical abstraction. In general, it not only specifies an input-output relation, but also - at some level of abstraction - constrains the states and transitions associated with computing this relation. This paper explores the relationship between a formal specification of an algorithm and the many different ways in which the associated states and transitions can be embodied in physical objects and agency. It illustrates the application of principles, tools and techniques that have been developed in the Empirical Modelling Project at Warwick and considers how such an approach can be used in conjunction with a formal specification for exploration and interpretation of a subject area. As a specific example, we consider how Empirical Modelling can be helpful in gaining an understanding of a formal development of a heapsort algorithm.'),
(553,''),
(554,'Motion planning is a field of growing importance as more and more computer controlled devices are being used. Many different approaches exist to motion planning|none of them ideal in all situations. This paper considers how to convert a general motion planning problem into one of global optimisation. We regard the general problem as being the classical configuration space findpath problem, but assume that the configurations of the device can be bounded by a hierarchy of hyper-spheres rather than being explicitly computed. A program to solve this problem has been written employing Genetic Algorithms. This paper describes how this was done, and some preliminary results of using it.'),
(555,'We propose an embedding of logic programming into  lazy functional programming in which each predicate in a Prolog program becomes a Haskell function, in such a way that both the declarative and the procedural reading of the Prolog predicate are preserved.             <br><br>              The embedding computes by means of operations on lazy lists. The state of each step in computation is passed on as a stream of answer substitutions, and all the logic operators of Prolog are implemented by explicit Haskell operators on these streams. The search strategy can be changed by altering the basic types of the embedding and the implementation of these operators. This model results in a perspicuous semantics for logic programs, and serves as a good example of modularisation in functional programming.'),
(556,'This paper is an essay in axiomatic foundations for discrete geometry intended, in principle, to be suitable for digital image processing and (more speculatively) for spatial reasoning and description as in AI and GIS. Only the geometry of convexity and linearity is treated here. A digital image is considered as a finite collection of regions, regions are primitive entities (they are not sets of points). The main result (Theorem 20) shows that finite spaces are sufficient. The theory draws on both region-based topology (also known as mereotopology) and abstract convexity theory.'),
(557,'Second-order algebraic methods provide a natural and expressive formal framework in which to develop correct computing systems. In this paper we consider using second-order algebraic methods to specify real-time systems and to verify their associated safety and utility properties. We demonstrate our ideas by presenting a detailed case study of the railroad crossing controller, a benchmark example in the real-time systems community. This case study demonstrates how real-time constraints can be modelled naturally using second-order algebras and illustrates the substantial expressive power of second-order equations.'),
(558,'In previous papers we have proposed an elementary discipline of strong functional programming (ESFP), in which all computations terminate. A key feature of the discipline is that we introduce a type distinction between data which is known to be finite, and codata which is (potentially) infinite. To ensure termination, recursion over data must be well-founded, and corecursion (the definition schema for codata) must be productive, and both of these restrictions must be enforced automatically by the compiler. In our previous work we used abstract interpretation to establish the productivity of corecursive definitions in an elementary strong functional language. We show here that similar ideas can be applied in the dual case to check whether recursive function definitions are strongly normalising. We thus exhibit a powerful termination analysis technique which we demonstrate can be extended to partial functions.'),
(559,''),
(560,'New properties and implications of inclusion  systems are investigated in the present paper. Many properties of lattices, factorization systems and special practical cases can be abstracted and adapted to our framework, making the various versions of inclusion systems useful tools for computer scientists and mathematicians.      <hr align=\"left\" width=\"50%\">      1 C.S.Calude and G.Stefanescu (eds.). <i>Automata, Logic, and Computability. Special issue dedicated to Professor Sergiu Rudeanu Festschrift.</i>'),
(561,'We explore some properties related to the underlying lattice structure of pseudo-Wajsberg algebras. We establish a residuation result and we characterize the Boolean center of pseudo-Wajsberg algebras.      <hr align=\"left\" width=\"50%\">      1 C.S.Calude and G.Stefanescu (eds.). <i>Automata, Logic, and Computability. Special issue dedicated to Professor Sergiu Rudeanu Festschrift.</i>'),
(562,'In this paper we investigate the <img src=\"a_pi_calculus_machine/images/img1.gif\">-calculus  guards, proposing a formalism which use exclusively machine tradition concepts: state, resource, transition. The reduction mechanism is similar to the token-game of Petri nets. We provide a multiset semantics for the <img src=\"a_pi_calculus_machine/images/img1.gif\">-calculus by using this formalism. Moreover, our machines have a graphical representation which emphasizes their structure. As a consequence, we give a new improved graphical representation for the asynchronous <img src=\"a_pi_calculus_machine/images/img1.gif\">-calculus.      <hr align=\"left\" width=\"50%\">      1 C.S.Calude and G.Stefanescu (eds.). <i>Automata, Logic, and Computability. Special issue dedicated to Professor Sergiu Rudeanu Festschrift.</i>'),
(563,'<p>We investigate the application of Galois connections to the identification of frequent item sets, a central problem in data mining.  Starting from the notion of closure generated by a Galois connection, we define the notion of extended closure, and we use these notions to improve the classical <tt>Apriori</tt> algorithm.  Our experimental study shows that in certain situations, the algorithms that we describe outperform the <tt>Apriori</tt> algorithm. Also, these algorithms scale up linearly.  </P>       <hr align=\"left\" width=\"50%\">      1 C.S.Calude and G.Stefanescu (eds.). <i>Automata, Logic, and Computability. Special issue dedicated to Professor Sergiu Rudeanu Festschrift.</i>'),
(564,'<p>We extend the classical hidden algebra formalism by a re-arrangement of the basic concepts. This re-arrangement of the hidden algebra formalism permits an extension to novel concepts which bring new practical strength to the specification and verification methodologies. The main novel concept, which constitutes the core of this work, is that of <i>behavioural coherence</I>, which is essentially a property of preservation of behavioural structures. We define this concept and study its main denotational and computational properties, and also show how the extension of hidden algebra with behavioural coherence still accommodates the coinduction proof method advocated by classical hidden algebra and, very importantly, permits operations with several hidden sorts in the arity. The emphasis of this paper is however on the methodologies related to behavioural coherence. We present the basic methodologies of behavioural coherence by means of examples actually run under the <IMG src=\"behavioural_coherence_in_object/images/img1.gif\"> system, including many proofs with the system exiled to appendices.  </P>       <hr align=\"left\" width=\"50%\">      1 C.S.Calude and G.Stefanescu (eds.). <i>Automata, Logic, and Computability. Special issue dedicated to Professor Sergiu Rudeanu Festschrift.</i>'),
(565,'A pseudo-Boolean function (pBf) is a mapping from {0,1}^n to the real numbers. It is known that pseudo-Boolean functions have polynomial representations, and it was recently shown that they also have disjunctive normal forms (DNFs). In this paper we relate the DNF syntax of the classes of monotone, quadratic and Horn pBfs to their characteristic inequalities.      <hr align=\"left\" width=\"50%\">      1 C.S.Calude and G.Stefanescu (eds.). <i>Automata, Logic, and Computability. Special issue dedicated to Professor Sergiu Rudeanu Festschrift.</i>'),
(566,'<p>In this paper we define the <i>monadic Pavelka algebras</i> as algebraic structures induced by the action of quantifiers in Rational Pavelka predicate logic. The main result is a representation theorem for these structures.  </P>       <hr align=\"left\" width=\"50%\">      1 C.S.Calude and G.Stefanescu (eds.). <i>Automata, Logic, and Computability. Special issue dedicated to Professor Sergiu Rudeanu Festschrift.</i>'),
(567,'A study of the classes of finite relations as  enriched strict monoidal categories is presented in [CaS91]. The relations there are interpreted as connections in flowchart schemes, hence an angelic   theory of relations is used. Finite relations may be used to model the connections between the components of dataflow networks [BeS98, BrS96], as well. The corresponding algebras are slightly different enriched strict monoidal categories modeling a forward-demonic theory of relations.      <br>    In order to obtain a full model for parallel programs one needs to mix control and reactive parts, hence a richer theory of finite relations is needed. In this paper we (1) define a model of such mixed finite relations, (2) introduce enriched (weak) semiringal categories as an abstract algebraic model for these relations, and (3) show that the initial model of the axiomatization (it always exists) is isomorphic to the defined one of mixed relations. Hence the axioms gives a sound and complete axiomatization for the these relations.                 <hr align=\"left\" width=\"50%\">      1 C.S.Calude and G.Stefanescu (eds.). <i>Automata, Logic, and Computability. Special issue dedicated to Professor Sergiu Rudeanu Festschrift.</i>'),
(568,'First, the dual set of a finite prefix-free set is  defined. Using this notion we describe equivalent conditions for a finite prefix-free set to be indefinitely extendible. This lead to a simple proof for the Kraft-Chaitin Theorem. Finally, we discuss the influence of the alphabet size on the indefinite extensibility property.                 <hr align=\"left\" width=\"50%\">      1 C.S.Calude and G.Stefanescu (eds.). <i>Automata, Logic, and Computability. Special issue dedicated to Professor Sergiu Rudeanu Festschrift.</i>'),
(569,'<p>We present explicitly in this expository note the automorphism group of the hypercube <I>Q<SUB>d</SUB></I> of dimension <I>d</I> as a permutation group acting on its 2<I><SUP>d</SUP></I> nodes. This group <IMG src=\"the_automorphism_group_of/images/img1.gif\">(<I>Q<SUB>d</SUB></I>) acts on the node set <I>V<SUB>d</SUB></I> of <I>Q<SUB>d</SUB></I> and thus has degree 2<SUP>d</SUP>. It is expressed as the binary operation called exponentiation which combines the two symmetric groups <I>S</I><SUB>2</SUB> (of degree and order 2) and <I>S<SUB>d</SUB></I> (of degree <I>d</I> and order <I>d</I>!). Specifically,</P>  <CENTER><P><IMG src=\"the_automorphism_group_of/images/img1.gif\">(<I>Q<SUB>d</SUB></I>) = [<I>S</I><SUB>2</SUB>]<I><SUP>S<SUB>d</SUB></SUP></I>. </P></CENTER>  <P>has order 2<I><SUP>d</SUP>d</I>!. </P>       <hr align=\"left\" width=\"50%\">      1 C.S.Calude and G.Stefanescu (eds.). <i>Automata, Logic, and Computability. Special issue dedicated to Professor Sergiu Rudeanu Festschrift.</i>'),
(570,'<p>We introduce two chains of unary operations in the <i>MV<sub>n</sub></i> algebra of Revaz Grigolia; they will be used in establishing many connections between these algebras and <i>n</i>-valued Lukasiewicz-Moisil algebras (<i>LM<sub>n</sub></i> algebras for short). The study has four parts. It is by and large self-contained.</P>  <P>The main result of the first part is that <i>MV<sub>4</sub></i> algebras coincide with <i>LM<sub>4</sub></i> algebras. The larger class of ``relaxed\'\'-<i>MV<sub>n</sub></i> algebras is also introduced and studied. This class is related to the class of generalized <i>LM<sub>n</sub></i> pre-algebras.</P>  <P>The main results of the second part are that, for <i>n <IMG SRC=\"connection_between_mvn_algebras/images/img1.gif\"> 5</i>, any <i>MV<sub>n</sub></i> algebra is an <i>LM<sub>n</sub></i> algebra and that the canonical <i>MV<sub>n</sub></i> algebra can be identified with the canonical <i>LM<sub>n</sub></i> algebra.  <P>In the third part, the class of good <i>LM<sub>n</sub></i> algebras is introduced and studied and it is proved that <i>MV<sub>n</sub></i> algebras coincide with good <i>LM<sub>n</sub></i> algebras.</P>  <P>In the present fourth part, the class of <IMG SRC=\"connection_between_mvn_algebras/images/img2.gif\">-proper <i>LM<sub>n</sub></i> algebras is introduced and studied. <IMG SRC=\" connection_between_mvn_algebras/images/img2.gif\">-proper <i>LM<sub>n</sub></i> algebras coincide (can be identified) with Cignoli\'s proper <i>n</i>-valued Lukasiewicz algebras.  <i>MV<sub>n</sub></i> algebras coincide with <IMG SRC=\"connection_between_mvn_algebras/images/img2.gif\">-proper <i>LM<sub>n</sub></i> algebras (<i>n <IMG SRC=\"connection_between_mvn_algebras/images/img1.gif\"> 2</i>).  We also give the construction of an <i>LM<sub>3</sub></i>(<i>LM<sub>4</sub>)</i> algebra from the odd (respectively even)-valued <i>LM<sub>n</sub></i> algebra (<i>n <IMG SRC=\"connection_between_mvn_algebras/images/img1.gif\"> 5</i>), which proves that <i>LM<sub>4</sub></i> algebras are as much important than <i>LM<sub>3</sub></i> algebras; <i>MV<sub>n</sub></i> algebras help to see this point.  </P>       <hr align=\"left\" width=\"50%\">      1 C.S.Calude and G.Stefanescu (eds.). <i>Automata, Logic, and Computability. Special issue dedicated to Professor Sergiu Rudeanu Festschrift.</i>'),
(571,'In this paper, we introduce a class of substructural logics, called <i>normal substructural logics</i>, which includes not only relevant logic, BCK logic, linear logic and the Lambek calculus but also weak logics with strict implication, and de ne Kripke- style semantics (Kripke frames and models) for normal substructural logics. Then we show a correspondence between axioms and properties on frames, and give a canonical construction of Kripke models for normal substructural logics.       <hr align=\"left\" width=\"50%\">      1 C.S.Calude and G.Stefanescu (eds.). <i>Automata, Logic, and Computability. Special issue dedicated to Professor Sergiu Rudeanu Festschrift.</i>'),
(572,'Computational complementarity was introduced to mimic the physical complementarity in terms of finite automata (with outputs but no initial state). Most of the work has been focussed on \"frames\", i.e., on fixed, static, local descriptions of the system behaviour. The first paper aiming to study the asymptotical description of complementarity was restricted to certain types of sofic shifts. In this paper we continue this work and extend the results to all irreducible sofic shifts. We also study computational complementarity in terms of labelled graphs rather than automata.                 <hr align=\"left\" width=\"50%\">      1 C.S.Calude and G.Stefanescu (eds.). <i>Automata, Logic, and Computability. Special issue dedicated to Professor Sergiu Rudeanu Festschrift.</i>'),
(573,'The note presents some personal thoughts on  Professor S. Rudeanu\'s scientific and human personality.      <hr align=\"left\" width=\"50%\">      1 C.S.Calude and G.Stefanescu (eds.). <i>Automata, Logic, and Computability. Special issue dedicated to Professor Sergiu Rudeanu Festschrift.</i>'),
(574,'H systems are DNA computing models, based on the  operation of splicing. P systems are membrane computing models, where objects can evolve in parallel in a hierarchical membrane structure. In particular, the objects can be strings and the evolution rules can be based on splicing. Both H systems with certain controls on the use of splicing rules and P systems of various types are known to be computationally universal, that is, they characterize the recursively ennumerable languages. So, they are equivalent as the generative power.             <br>             The present paper presents a direct simulation of some controlled H systems by splicing P systems. We achieve this goal for three basic regulation mechanisms: H systems with permitting contexts, H systems with forbidding contexts, and communicating distributed H systems. We can say that in this way we get a uniform implementation of the three types of H systems in the form of a computing cell.                 <hr align=\"left\" width=\"50%\">      1 C.S.Calude and G.Stefanescu (eds.). <i>Automata, Logic, and Computability. Special issue dedicated to Professor Sergiu Rudeanu Festschrift.</i>'),
(575,'Generalizations of Craig interpolation are investigated for equational logic. Our approach is to do as much as possible at a categorical level, before drawing out the concrete implications.             <hr align=\"left\" width=\"50%\">      1 C.S.Calude and G.Stefanescu (eds.). <i>Automata, Logic, and Computability. Special issue dedicated to Professor Sergiu Rudeanu Festschrift.</i>'),
(576,'While the existence of inverses is a natural  condition in Algebra it is seldom satisfied in Computer Science applications. Since the group-theoretical orientation has to be abandoned we consider an advantage when the non-conventional structures needed are linked to an already existing knowledge. We propose semirings as a candidate and we aim at the Computer Science applications such as processes semantics, parallel composition, Fuzzy Theory, images ordering or MV-algebras. After the definition of pa-ordered semiring four typical examples are given. Some results concerning additively idempotent semirings are extended to monoids considered as their natural background. A direct sum representation is given for lower semilattice-ordered Gelfand semirings s-ordered. A sufficient condition is given for having the natural quasi-order an s-order. A multiplicative ordering is built up and its application to Visual Data is indicated. Wrt complements in pa-semirings we give sufficient conditions for the existence of some sums and for commutativity.            <hr align=\"left\" width=\"50%\">      1 C.S.Calude and G.Stefanescu (eds.). <i>Automata, Logic, and Computability. Special issue dedicated to Professor Sergiu Rudeanu Festschrift.</i>'),
(577,'Extractors are a special type of binary graphs that can be utilized to improve the quality of randomness sources that generate strings with small entropy. The paper explores constructions of extractors that are practical and easy to implement. Randomized and deterministic constructions are presented and compared with some previously known constructions that achieve very good asymptotical performances. One of our methods is shown to have a better behavior for reasonable values of the involved parameters.                 <hr align=\"left\" width=\"50%\">      1 C.S.Calude and G.Stefanescu (eds.). <i>Automata, Logic, and Computability. Special issue dedicated to Professor Sergiu Rudeanu Festschrift.</i>'),
(578,'<p>We continue the investigation of the power of the computability models introduced in [12] under the name of <I>transition super-cell systems</I>. We compare these systems with classic mechanisms in formal language theory, context-free and matrix grammars, <TT>E0L</TT> and <TT>ET0L</TT> systems, interpreted as generating mechanisms of number relations (we take the Parikh image of the usual language generated by these mecha- nisms rather than the language). Several open problems are also formulated.</P>'),
(579,''),
(580,'We investigate the application of automated deduction  techniques to retrieve software components based on their formal specifications. The application profile has major impacts on the problem solving process and requires an open system architecture in which different deductive engines work in combination because the proof problems are too difficult for a single monolithic system. We describe our system architecture, a pipeline of filters of increasing deductive strength, and concentrate on the final filter, in which theorem provers are applied. Here, we use the Ilf-system as a control and integration shell to combine different provers. We support two different combination styles, competition and cooperation. Experiments confirm our approach. With moderate timeouts we already achieve an overall recall of approximately 80%.'),
(581,'This paper reports on the integration of the higher-order theorem proving environment Tps [Andrews et al., 1996] into the mathematical assistant Omega [Benzmueller et al., 1997] . Tps can be called from Omega either as a black box or as an interactive system. In black box mode, the user has control over the parameters which control proof search in Tps, in interactive mode, all features of the Tps-system are available to the user. If the subproblem which is passed to Tps contains concepts defined in Omega\'s database of mathematical theories, these definitions are not instantiated but are also passed to Tps. Using a special theory which contains proof tactics that model the ND-calculus variant of Tps within Omega, any complete or partial proof generated in Tps can be translated one to one into an Omega proof plan. Proof transformation is realised by proof plan expansion in Omega\'s 3-dimensional proof data structure, and remains transparent to the user.'),
(582,'Real-world applications of automated theorem proving require modern software environments that enable modularisation, networked inter-operability, robustness, and scalability. These requirements are met by the Agent-Oriented Programming paradigm of Distributed Artificial Intelligence. We argue that a reasonable framework for automated theorem proving in the large regards typical mathematical services as autonomous agents that provide internal functionality to the outside and that, in turn, are able to access a variety of existing external services. This article describes the MathWeb architecture that encapsulates a wide range of traditional mathematical systems each into a social agent-shell. A communication language based on the Knowledge Query and Manipulation Language (KQML) is proposed in order to allow conversations between these mathematical agents. The individual speech acts of their conversations are about performances of the encapsulated services. The objects referred by these speech acts are mathematical objects, formulae in various log_ ics, and (partial) proofs in different calculi whose formalisation is done in an extension to the OpenMath standard. The result is a flexible framework for automated theorem proving which has already been implemented to a large extent in the context of the Omega proof development system.'),
(583,'Logic has become a cross-sectional formal specifcation language for applications in Artficial Intelligence, Computing, and Mathematics.  Deduction is the corresponding derivation mechanism used to execute and analyse formal models, to predict properties, to generate plans or to detect errors. For about 35 years, researchers have developed different kinds of computational logics, calculi and computer programs for interactive and fully automated deduction.'),
(584,'We present a uniform procedure for proof search in classical logic, intuitionistic logic, various modal logics, and fragments of linear logic. It is based on matrix characterizations of validity in these logics and extends Bibel\'s connection method, originally developed for classical logic, accordingly. Besides combining a variety of different logics it can also be used to guide the development of proofs in interactive proof assistants and shows how to integrate automated and interactive theorem proving.'),
(585,'Formal specification and verification techniques  can improve the quality of object-oriented software by enabling semantic checks and certification of properties. To be applicable to object-oriented programs, they have to cope with subtyping, aliasing via object references, as well as abstract and recursive methods. For mastering the resulting complexity, mechanical aid is needed.   <br><br>      The article outlines the specific technical requirements for the specification and verification of object-oriented programs. Based on these requirements, it argues that verification of OO-programs should be done interactively and develops an modular architecture for this task. In particular, it shows how to integrate interactive program verification with existing universal interactive theorem provers, and explains the new developed parts of the architecture. To underline the general approach, we describe interesting features of our prototype implementation.'),
(586,'A generic tableau prover has been implemented and integrated with Isabelle (Paulson, 1994). Compared with classical first-order logic provers, it has numerous extensions that allow it to reason with any supplied set of tableau rules. It has a higher-order syntax in order to support user-defined binding operators, such as those of set theory. The unification algorithm is first-order instead of higher-order, but it includes modifications to handle bound variables. The proof, when found, is returned to Isabelle as a list of tactics. Because Isabelle verifies the proof, the prover can cut corners for efficiency\'s sake without compromising soundness. For example, the prover can use type information to guide the search without storing type information in full.'),
(587,'Logic calculi, and Gentzen-type calculi in particular, can be categorised into two types: search-oriented and interaction-oriented calculi. Both these types have certain inherent characteristics stemming from the purpose for which they are designed. In this paper, we give a general characterisation of the two types and present two calculi that are typical representatives of their respective class. We introduce a method for transforming proofs in the search-oriented calculus into proofs in the interaction-oriented calculus, and we demonstrate that the difficulties arising with devising such a transformation do not pertain to the specific calculi we have chosen as examples but are general. We also give examples for the application of our transformation procedure.'),
(588,'Automated reasoning systems often suffer from redundancy: similar parts of derivations are repeated again and again. This leads us to the problem of loop-detection, which clearly is undecidable in general. Nevertheless, we tackle this problem by extending the hyper-tableau calculus as proposed in [Baumgartner, 1998] by generalized terms with exponents, that can be computed by means of computer algebra systems. Although the proposed loop-detection rule is incomplete, the overall calculus remains complete, because loop-detection is only used as an additional, optional mechanism. In summary, this work combines approaches from tableau-based theorem proving, model generation, and integrates computer algebra systems in the theorem proving process.'),
(589,'The Culik-Kari recursive inference algorithm for  WFA is based on an effcient way of expressing subsquares of the given image as linear combinations of available states. Here we improve it in two ways. First, we allow the use of rotations, flippings and negations of the states in the linear combination. Second, in order to get the best possible representation of simple fractal images we allow the creation of edges pointing to ancestors of states under construction which, for technical reasons, was not done in the original recursive algorithm.'),
(590,'Data restructuring is often an integral but non-trivial part of information processing, especially when the data structures are fairly complicated. This paper describes the underpinnings of a program, called the Restructurer, that relieves the user of the \"thinking and coding\" process normally associated with writing procedural programs for data restructuring. The process is accomplished by the Restructurer in two stages. In the first, the differences in the input and output data structures are recognized and the applicability of various transformation rules analyzed. The result is a plan for mapping the specified input to the desired output. In the second stage, the plan is executed using embedded knowledge about both the target language and run-time efficiency considerations. The emphasis of this paper is on the planning stage. The restructuring operations and the mapping strategies are informally described and explained with mathematical formalism. The notion of solution of a set of instantiated forms with respect to an output form is then introduced. Finally, it is shown that such a solution exists if and only if the Restructurer produces one.'),
(591,''),
(592,'Demand-driven simulation is an approach to the simulation of digital logic circuits that was proposed, independently, in the work of several authors. Experimental studies of the paradign have indicated that this approach may reduce the time required for simulation, when compared with event-driven techniques. In this paper we present some analytic support for these experimental results by analysing the average number of gates evaluated with a naive demand-driven algorithm for formula evaluation.'),
(593,'Logic gates with three input bits and three output bits have a privileged position within fundamental computer science: they are a sufficient building block for constructing arbitrary reversible boolean networks and therefore are the key to reversible digital computers. Such computers can, in principle, operate without heat production. As there exist as many as 8! = 40,320 different 3-bit reversible truth tables, the question arises as to which ones to choose as building blocks. Because these gates form a group with respect to the operation \"cascading\" , we can apply group theoretical tools, in order to make such a choice.'),
(594,''),
(595,'Floating point expansion is a technique for  implementing multiple precision using a processor\'s floating point unit instead of its integer unit. Research on this subject has arised recently from the observation that the floating point unit becomes a more and more efficient part of modern computers. Many simple arithmetic operators and some very useful geometric operators have already been presented on expansions. Yet previous work included only a very simple division algorithm. We present in this work a new algorithm that allows us to extend the set of geometric operators with Bareiss\' determinant on a matrix of size between 3 and 10. Running times with different determinant algorithms on different machines are compared with GMP, a very common multi-precision package.'),
(596,'The goal of the ORGTECH-project is to improve the cooperation between two engineering offices on the one hand, and a major German steel mill, on the other. An integrated change process has been initiated which combines the introduction of a groupware application with methods of organization development. This change process draws on the framework of Integrated Organization and Technology Development which is presented first. Then we describe its application in the ORGTECH project. The results of the first project phase are presented and discussed.'),
(597,''),
(598,'<p>A multi-secret sharing scheme is a protocol to share a number of (arbitrarily related) secrets among a set of participants in such a way that only qualified sets of participants can recover the secrets, whereas non-qualified sets of participants might have partial information about them.  </p>  <p>In this paper we analyze the amount of randomness needed by multi-secret sharing schemes. Given an <i>m</i>-tuple of access structures, we give a lower bound on the number of random bits needed by multi-secret sharing schemes; the lower bound is expressed in terms of a combinatorial parameter that depends only upon the access structures and not on the particular multi-secret sharing scheme used.  </P>'),
(599,'The WWW is currently considered as the most promising and rapidly evolving software platform for the deployment of applications in wide area networks (telematics) as well as enterprise intranets. Another technology that proliferates rapidly in such environments is wireless communication networks (e.g., GSM, wireless LANs). The combination - merging of the two technologies (i.e., mobile computing based on the WWW) is considered of major importance to the computing industry for the forthcoming years. A survey of the research activities undertaken in this area over the previous years is reported. The different approaches that have been proposed to overcome the associated technical difficulties at different layers of the OSI reference model (e.g., the application layer, the transport/network layer) are described and compared. The presented architectures incorporate mechanisms and ideas extensively used in the area of mobile computing (e.g., caching, pre-fetching).'),
(600,'This article introduces the idea that probabilistic reasoning (PR) may be understood as information compression by multiple alignment, unification and search (ICMAUS). In this context, multiple alignment has a meaning which is similar to but distinct from its meaning in bio-informatics, while unification means a simple merging of matching patterns, a meaning which is related to but simpler than the meaning of that term in logic. <br><br>       A software model, SP61, has been developed for the discovery and formation of \"good\"  multiple alignments, evaluated in terms of information compression. The model is described in outline.  <br><br>       Using examples from the SP61 model, this article describes in outline how the ICMAUS framework can model various kinds of PR including: PR in best-match pattern recognition and information retrieval, one-step \"deductive\"  and \"abductive\"  PR, inheritance of attributes in a class hierarchy, chains of reasoning (probabilistic decision networks and decision trees, and PR with \"rules\" ), geometric analogy problems, nonmonotonic reasoning and reasoning with default values, modelling the function of a Bayesian network.'),
(601,''),
(602,'A version of the communicating stream X-machine model is proposed, which gives a precise representation of the operation of transferring data from one X-machine to another. For this model it is shown that systems of communicating X-machines have the same computational power as single stream X-machines. This enable existing methods for deriving test strategies for stream X-machines to be extended to communicating stream X-machines.'),
(603,'Pollypodes is an algebraic structure in between monoids and <IMG SRC=\"an_introduction_to_polypodic/images/img1.gif\"> - <i>a</i> lg <i>ebras</i> having the advantages of both of them. Many objects of different nature such as words, trees, graphs, functions, etc, can be studied in common into the framework of polypodes.'),
(604,'<p>Iterated Function Systems (IFSs) are among the best&shy;known methods for constructing fractals. The sequence of pictures <I>E</I><SUB>0</SUB> , <I>E</I><SUB>1</SUB> , <I>E</I><SUB>2</SUB> , ... generated by an IFS {<I>X</I>; <I>f</I><SUB>1</SUB> , <I>f</I><SUB>2</SUB> , ... , <I>f</I><SUB>t </SUB>} converges to a unique limit <IMG SRC=\"shrink_indecomposable_fractals/images/img1.gif\">, which is independent of the choice of starting set <I>E</I><SUB>0</SUB>, but completely determined by the choice of the maps <I>f<SUB>i </SUB></I>. </P>  <P>Random context picture grammars (rcpgs) are a method of syntactic picture generation. The terminals are subsets of the Euclidean plane and the replacement of variables involves the building of functions that will eventually be applied to terminals. Context is used to enable or inhibit production rules. </P>  <P>We show that every IFS can be simulated by an rcpg that uses inhibiting context only. Since rcpgs use context to control the sequence in which functions are applied, they can generate a wider range of fractals or, more generally, pictures than IFSs. We give an example of such a fractal. Then we show that under certain conditions the sequence of pictures generated by an rcpg converges to a unique limit. </P>'),
(605,'<p>For any language <i>L</i> over an alphabet <i>X</i>, we define the root set, <i>root(L)</i> and the degree set, <i>deg(L)</i> as follows: (1) <i>root(L)</i> = <IMG src=\"decidable_and_undecidable_problems/images/img2.gif\"> where <i>Q</i> is the set of all primitive words over <i>X</i>, (2) <i>deg(L)</i> = <IMG src=\"decidable_and_undecidable_problems/images/img2.gif\">. We deal with various decidability problems related to root and degree sets.  </p>'),
(606,'In this article we show that the parallel  processing of pictures requires extending the notion of pictures with blank spaces. For the purposes of this article, parallel processing is defined in terms of a product construction.'),
(607,'This paper deals with solutions of algebraic, linear, and rational systems of equations over an <img src=\"some_algebraic_structures_with/images/img1.gif\">-complete semiring, and their iteration lemmata. These are guaranteed if the underlying structure has an associative multiplicative operation, and its elements have a norm. A number of such structures like words, vectors, traces, trees, graphs, are presented.'),
(608,'In this paper, we define cellular automata on a grid of the  hyperbolic plane, based on the tessellation obtained from the regular pentagon with right angles. Taking advantage of the properties of that grid, we show that 3-SAT can be solved in polynomial time in that setting, and then we extend that result for any NP problem. Several directions starting from that result are indicated.'),
(609,'In this paper we extend the notion of  homomorphisms and decomposition to timed automata. This is based on the classical Hartmanis-Stearns decomposition results for finite state automata. As in the classical theory, the existence of non-trivial orthogonal partitions is both necessary and sufficient for non-trivial decompositions. Of course, now these partitions have to include both the set of states and the set of timers (or clocks) in the system. We present an example which illustrate the various issues.'),
(610,''),
(611,'The ambient calculus and the P-systems are models developed in different areas of computer science. Still, they are based on similar concepts and structures and are inspired from the same <i>natural</i> model of computation [BeBo92]. On this basis, we point out how to transfer ideas and results from one framework to the other. We prove that any P-system can be simulated in ambient calculus. We also introduce the notion of mobile P-systems, suitable to model and motivate security features for membrane computing.'),
(612,'The closure under the splicing operation with finite and recognizable sets of rules, is extended to the family of generalized synchronized forests. Moreover, we investigate the application of the iterated splicing on known families of forests. Interesting properties of this operation are established.'),
(613,'Synchronization expressions (SEs) were originally  developed as practical high-level constructs for specifying synchronization constraints between parallel processes. The family of synchronization languages was introduced to give a precise semantic description for synchronization expressions. In addition to its use for defining the meaning of SEs, the family of synchronization languages is interesting on its own from a formal languages point of view. We consider two variants of the definition of synchronization languages, and survey characterization results for the language families. Synchronization languages also provide us a systematic approach for the implementation and simplification of SEs.'),
(614,'Generally, programming problems are formally described as function computation problems. In this paper they are viewed as language recognition problems. More precisely, we sugget to specify types, and programs using the concept of languages of concatenation of level n, i.e, languages built from regular languages on which language transformation operations are applied to them. Regular languages denoted by regular expressions allow an easy connection between those languages of concatenation and programming. This connection is naturally done via recurrence relations. We explain our approach through some examples.'),
(615,'Microprocessor-based embedded systems are increasingly used to control safety-critical systems (e.g., air and railway traffic control, nuclear plant control, aircraft and car control). In this case, fault tolerance mechanisms are introduced at the hardware and software level. Debugging and verifying the correct design and implementation of these mechanisms ask for effective environments, and Fault Injection represents a viable solution for their implementation. In this paper we present a Fault Injection environment, named FlexFI, suitable to assess the correctness of the design and implementation of the hardware and software mechanisms existing in embedded microprocessor-based systems, and to compute the fault coverage they provide. The paper describes and analyzes different solutions for implementing the most critical modules, which differ in terms of cost, speed, and intrusiveness in the original system behavior.'),
(616,''),
(617,'Proving that a program suits its specification and thus can be called <i>correct</i> has been a research subject for many years resulting in a wide range of methods and formalisms. However, it is a common experience that even systems which have been proven correct can fail due to physical faults occurring in the system. As computer programs control an increasing part of todays critical infrastructure, the notion of correctness has been extended to <i>fault tolerance</i>, meaning correctness in the presence of a certain amount of faulty behavior of the environment. Formalisms to verify fault-tolerant systems must model faults and faulty behavior in some form or another.  Common ways to do this are based on a notion of <i>transformation</i> either at the program or the specification level. We survey the wide range of formal methods to verify fault-tolerant systems which are based on some form of transformation. Our aim is to classify these methods, relate them to one another and, thus, structure the area. We hope that this might faciliate the involvement of researchers into this interesting field of computer science.'),
(618,'The Reliability analysis of information and automation systems has to cope with complex system structures and a large number of different components. Adapted to these requirements, the Markovian minimal cut approach has been developed. The Markovian minimal cut approach combines the advantages of two well-known approaches, the minimal cut and the Markovian path approach. The minimal cut approach allows the efficient evaluation of large scale systems. The Markovian path approach is able to model and evaluate real operation and outage behavior under realistic conditions. It includes outage and disconnection partitions, maintenance strategies (inspection, maintenance, repair), and operation and control strategies, which may lead to complicated stochastic dependencies. The Markovian minimal cut approach reduces the modelling and evaluation effort of real systems significantly because only a small number of Markovian states have to be modelled. In some applications the use of this approach first makes it possible to model and calculate the reliability of the system. The error of the approximations, induced by the Markovian path models have been proven to be less than 0.1% in practical systems. The approximations give the advantage of a result in an analytical context vs. pure computer-based numerical- or simulation-methods.'),
(619,'The so-called Electronic Throttle Control unit was a big step towards reducing important parameters like fuel consumption or exhaust emmission. Due to its safety-criticality, a dependability study was initiated by the manufacturer Siemens Automotive. As the most important result, values could be stated for the quantitative estimation of the safety-critical and the availability-relevant cases. The study was based on the existing safety concept, but after this study, a slightly changed system architecture of the ECU was proposed to VDA (Verband der Automobilindustrie), which enhances availability and safety of the ECU significantly, at about the same cost. For this study, a new kind of Markov evaluation method was used, called TEFT (Time-Extended Fault Trees). This was necessary to deal with concepts like multiple faults, faulty states, and time. In this paper, the questions raised by the Electronic Throttle Control system are described, together with our way to solve these problems.'),
(620,'The expanding application of computing systems and the continuing advances in semiconductor technology are forcing the on-chip inclusion of design for dependability features (concurrent fault tolerance). These features detect, log and provide recovery from errors induced by faults concurrently with the operation of the system. A very difficult task is the hardware validation of concurrent fault tolerant design features in a nondestructive, unobtrusive manner. A semi-automated facility has been developed at the University of South Florida for this purpose using Laser Fault Injection (LFI) to simulate soft errors during system operation. The facility provides means to extract target coordinates from the CAD database, synchronize the laser pulse with the operation of and capture health warnings issued by the system under test. The key fault tolerant features (automatic instruction retry on single soft errors) of a state of the art fault tolerant computer for space applications were validated at this facility.'),
(621,'Fault trees show which joint components\' faults mean system faults. Fault trees can often be used to determine dependability parameters of systems. Here it is shown that i) binary decision diagrams (BDDs) can also be used to calculate system mean failure frequency, ii) modeling dynamics of fault trees does not always mean Markov modeling, iii) a deeper understanding of interrelations between s-dependent components is supported, rather, by Petri nets than by state transition graphs.'),
(622,'An arbitrary number of points are arranged in a given twodimensional simply connected region such that their mutual distances and the distance from the region boundary becomes a maximum.'),
(623,'Laha distribution has been introduced in 1958 as an example of a non-normal distribution where the quotient follows the Cauchy law. In this paper we present two procedures for the computer generation of this distribution and we discuss its applications to life time modelling.'),
(624,'Institutions were introduced by Goguen and Burstall [GB84, GB85, GB86, GB92] to formally capture the notion of logical system. Interpreting institutions as functors, and morphisms and representations of institutions as natural transformations, we give elegant proofs for the completeness of the categories of institutions with morphisms and representations, respectively, show that the duality between morphisms and representations of institutions comes from an adjointness between categories of functors, and prove the cocompleteness of the categories of institutions over small signatures with morphisms and representations, respectively.'),
(625,''),
(626,'This paper computationally obtains optimal  bounded-weight, binary, error-correcting codes for a variety of distance bounds and dimensions. We compare the sizes of our codes to the sizes of optimal constant-weight, binary, error-correcting codes, and evaluate the differences.'),
(627,'<p>In the early 1980s, Selman\'s seminal work on positive Turing reductions showed that positive Turing reduction to NP yields no greater computational power than NP itself.  Thus, positive Turing and Turing reducibility to NP differ sharply unless the polynomial hierarchy collapses.</p>  <p>We show that the situation is quite different for DP, the next level of the boolean hierarchy.  In particular, positive Turing reduction to DP already yields all (and only) sets Turing reducibility to NP.  Thus, positive Turing and Turing reducibility to DP yield the same class.  Additionally, we show that an even weaker class, P<SUP>NP[1]</SUP>, can be substituted for DP in this context.</p>   </P>'),
(628,'In this paper, a genetic algorithm based method is proposed to solve the problem of minimizing the PCB assembly time for multi-head surface mounting machines. By grouping the reels and by clustering the components the multi-head problem is transformed into a single_head one and then the single-head method is simply applied to the multi-head case. To implement the genetic algorithm, a partial link concept is proposed for genetic operations. Computer simulation results show that the proposed algorithm is superior to the heuristic algorithm that is currently used in industry.'),
(629,''),
(630,'This paper is a short description of HIKS, a working prototype of an interactive knowledge management system. HIKS might be used as a dynamic background library in a Web-based training environment. Research in the field of Web-based learning at the [IICM] has shown that courseware combined only with static background libraries does not satisfy the learners\' needs. An additional dynamic background library will always guarantee up-to-date knowledge. Relevant knowledge spaces from the biggest knowledge store, the Internet, will be extracted by HIKS. The core of this system is a sophisticated information gatherer and knowledge area broker system, which will be combined with a Hyperwave-based distance training system. This paper describes the technique of the gatherer and broker and their interaction with the learning process. Furthermore, the realisation of knowledge hierarchy for specific topics and co-operations between organisations is shown. <br><br>      A short version of this paper was accepted for presentation by the ICCE 98 conference. The research described in this paper was conducted as part of the IICM Knowledge Discovery Project, supported by the Austrian Federal Ministry of Science and Transport.'),
(631,'We show that it is decidable whether or not a given D0L power series over a semiring A is A-algebraic in case A = Q+ or A = N. The proof relies heavily on the use of elementary morphisms in a power series framework and gives also a new method to decide whether or not a given D0L language is context-free.'),
(632,'Abstract: Efficient hashing is a centerpiece of modern Cryptography. The progress in computing technology enables us to use 64-bit machines with the promise of 128-bit machines in the near future. To exploit fully the technology for fast hashing, we need to be able to design cryptographically strong Boolean functions in many variables which can be evaluated faster using partial evaluations from the previous rounds. We introduce a new class of Boolean functions whose evaluation is especially efficient and we call them rotation symmetric. Basic cryptographic properties of rotation-symmetric functions are investigated in a broader context of symmetric functions. An algorithm for the design of rotation-symmetric functions is given and two classes of functions are examined. These classes are important from a practical point of view as their forms are short. We show that shortening of rotation_symmetric functions paradoxically leads to more expensive evaluation process.'),
(633,''),
(634,'The Duration Calculus is an interval logic introduced for designing real-time systems. This calculus is able to capture important real-time problems like the specification of the behaviours of schedulers and classical examples like a gas burner. From a practical point of view an important challenge becomes to define automated proof procedures for this calculus. Since the propositional calculus is undecidable, we are interested then into isolating decidable fragments of this calculus. A first fragment was given and its decidability proved via regular languages. In this paper we isolate another fragment and we define a tableau method which gives a natural procedure to decide whether a given formula is satisfiable. This fragment is strong enough to embed Allen\'s Interval Algebra.'),
(635,'A matrix method is extended to include the detection of logic hazards in combinational logic circuits involving EX-OR gates. Essentially, the method generates 0- and 1-sets, or P- and S-sets, of all nodes in each gate level of a circuit progressively until it reaches the output of the circuit. The sets generated are subsequently used to determine the existence of static or dynamic hazards.'),
(636,'This paper argues that the operations of a `Universal Turing Machine\' (UTM) and equivalent mechanisms such as the `Post Canonical System\' (PCS)  which are widely accepted as definitions of the concept of `computing\'  may be interpreted as <i>information compression by multiple alignment, unification and search</i> (ICMAUS).  The motivation for this interpretation is that it suggests ways in which the UTM/PCS model may be augmented in a proposed new computing system designed to exploit the ICMAUS principles as fully as possible. The provision of a relatively sophisticated search mechanism in the proposed `SP\' system appears to open the door to the <i>integration</i> and <i>simplification</i> of a range of functions including unsupervised inductive learning, best-match pattern recognition and information retrieval, probabilistic reasoning, planning and problem solving, and others. Detailed consideration of how the ICMAUS principles may be applied to these functions is outside the scope of this article but relevant sources are cited in this article.'),
(637,''),
(638,'<p>If I<sub>n</sub> is the approximation of a definite integral <IMG SRC=\"dynamical_control_of_computations/abstract_images/img1.gif\"> with step <IMG SRC=\"dynamical_control_of_computations/abstract_images/img2.gif\"> using the trapezoidal rule (respectively Simpson\'s rule), if C<sub>a,b</sub> denotes the number of significant digits common to a and b, we show, in this paper, that <IMG SRC=\"dynamical_control_of_computations/abstract_images/img3.gif\"> <IMG SRC=\"dynamical_control_of_computations/abstract_images/img4.gif\">  <P> According to the previous theorems, using the CADNA library which allows on computers to estimate the round-off error effect on any computed result, we can compute dynamically the optimal value of n to approximate I and we are sure that the exact significant digits of I<sub>n</sub> are in common with the significant digits of I.'),
(639,'This paper describes the use of interval arithmetic to bound errors in an experiment for determining Newton\'s constant of gravitation. Using verified Gaussian quadrature we were able to assess the numerical errors as well as the effect of several tolerances in the physical experiment.'),
(640,'In this note we present rapidly convergent algorithms depending on the method of arithmetic-geometric means (AGM) for the computation of Jacobian elliptic functions and Jacobi\'s Theta-function. In particular, we derive explicit a priori bounds for the error accumulation of the corresponding Landen transform.'),
(641,'We introduce and study abstract algebraic systems generalizing the arithmetic systems of intervals and convex bodies involving Minkowski operations such as quasimodules and quasilinear systems. Embedding theorems are proved and computational rules for algebraic transformations are given.'),
(642,''),
(643,'The arithmetic on the extended set of proper and improper intervals is an algebraic completion of the conventional interval arithmetic and thus facilitates the explicit solution of certain interval algebraic problems. Due to the existence of inverse elements with respect to addition and multiplication operations certain interval algebraic equations can be solved by elementary algebraic transformations. The conditionally distributive relations between extended intervals allow that complicated interval algebraic equations, multi-incident on the unknown variable, be reduced to simpler ones. In this paper we give the general type of \"pseudo-linear\" interval equations in the extended interval arithmetic. The algebraic solutions to a pseudo-linear interval equation in one variable are studied. All numeric and parametric algebraic solutions, as well as the conditions for nonexistence of the algebraic solution to some basic types pseudo-linear interval equations in one variable are found. Some examples leading to algebraic solution of the equations under consideration and the extra func- tionalities for performing true symbolic-algebraic manipulations on interval formulae in a <I>Mathematica</I> package are discussed.'),
(644,'<p>A perturbation matrix <IMG SRC=\"componentwise_distance_to_singularity/abstract_images/img1.gif\"> is considered, where <IMG SRC=\"componentwise_distance_to_singularity/abstract_images/img2.gif\"> and <IMG SRC=\"componentwise_distance_to_singularity/abstract_images/img3.gif\">. The matrix <IMG SRC=\"componentwise_distance_to_singularity/abstract_images/img4.gif\"> is singular iff <IMG SRC=\"componentwise_distance_to_singularity/abstract_images/img4.gif\"> contains a real singular matrix. A problem is to decide if <IMG SRC=\"componentwise_distance_to_singularity/abstract_images/img4.gif\"> is singular or nonsingular, a NP-hard problem. The decision can be made by the computation of the componentwise distance to the nearest singular matrix defined on the basis of the real spectral radius, and by the solution of 4<sup>n</sup> eigenvalue problems.  <P> <i>Theorem 6</i> gives a new computation basis, a natural way to the \"componentwise distance ...\" definition, and a motivation to rename this in <I>radius of singularity</I> denoted by <I>sir</I> <IMG SRC=\"componentwise_distance_to_singularity/abstract_images/img5.gif\">.</P>  <P> This new way shows: (i) - <I>sir</I> results from a real nonnegative eigensolution of a non-linear mapping, (ii) - <I>sir</I> has a norm representation, (iii) - <I>sir</I> can be computed by 2<sup>n-1</sup> nonnegative eigensolutions of the nonlinear mapping, (iv) - for the special case <IMG SRC=\"componentwise_distance_to_singularity/abstract_images/img6.gif\"> a formula for a computation of sir is given, also a trivial <i>algorithm</i> for the computation, and some examples as demonstration. </P>'),
(645,'This paper presents a proposal of measuring uncertain modelling referred to a geometrical model obtained from a <i>Coordinate Measuring Machine (CMM)</i>. The measurement of objects by <i>CMM</i> is achieved considering a particular region of measurement described through a set of finitely many points which defines the ideal reference mesh and it is exposed to, at least, three kinds of error: systematical, aleatory and rounding error. For each point of this mesh is associated the measure error bounds.  Those ones have two components: error direction, and error numerical value. The measure error bound identification, as showed in this paper, makes possible to know about the best region to measure a specific object with smallest error bound.'),
(646,'In this article we briefly discuss the advantages and disadvantages of the language Java for scientific computing. We concentrate on Java\'s type system, investigate its support for hierarchical and generic programming and then discuss the features of its floating-point arithmetic. Having found the weak points of the language we propose workarounds using Java itself as long as possible.'),
(647,'A classical error estimation technique is used  together with an interactive visualization software to validate numerical solutions of ordinary differential equations in the complex field. Examples showing how a modification of the integration path can help to reduce the absolute value of the global error committed by the integration process are provided.'),
(648,'SNAP - The Stanford Sub-nanosecond arithmetic processor is an interdisciplinary effort to develop validated theory, and tools for realizing an arithmeti c processor with execution rates under 1ns. The project has targeted the full spectrum of tradeoffs from algorithms, circuit optimizations, system issues, and development of metrics to characterize processors.'),
(649,'In the Fourier series approximation of real functions discontinuities of the functions or their derivatives cause problems like Gibbs phenomenon or slow uniform convergence. In the case of a finite number of isolated discontinuities the problems can be to a large extend rectified by using periodic splines in the series. This modified Fourier series (Spline-Fourier series) is applied to the numerical solution of the wave equation (in periodic form) where discontinuities in the data functions or their derivatives appear quite often. The solution is sought in the form of a Spline-Fourier series about the space variable and close bounds are obtained using a certain iterative procedure of Newton type.'),
(651,'This paper discusses the processing of non-linear  polynomial systems using a branch and prune algorithm within the framework of constraint programming. We propose a formalism for a kind of branch and prune algorithm implementing symbolic and numerical methods to reduce the systems with respect to a relation defined from both inclusion of variable domains and inclusion of sets of constraints. The second part of the paper presents an instantiation of this general scheme. The pruning step is implemented as a cooperation of factorizations, substitutions and partial computations of Groebner bases to simplify the systems, and interval Newton methods address  the numerical, approximate solving. The branching step creates a partition of domains or generates disjunctive constraints from equations in factorized form. Experimental results from a prototype show that interval methods generally benefit from the symbolic processing of the initial constraints.'),
(652,'Rigorous a priori error bounds for floating-point computations are derived. We will show that using interval tools in combination with function and operator overloading such bounds can be computed on a computer automatically in a very convenient way. The bounds are of worst case type. They hold uniformly for the specified domain of input values. That means, whenever the floating point computation is repeated later on with any set of point input values from that domain the difference of the exact result and the computed result is guaranteed to be smaller than the a priori error bound.  Our techniques can be used to get reliable a priori error bounds for already existing program code. Here, loops, recursion, and iterations are allowed. To demonstrate the power of the methods several examples are given.'),
(653,'This paper deals with the Pascal subroutines for solving certain problems in the interval linear programming, especially with calculating the exact range, i.e. the supremum and the infimum of optimal objective function values of a family of LP problems in which all coefficients in constraints vary in given intervals. A theoretical background of the algorithms and a description of the package is included. An application of algorithms regarding a set of feasible coefficients and the solvability set is described in this paper and numerical experiences are also mentioned.      <hr align=\"left\" width=\"50%\"> <br><br> 1.) his research was supported by the Grant Agency of the Czech Republic under the grant No. 201/95/1484.'),
(654,''),
(655,'A linear programming-based method is presented for finding all solutions of nonlinear systems of equations with guaranteed accuracy. In this method, a new effective linear programming-based method is used to delete regions in which solutions do not exist. On the other hand, Krawczyk\'s method is used to find regions in which solutions exist. As an illustrative example, all solutions of the nonlinear system of equations describing equilibrium conditions of the \"high polymer liquid system\", which is a well-known ill-conditioned system of equations, are identified by the method.'),
(656,'This paper is concerned with the reconstruction of an unknown impedance p(x) in the Sturm-Liouville problem with Dirichlet boundary conditions, when only a finite number of eigenvalues are known. The problem is transformed into a system of nonlinear equations. A solution of this system is enclosed in an interval vector by an interval Newton\'s method. From the interval vector, an interval function [p](x) is constructed that encloses an impedance p(x) corresponding to the prescribed eigenvalues. To make this numerical existence proof rigorous, all discretization and roundoff errors have to be taken into account in the computation.'),
(657,'In this paper, a numerical method is presented for proving the existence and inclusion of connecting orbits of continuous dynamical systems described by parameterized nonlinear ordinary differential equations. Taking a certain second order nonlinear ordinary differential equaiton as an example, the existence of homoclinic bifurcation points is proved by the method.'),
(658,'The paper presents a new enclosure method for initial value problems in systems of ordinary differential equations. Like the common enclosure methods (eg Lohner\'s algorithm AWA), it is based on Taylor expansion. In contrast to them, however, it is an implicit method. The solution sets of nonlinear inequalities have to be enclosed by a Newton-like algorithm. As the presented examples show, the new method sometimes yields much tighter bounds than any of the common explicit methods.'),
(659,'The quality of an intelligent tutoring system is measured in terms of the speed and efficiency of learning. Many elements can improve this quality. If we consider the example of classical learning strategies, it is clear that they are not sufficient because the learner needs to be more involved in the learning session. Thus, there is a need for new co-operative learning strategies. For these strategies to be effective we need to be able to measure the weaknesses of the learner, and more specifically the discord in his or her ideas (internal conflict), in order to know which strategy is most suitable, when to use it, and what concepts need to be emphasised.  Using the theory of cognitive-dissonance (discord between ideas), we have determined an indicator that measures the discord between the understanding of two elements of knowledge. To do this we have used the <I>learning-by-disturbing</I> strategy to test the confidence of the learner with regards to these units of knowledge and to make the learner aware of potential internal conflicts. We have developed a method allowing the detection of discordant concepts and the measure of dissonance rate. We also have shown that the learning process is improved when the tutor knows, for each learner, which concepts to focus.'),
(660,'Reviews of information CD-ROM products were examined in relation to those criteria which home users identify as important in the selection of information CD-ROMs. Specifically we were interested in identifying what relationship, if any, existed between the key overall elements which characterise the reviews and the medium (print or electronic) and type of publication in which they appeared. A coding scheme was devised based on work undertaken previously by the authors and with reference to appropriate literature. Content analysis was undertaken using NUD.IST qualitative data analysis software. Our findings indicate that the nature of the review <i>is</i> influenced by the publication medium and type.'),
(661,'Enormous sums of money and human effort have gone into educational technologies over the past decade. Yet nagging questions surface as to whether this tremendous investment produces advantageous results.  While we intuitively feel that the influence of technology should be substantial, little sound guidance exists as to what is effective and why or how to use it. We seem to have cleared several of the hurdles for building a computer-aided instruction infrastructure; now we must turn our attention to richer understandings of research into the impact of technologies in the classroom. This special issue of the <I>Journal of Universal Computer Science </I>focuses on assessment and evaluation practices. The six articles in this collection have been clustered around three major issues: (1) pragmatics - cost estimations and product reviews, (2) measuring the effectiveness of theory-driven design, (3) extending paradigms for capturing more profound understanding of variables and outcomes.'),
(662,'The main purpose of this paper is to test a  conceptual framework for the evaluation of the effectiveness of an open and distance learning (ODL) hypermedia system (EONT-ODL system) and courseware developed and trial used at the National Technical University of Athens (NTUA), Greece, within the EONT project. EONT is a partnership project between seven universities from seven European Union countries within the Socrates Framework Program. In this paper we deal with data elicited from the NTUA, since it was the only partner institution which provided adequate data for quantitative analysis. The evaluation framework is based on the assumption that ODL hypermedia systems are complex systems with a variety of organisational, administrative, instructional, and technological components. It has been hypothesized that the effectiveness of the EONT-ODL system would be influenced by a number of independent variables such as: design and presentation of the courseware, previous experience, time spent on working through the courseware, preference of mode of study, learning styles, interactions with peers, instructors and means of communication. In this evaluation research, two instruments integrated into one questionnaire for data collection were developed: the first was based on a number of standardized questions, reflecting the previously stated theoretical framework and the second on a number of open-ended questions, reflecting, likes and dislikes, added value, problems identified, suggestions etc. The regression analysis indicates that the `design and presentation of instructional material\' alone explained almost 28% of the EONT- ODL system\'s effectiveness (R2 adj.= .278). The preferred `mode of study\' entered second by adding 11% (R2 ch.=.113) of the effectiveness variance and finally students interactions with the instructor increased the effectiveness explained variance to 48%, a quite high percentage accounted for three significant predictors alone. All the other predictors, that is, previous experience with computers, time spent working with the EONT-ODL courseware, student learning styles, and interactions among students and communication means (e-mail and computer conferencing) did not significantly contribute to the prediction of the effectiveness measure. These quantitative results are complemented by the qualitative conclusions.'),
(663,'Researchers at the NASA Classroom of the Future have been using the design experiment framework to conduct evaluations of multimedia curricula. This method stands in contrast to more traditional, controlled experimental methods of evaluating curricular reforms. The methodology presented here is integrated with Walter Doyle\'s [1983] notion of using academic tasks to describe how classroom activities impact student learning. We will report the results from a design experiment with a multimedia program developed at the NASA Classroom of the Future, and we will examine the methodologies that were used in the evaluation.'),
(664,'This paper discusses evaluation of hypermedia-based learning environments mainly from the point of view of the learner or student. The evaluation of a learning environment should be based on modern learning theories. These emphasise the importance of constructivism and the learner\'s activity in building mental models of the mathematical knowledge. The environment should also support conversational and collaborative learning. From the point of view of the learner it should be intentional and provide real life situations and contexts to motivate the study of abstract mathematical contents. Also, it should give sufficient feedback and be able to adapt to the needs of various learners. The purpose of the paper is to discuss basic pedagogical principles that may serve as starting points and guidelines in the evaluation of hypermedia-based learning environments. Two existing hypermedia learning environments will be introduced and evaluated on the basis of the pedagogical principles presented.'),
(665,'Software engineering methods and metrics to estimate development time for the development and maintenance of computer-based training (CBT) differ from methods and metrics used to develop large information systems. The estimation techniques for large information systems employ Lines-Of-Code and Feature/Function points to calculate project effort in staff-months [Boehm 1981], techniques that are difficult to apply to CBT effort estimation. For the development of computer-based training systems, Development to Delivery Time Ratios have been the usual estimation metric, but these also have limitations [Marshall et al. 1995]. Metrics to accurately measure the development effort of Multimedia Information Systems (MIS) are currently being developed and investigated [Fletcher et al. 1997], but still differ from computer-based training systems development. This paper presents an estimation model for effort development of small courseware projects (less than 2 staff-months). By identifying the sub-tasks of the development phase, an individual estimation technique is suggested for each sub-task. The sum of all sub-tasks estimations determines the total effort estimation for the development phase of a particular lesson. Incorporating historical data as a baseline and identifying risk cost factors, this method is accurate for estimating effort of some sub-tasks and for the lesson unit as a whole. This method is not meant to be a \"silver bullet\" [Brooks 1995] but a start toward building an accurate estimation tool and a refinement of the development process.'),
(666,'Adaptive link annotation is a new direction within the field of user-model based interfaces. It is a specific technique in Adaptive Navigation Support (ANS) whose aim is to help users find an appropriate path in a learning and information space by adapting link presentation to the goals, knowledge, and other characteristics of an individual user. More specifically, ANS has been implemented on the WWWin the InterBook system as link annotation indicating several states such as visited, ready to be learned, or not ready to be learned. These states represent an expert\'s suggested path for an individual user through a learning space according to both a history-based (tracking where the user has been), and a pre-requisite based (indexing of content as a set of domain model concepts) annotation. This particular process has been more fully described elsewhere [Brusilovsky, Eklund & Schwarz 1998]. <br><br>This paper details results from an investigation to determine the effectiveness of user-model based link annotation, in a real-world teaching and learning context, on learning outcomes for a group of twenty-five second year education students in their study of databases and spreadsheets. Using sections of a textbook on ClarisWorks databases and spreadsheets, which had been authored into the InterBook program, students received sections of the text both with and without the adaptive link annotation. Through the use of audit trails, questionnaires and test results, we show that while this particular form of ANS implemented in InterBook initially had a negative effect on learning of the group, it appears to have been beneficial to the learning of those particular students who tended to accept the navigation advice, particularly initially when they were unfamiliar with a complex interface. We also show that ANS provided learners with the confidence to adopt less sequential paths through the learning space. Considering ANS tools comprised a minimal part of the interface in the experiment, we show that they functioned reliably well. Discussion and suggestions for further research are provided.'),
(667,'This article describes a methodological approach to conditional reasoning in online asynchronous learning environments such as Virtual-U VGroups, developed by SFU, BC, Canada, consistent with the notion of meaning implication: If part of a meaning C is embedded in B and a part of a meaning B is embedded in A, then A implies C in terms of meaning [Piaget 91]. A new transcript analysis technique was developed to assess the flows of conditional meaning implications and to identify the occurrence of hypotheses and connections among them in two human science graduate mixed-mode online courses offered in the summer/spring session of 1997 by SFU. Flows of conditional meaning implications were confronted with Virtual-U VGroups threads and results of the two courses were compared. Findings suggest that Virtual-U VGroups is a knowledge-building environment although the tree-like Virtual-U VGroups threads should be transformed into neuronal-like threads. Findings also suggest that formulating hypotheses together triggers a collaboratively problem-solving process that scaffolds knowledge-building in asynchronous learning environments: A pedagogical technique and an built-in tool for formulating hypotheses together are proposed.'),
(668,''),
(669,'A shared design memory emerging from the contributions of novice designers affords, theoretically, unique opportunities to support individual and organizational learning. Evaluation must take into account the   \"distributed\" nature of the system that becomes realized. The proposed evaluation model is based on a cross-analysis of: the contents of the shared design memory, the quality of the design artifact produced be the designers teams, the characteristics of the student population, and their perceptions of the adequacy and usefulness of the representational formats adopted for the shared memory. Effects being sought are generational changes that indicate that design weaknesses typical of novices are being offset, and that good design practices are diffused and gradually incorporated as new quality standards. Preliminary results of the evaluation of a shared memory for Information Systems design show that shared memory underpinned an emergent quality in the new designs, characterized by increased structuredness, communicability, and attention to the dynamics of interactions in the system being designed. The shared memory was deemed useful and usable by the learners. Findings also clarify the relative merits of some representational formats (links among design cases and reviews attached to design cases) used for conveying design knowledge.'),
(670,'This paper looks at a variety of on-line help systems and at guidelines for their design, and indentifies general problem-solving strategies which are important for the effectiveness and usability of on-line help. The lack of a suitable evaluation instrument is identified and a questionnaire to address this need is developed: the On-line Help Evaluation Checklist. The new instrument is to assist instructional designers (who develop courses that require computer problem-solving skills of the target audience) to assess the adequacy of a tool\'s on-line help. The instrument is subsequently applied to the evaluation of software tools to be used in a first-year, university-level course on instructional instrumentation.'),
(671,'This paper proposes that the most important role the computer may play in education could be contributing to the ubiquitous use of assessment for the improvement of instruction. In order to realize this potential, newly emerging WWW-based learning systems should support a very wide range of embedded assessment features. These systems require architectures with a core of reliable integrated management tools, one or more modules with instruction and assessment, standard database connectivity, and an acceptable level of attention to permissions and security. No company will adequately address all of the possibilities for assessment in WWW-based learning systems, so it is critical that WWW based learning systems have \"open system\" architectures and company policies for cooperating with other companies to support interoperable modules. The point is raised that some products with similar types of assessment features can have very different architectures and policies for supporting interoperable modules. It is recommended that \"checklists\" for comparing assessment capabilities should be viewed with skepticism, because they can favor products with weaker architectures and policies for accommodating assessment capabilities.'),
(672,'This paper reports the results of applying metrics  to hypermedia authoring under the SHAPE research project. The aim of SHAPE is to help authors develop high quality large hypermedia applications for education. The quality characteristics considered are the reusability of information, the maintainability of applications and the development effort. Although a number of metrics for hypertext systems have been proposed, we believe that many of the measures proposed in the past lack the necessary mathematical and/or empirical justification. The metrics proposed in this paper have been developed using the Goal-Question-Metric approach, and adhere to the representational theory of measurement. We describe the development of the metrics and the results of a quantitative empirical study which compares two different hypermedia authoring systems.'),
(673,'Spatial orientation is an important ability, which should be facilitated in geometry courses of elementary schools. A preferred approach (in Germany) typically involves navigation and orientation tasks with pictures of a town and city maps depicted in a book. Because of increasing use of computer systems in schools, it is very interesting to explore the value a virtual environment possesses for teaching spatial orientation. This articel describes the \'City Game\', a computer-generated, virtual 3D city, developed for teaching spatial orientation in elementary schools, and an informal study to see first reactions of children and adults when using the \'City Game\'.'),
(674,'Research has shown the potential of a problem-based approach to enhance students\'  learning. The interactive nature of hypermedia technology and its ability to deliver information in different media formats can provide unique capabilities for implementing problem-based learning (PBL) environments. Yet, we know little about the types of tools that are effective in supporting students\' learning in a hypermedia supported PBL environment. The purpose of this study was to investigate both the use of tools and design features in a piece of PBL software and their effectiveness on middle school students\' learning. The findings of this study show that students who were exposed to the PBL environment increased their achievement scores from pre to posttest more than those students who learned the same content in the traditional classroom. Students\' reading ability was found to be a better predictor for their achievement in PBL than their math ability. However, the brief treatment of the study had only limited impact on students\' attitude toward learning science. The findings of the study and their implications are discussed in detail.'),
(675,'This paper presents some numerical simulations of  rounding errors produced during evaluation of Chebyshev series. The simulations are based on perturbation theory and use recent software called AQUARELS. They give more precise results than the theoretical bounds (the difference is of some orders of magnitude). The paper concludes by confirming theoretical results on the increment of the error at the end of the interval [-1; 1] and the increased performance achieved by some modifications to Clenshaw\'s algorithm near those points.'),
(676,'Hemaspaandra, Hempel, and Wechsung [HHW] initiated the field of query order, which studies the ways in which computational power is affected by the order in which information sources are accessed. The present paper studies, for the first time, query order as it applies to the levels of the polynomial hierarchy. <IMG src=\"query_order_and_the/images/img1.gif\"> denotes the class of languages computable by a polynomial-time machine that is allowed one query to <IMG src=\"query_order_and_the/images/img2.gif\"> followed by one query to <IMG src=\"query_order_and_the/images/img3.gif\"> [HHW]. We prove that the levels of the polynomial hierarchy are order-oblivious: <IMG src=\"query_order_and_the/images/img4.gif\"> Yet, we also show that these ordered query classes form new levels in the polynomial hierarchy unless the polynomial hierarchy collapses. We prove that all leaf language classes - and thus essentially all standard complexity classes - inherit all order-obliviousness results that hold for P.'),
(677,'The efficiency of global optimization methods in connection with interval arithmetic is no more to be demonstrated. They allow to determine the <I>global optimum</I> and the corresponding <I>optimizers</I>, with certainty and arbitrary accuracy. One of the main features of these algorithms is to deliver a function enclosure defined on a box (right parallelepiped). The studied method provides a <I>lower bound</I> (or <I>upper bound</I>) of a function in that box throughout two different strategies. As we shall see, these algorithms associated with various Branch and Bound methods lead to accelerated convergence and permit to avoid the cluster problem.'),
(678,'Abstract: In this paper, we discuss the design and development of a particular type of electronic publication that has gained recent popularity: electronic conference proceedings. We suggest that modern electronic proceedings should provide a high degree of interactivity. To support such interactivity, proceedings should include an extensive collection of features and diverse multimedia components. Features appropriate for electronic proceedings include annotation, presentation, and retrieval mechanisms. Conference papers and multimedia reproductions of conference presentations with features that allow readers to manipulate these reproductions particularly enhance the interactivity of electronic proceedings. Experience from interactive proceedings the authors have designed is also discussed. Special attention is given to the multiple roles video elements can and should play in interactive proceedings.'),
(679,''),
(680,'The paper proposes a neural agent that performs self-organizing classification to assist in searching and contributing to webs of documents and in the process of document reuse. By applying the Kohonen self-organizing feature map (SOFM) algorithm to patterns of influence links among documents it is possible to originate clusters of documents that help infer the aspects that such documents implicitly share. The approach complements search techniques based on semantic indexes. The resulting classification is sensitive to the multiple aspects of a document that may belong to multiple classes with a varying degree and allows for treating effectively items that typically have a limited life span, either because they are means to the collaborative production of a more complex item, or because they belong to fast evolving domains. The method has been implemented by Lotus Notes Domino Web server for a case-based application in the domain of information systems design.'),
(681,'The presented paper demonstrates a method to embed a unique signature into a coating material used in a smart card or in the covering material of some other secure hardware device. The method bases on the impossibility of exactly reproducing a specific piece of plastic or other material used to cover the secure hardware. By using a very inhomogeneous materials or mixtures of conductors and insulators such a cover is made unique by the method of production. This inhomogeneous piece and the non-reproducible and random properties are incorporated into an electronic signature which is checked whenever needed. Assuming that the surface is covered totally with an \"active coating material\" it is impossible to partially penetrate or destroy the coating without destroying the signature. Unpenetrable hardware is an inevitable element in nearly all secure designs and with the promotion of digital signatures such unpenetrable hardware becomes even more important. The result gained with the presented work is the possibility to make a hardware unique depending on randomness, and to assure that penetration is not only detected but also features logical destruction of the secure hardware [PAT96]. Implementing such penetration sensors with memory enhances the security to a large extent, and since the destruction upon perceived penetration is logical there is no possible false alarm.'),
(682,''),
(683,'Hemaspaandra and Torenvliet showed that each P-selective set can be accepted by a polynomial-time nondeterministic machine using linear advice and <I>quasi-linear</I> nondeterminism. We show that each P-selective set can be accepted by a polynomial-time nondeterministic machine using <I>linear</I> advice and linear nondeterminism.'),
(684,'<p>We present a novel approach to parallel computing, where (virtual) PRAM processors are represented as light-weight threads, and each physical processor is capable of managing several threads. Instead of moving read and write requests, and replies between processor&memory pairs (and caches), we move the light-weight threads. Consequently, the processor load balancing problem reduces to the problem of producing evenly distributed memory references. In PRAM computations, this can be achieved by properly hashing the shared memory into the processor&memory pairs.</P>  <P>We describe the idea of moving threads, and show that the moving threads framework provides a natural validation for Brent\'s theorem in work-optimal PRAM simulation situations on mesh of trees, coated mesh, and OCPC based distributed memory machines (DMMs). We prove that an EREW PRAM computation <IMG src=\"balanced_pram_simulations_via/images/img1.gif\"> requiring work <I>W</I> and time <I>T</I>, can be implemented work-optimally on those <I>p</I>-processor DMMs with high probability, if <I>W</I> = <IMG src=\"balanced_pram_simulations_via/images/img2.gif\">, where <I>D</I> is the diameter of the underlying routing machinery. The computation is work-optimal regardless how (virtual) PRAM processors terminate or initiate new PRAM processors during the computation.</P>  <P>Our result is based on using only one randomly chosen hash function and on showing, how the threads (PRAM processors) can spawn new threads in required time on <I>p</I>-processor OCPC, 2-dimensional mesh of trees, 2-dimensional coated, and 3-dimensional coated mesh. A deterministic spawning algorithm is provided for all cases, although a randomized algorithm would be sufficient due to the randomized nature of time-processor optimal PRAM simulations.</P>'),
(685,'Nonperfect secret sharing schemes (NSSs) have an advantage such that the size of shares can be shorter than that of perfect secret sharing schemes. This paper shows some basic properties of general NSS. First, we present a necessary and sufficient condition on the existence of an NSS. Next, we show two bounds of the size of shares, a combinatorial type bound and an entropy type bound. Further, we define a compact NSS as an NSS which meets the equalities of both our bounds. Then we show that a compact NSS has some special access hierarchy and it is closely related to a matroid. Verifiable nonperfect secret sharing schemes are also presented.'),
(686,'Boolean functions used in cryptographic applications have to satisfy various cryptographic criteria. Although the choice of the criteria depends on the cryptosystem in which they are used, there are some properties (balancedness, nonlinearity, high algebraic degree, correlation immunity, propagation criteria) which a cryptographically strong Boolean function ought to have. We study the above mentioned properties in the set of all Boolean functions (all balanced Boolean functions) and prove that almost every Boolean function (almost every balanced Boolean function) satisfies all above mentioned criteria on levels very close to optimal and therefore can be considered to be cryptographically strong.'),
(687,''),
(688,'Assistance in retrieving of documents on the World Wide Web is provided either by search engines, through keyword based queries, or by catalogues, which organise documents into hierarchical collections. Maintaining catalogues manually is becoming increasingly difficult due to the sheer amount of material on the Web, and therefore it will be soon necessary to resort to techniques for automatic classification of documents. Classification is traditionally performed by extracting information for indexing a document from the document itself. The paper describes the technique of categorisation by context, which exploits the context perceivable from the structure of HTML documents to extract useful information for classifying the documents they refer to. We present the results of experiments with a preliminary implementation of the technique.'),
(689,'Perusal of textual displays of document surrogates  produced by Web-based ranked-output retrieval services may require much user time, effort, and money. In this paper we present VIEWER, a graphical interface that allows visualization and manipulation of views of retrieval results, where a view is the subset of retrieved surrogates that contain a specified subset of query terms. We argue that VIEWER helps the user focus on relevant parts of the results and, in addition, it may facilitate query reformulation. We present the results of an experiment performed by six subjects on two medium size bibliographical test collections in which VIEWER, used as an interactive ranking systems, outperformed both best match ranking and coordination'),
(690,'The World Wide Web is in constant renovation, with  new technologies emerging every day. Most of these technologies are still incipient, and there are few de facto standards for this \"new Web\". There is a need for tools that can run with current standard support, but which are flexible and extensible enough to be eventually ported to new APIs and to incorporate new technologies. On the other hand, many Web developers cannot keep pace with the fast track of Web technologies. Therefore, it is important for new tools to be simple enough to be mastered quickly by the average programmer. This paper presents CGILua, a Web development tool that matches these requirements. The paper also discusses why this tool is being adopted in many commercial and academic projects, focusing on issues such as flexibility, extensibility, simplicity, and portability.'),
(691,'Abstract: It is critical to understand WWW latency in order to design better HTTP protocols. In this study we characterize Web response time and examine the effects of proxy caching, network bandwidth, traffic load, persistent connections for a page, and periodicity. Based on studies with four workloads, we show that at least a quarter of the total elapsed time is spent on establishing TCP connections with HTTP/1.0. The distributions of connection time and elapsed time can be modeled using Pearson, Weibul, or Log-logistic distributions. Response times display strong daily and weekly patterns. We also characterize the effect of a user\'s network bandwidth on response time. Average connection time from a client via a 33.6 K modem is two times longer than that from a client via switched Ethernet. We estimate the elapsed time savings from using persistent connections for a page to vary from about a quarter to a half. This study finds that a proxy caching server is sensitive to traffic loads. Contrary to the typical thought about Web proxy caching, this study also finds that a single stand-alone squid proxy cache does not always reduce response time for our workloads. Implications of these results to future versions of the HTTP protocol and to Web application design are discussed.'),
(692,''),
(693,'Within an Electronic Education Market an Electronic Education Mall is defined as a virtual service center to support various transaction processes by providing a technological platform with appropriate value-added services and interfaces for suppliers and customers. In this context, an Education Broker service is of central importance because the quality of the learning process is strongly determined by the quality of the available materials and their configuration to an integrated course according to a pedagogical concept and the respective customers\' needs. To support these tasks an Education Broker toolset is introduced which allows to select the  \'right\'  elements out of a set of generally suitable learning modules, to adjust and structure the chosen learning modules to an integrated course in a pedagogically and didactically useful way, to add navigational guides, to provide added values and to deliver the integrated course to allow an intuitive application by the student.'),
(694,'Style sheets, which are used to specify the appearance of documents, are rapidly growing in their importance for the World Wide Web. Cascading Style Sheets are now in widespread use and work on a future Web standard, the Extensible Style Language (XSL), is proceeding at a rapid pace. In this paper, we show how a different style sheet language, PSL, represents an attractive midpoint between CSS and XSL in complexity and power. PSL is based on general language design principles that give it simple syntax, easily-described semantics, and considerable expressive power. PSL is supported by Proteus, a portable style sheet system that allows the construction of novel user interfaces for Web browsers and other presentation tools through support for multiple simultaneous presentations of the same document.'),
(695,'The mStar environment for distributed education utilizes the WWW and IP-multicast to enable teacher-student collaboration over large geographic distances. Several educational projects, spanning from secondary school courses to company internal training, have deployed the mStar environment.  This paper reports on experiences gained over a year of practice at the Lule University of Technology and the Centre for Distance-spanning Technology. The paper presents the methodology and technology used, while recognizing usage scenarios such as preparation of presentation material, distributed presentations, asynchronous playback of recorded and edited material, and virtual meetings for educational support.'),
(696,''),
(697,'This paper presents aspects of modelling, authoring and presenting structured documents corresponding to teaching material presented in the <I>World Wide Web</I>. In this context, it is discussed the importance of providing the formalization of the structure of the documents using SGML. Specifications for structured documents corresponding to didactic texts and questionnaires in SGML are described. Software tools to the authoring and presentation of those documents are also presented in order to manipulate and delivery them in the Web, showing an effective support for structured web teaching material.'),
(698,'<p>Users tend to lose their way in the maze of information within hypertext. Much work done to address the \"lost in hyperspace\" problem is <I>reactive</I>, that is, doing remedial work to correct the deficiencies within hypertexts because they are (or were) poorly designed and built. What if solutions are sought to <I>avoid</I> the problem? What if we do things well from the start? </P>  <P>This paper reviews the \"lost in hyperspace\" problem, and suggests a framework to understand the design and usability issues. The issues cannot be seen as purely psychological or purely computing, they are multi-disciplinary.  Our <I>proactive</I>, <I>multi-disciplinary</I> approach is drawn from current technologies in sub-disciplines of hypertext, human-computer interaction, cognitive psychology and software engineering. To demonstrate these ideas, this paper presents HyperAT, a hypertext research authoring tool, developed to help designers build usable web documents on the World Wide Web without getting \"lost.\" </P>'),
(699,'This paper describes in brief a basic framework for implementing a telematic-platform for patient oriented services. We first show the current situation in the medical field and work out the requirements for an integrating platform. Afterwards we introduce Active Node Technology (ANT), the technology the platform will be based on.'),
(700,''),
(701,'In this paper we propose a modification of a part of the global adaptive integration algorithm that is usually taken for granted: the subdivision strategy. We introduce a subdivision strategy where the routine decides whether it is best to divide a hyper-rectangular region or a <I>n</I>-simplex in 2 or 2<SUP>n</SUP> parts or something in between.'),
(702,'In this paper, we present a data model for  hypermedia structuring and navigation. The three fundamental building blocks are values, objects and sets, for modeling atomic and aggregate content, as well as navigational structure. In order to formally define operators that enable an end user to access elements of a set, zoom in on or out of a set, we introduce the concepts of user state, topology of a set and anchors.'),
(703,'At first sight, Java\'s position as the de-facto standard for portable software distributed across the Internet seems virtually unassailable. Interestingly enough, however, it is surprisingly simple to provide alternatives to the Java platform, using the plug-in mechanism supported by the major commercial World Wide Web browsers. <br><br>      We are currently developing a comprehensive infrastructure for mobile software components. This is a long-term research activity whose primary objectives are not directly related to today\'s World Wide Web, but which targets future high-performance component-software systems. However, purely as a technology demonstration, we have recently started a small spin-off project called \"Juice\" with the intent of extending our experimental mobile-code platform into the realm of the commercial Internet. <br><br>      Juice is implemented in the form of a browser plug-in that generates native code on-the-fly. Although our software distribution format and run-time architecture are fundamentally different from Java s, and arguably more advanced, once that the appropriate Juice plug-in has been installed on a Windows PC or a Macintosh computer, end-users can no longer distinguish between applets that are based on Java and those that are based on Juice. The two kinds of applets can even coexist on the same Web-page. <br><br>      This, however, means that Java can in principle be complemented by alternative technologies (or even gradually be displaced by something better) with far fewer complications than most people seem to assume. As dynamic code generation technology matures further, it will become less important which code-distribution format has the largest \"market share\", many such formats can be supported concurrently. Future executable-content developers may well be able to choose from a wide range of platforms, probably including several dialects of Java itself. Hence, a pattern of \"open standards\" for mobile code is likely to eventually emerge, in spite of Java\'s current dominance.'),
(704,'The increasing availability of 3D input and output devices demands a better understan ding and comparison of their quality. This paper describes an empirical experiment that provides quantitative results of the viewing quality of stereoscopic and perspective display modes. In this study such results accuracy and time of eightyone users were measured performing realistic 3D tasks to research the structures of organic molecules. A subsequent comparison of cost vs. performance can represent a meaningful help for decision-making of users and designers. The experiment was designed and conducted in cooperation of computer and chemical scientists. The experience of both sides was necessary to get a controlled testing environment with appropriate tasks. The considered display modes were <I>perspective mode, anaglyph mode</I> and <I>shutter glass stereo mode</I>. Additionally the effect of the level of expertise of a subject on accuracy and response time were explored. Mean response errors and mean response times were computed separately by a two-way analyses (ANOVA) for a series of six tasks and eighty-one subjects.'),
(705,'We introduce an inference system for deriving all keys of a relation schema. Then we show that the number of keys of a relation schema R = <IMG src=\"on_the_number_of/abstract_images/img1.gif\"> is bounded by <IMG src=\"on_the_number_of/abstract_images/img2.gif\"> .'),
(706,''),
(707,'The object-oriented design community has recently  begun to collect so-called software design patterns: descriptions of proven solutions common software design problems, packaged in a description that includes a problem, a context, a solution, and its properties. Design pattern information can improve the maintainability of software, but is often absent in program documentation.  <br><br>      We present a system called Pat for localizing instances of structural design patterns in existing C ++ software. It relies extensively on a commercial CASE tool and a PROLOG interpreter, resulting in a simple and robust architecture that cannot solve the problem completely, but is industrial-strength, it avoids much of the brittleness that many reverse engineering tools exhibit when applied to realistic software. The contribution of our work is not so much in the engineering value represented by this concrete system, but in its methodological approach.  <br><br>      To evaluate Pat, we quantify its performance in terms of precision and recall. We examine four applications, including the popular class libraries zApp and LEDA. Within Pat s restrictions all pattern instances are found, the precision is about 40 percent, and manual filtering of the false positives is relatively easy. Therefore, we consider Pat a good compromise: modest functionality, but high practical stability for recovering design information.'),
(708,'In [Wastl 1998] we have introduced the Hilbert style inference system K for deriving all keys of a database relation schema. In this paper we investigate formal K-derivations more closely using the concept of tableaux. The analysis gives insight into the process of deriving keys of a relation schema. Also, the concept of tableaux gives a proof procedure for computing all keys of a relation schema. In practice, the methods developed here will be usefull for computing keys or for deciding whether an attribute is a key attribute, respectively non-key attribute. This decision problem becomes relevant when checking whether a relation schema is in third normal form, or when applying the well-known 3NF-decomposition algorithm (a.k.a. 3NF-synthesis algorithm).'),
(709,''),
(710,'In his work on the information content of English text in 1951, Shannon described a method of recoding the input text, a technique which has apparently lain dormant for the ensuing 45 years. Whereas traditional compressors exploit symbol frequencies and symbol contexts, Shannon\'s method adds the concept of \"symbol ranking\", as in `the next symbol is the one third most likely in the present context\'. While some other recent compressors can be explained in terms of symbol ranking, few make explicit reference to the concept. This report describes an implementation of Shannon\'s method and shows that it forms the basis of a good text compressor.'),
(711,'This paper describes how to design and use a framework of hardware and software for flexible interfacing and prototyping on the PC. The hardware comprises a card with programmable hardware provided by FPGAs, with an interface including DMA block transfer and interrupts. A library of hardware macros is described. Software routines are provided to enable the FPGAs to be programmed and to allow communication between the host PC and the peripheral card. Examples are given to show its use in building and testing designs, so that new applications can be prototyped quickly using a proven and reliable interface.'),
(712,'Objects in object-oriented languages have often been treated as a special kind of entity different from other variables or constants. Similarly, their types, which are typically called classes, have often been treated differently from other types. This complicates the understanding of these concepts. The present paper proposes to see the classes as module types leading to a very natural integration of objects and classes into the framework of contemporary programming languages. The main part of the paper contains typing rules for module types for assignment and for value and variable parameters. It is shown that the rules for reference parameters in some existing languages lead to unexpected results and sometimes to undefined behavior. Furthermore, assignment involving dereferenced pointers to modules is studied for the first time in detail. The paper shows that the type compatibility rule for pointer assignment is not sufficient for deref assignment. The last part of the paper contains a comparison of the language definitions and of compilers for Borland Pascal with Objects, C++, Oberon, and Object CHILL.'),
(713,''),
(714,'We prove the Linear Time Hierarchy Theorems for random access machines and Gurevich abstract state machines. One long-term goal of this line or research is to prove lower bounds for natural linear time problems.'),
(715,''),
(716,'We demonstrate that Schoenhage storage modification machines are equivalent, in a strong sense, to unary abstract state machines. We also show that if one extends the Schoenhage model with a pairing function and removes the unary restriction, then equivalence between the two machine models survives.'),
(717,'Failure resilience is an essential requirement for database systems, yet there has been little effort to specify and verify techniques for failure recovery formally. The desire to improve performance has resulted in algorithms of considerable sophistication, yet understood by few and prone to errors. In this paper, we illustrate how the methodology of Gurevich Abstract State Machines can elucidate recovery and provide formal rigor to the design of a recovery algorithm. In a series of refinements, we model a recovery algorithm at several levels of abstraction, verifying the correctness of each model. This work suggests that our approach can be applied to more advanced recovery mechanisms.'),
(718,'According to the ASM thesis, any algorithm is essentially a Gurevich abstract state machine. The only objection to this thesis, at least in its sequential version, has been that ASMs do not capture recursion properly. To this end, we suggest recursive ASMs.     <hr align=\"left\" width=\"50%\">     1.) Partially supported by NSF grant CCR 95-04375 and ONR grant N00014-94-1-1182.<br>     2.) Visiting scholar at the University of Michigan, partially supported by DAAD and The University of Michigan.'),
(719,'We present a systematic reconstruction of a compilation method for an extension to logic programming that permits procedure definitions to be given a scope. At a logical level, this possibility is realized by permitting implications to be embedded in goals. Program clauses that appear in the antecedents of such implications may contain variables that are bound by external quantifiers, leading to non-local variables in procedure declarations. In compiling programs in this extended language, there is, therefore, a need to consider the addition to given programs of program clauses that are parameterized by bindings for some of their variables. A proposed approach to dealing with this aspect uses a closure representation for clauses. This representation separates an instance of a clause with parameterized bindings into a skeleton part that is fixed at compile-time and an environment that maintains the part that is dynamically determined. A development of this implementation scheme is provided here by starting with an abstract interpreter for the language and then refining this to arrive at an interpreter that uses the closure representation for clauses. The abstract state machine formalism of Gurevich is used in specifying the interpreters that are of interest at the two different stages. We also justify this refinement by showing that the essential notion of a computation is preserved by the refinement and thus the refinement is a correct one.'),
(720,'This paper describes the first half of the formal verification of a Prolog compiler with the KIV (\"Karlsruhe Interactive Verifier\") system. Our work is based on [BR95], where an operational Prolog semantics is defined using the formalism of Gurevich Abstract State Machines, and then refined in several steps to the Warren Abstract Machine (WAM). We define a general translation of sequential Abstract State Machines to Dynamic Logic, which formalizes correctness of such refinement steps as a deduction problem. A proof technique for verification is presented, which corresponds to the informal use of proof maps. 6 of the 12 given refinement steps were verified. We found that the proof sketches given in [BR95] hide a lot of implicit assumptions. We report on our experiences in uncovering these assumptions incrementally during formal verification, and the support KIV offers for such `evolutionary\' correctness proofs.'),
(721,'This paper addresses the correctness problem of an algorithm solving the constrained shortest path problem. We define an abstract, nondeterministic form of the algorithm and prove its correctness from a few simple axioms. We then define a sequence of natural refinements which can be proved to be correct and lead from the abstract algorithm to an efficient implementation due to Ulrich Lauther [Lauther 1996] and based on [Desrosiers et al. 1995]. Along the way, we also show that the abstract algorithm can be regarded as a natural extension of Moore s algorithm [Moore 1957] for solving the shortest path problem.'),
(722,'This work provides both a specification and a proof of correctness for the system PDP (Prolog Distributed Processor) which make use of Abstract State Machines (ASMs). PDP is a recomputation-based model for parallel execution of Prolog on distributed memory. The system exploits OR_parallelism, Independent AND_parallelism as well as the combination of both. The verification process starts from the SLD trees, which define the execution of a Prolog program, and going through the parallel model, it arrives to the abstract machine designed for PDP, an extension of the WAM (Warren Abstract Machine), the most common sequential implementation of Prolog. The first step of this process consists in defining parallel SLD subtrees, which are a kind of partition of the SLD tree for programs whose clauses are annotated with parallelism. In a subsequent step the parallel execution approach of PDP is modeled by means of an OR_TASK ASM. In this ASM each task is associated with the execution of a parallel SLD subtree. The execution of the parallel SLD subtree corresponding to each task is modeled by a NODE submachine which is an extension of the one proposed by Brger and Rosenzweig to verify the sequential execution of Prolog. Accordingly, the verification leans on the results of this work in order to avoid the verification of the common points with the sequential execution. The new elements of the execution due to parallelism exploitation are modeled at successive steps of the verification process, finally leading to the extended WAM which implements PDP. The PDP verification proves correctness for this particular system but it can readily be adapted to prove it in other related parallel systems exploiting AND, OR or both kinds of parallelism.'),
(723,'In this paper we show how to integrate the use of Gurevich s Abstract State Machines (ASMs) into a complete software development life cycle. We present a structured software engineering method which allows the software engineer to control efficiently the modular development and the maintenance of well documented, formally inspectable and smoothly modifiable code out of rigorous ASM models for requirement specifications. We show that the code properties of interest (like correctness, safety, liveness and performance conditions) can be proved at high levels of abstraction by traditional and reusable mathematical arguments which-where needed-can be computer verified. We also show that the proposed method is appropriate for dealing in a rigorous but transparent manner with hardware-software co-design aspects of system development. The approach is illustrated by developing a C ++ program for the production cell control problem posed in [Lewerentz, Lindner 95]. The program has been validated by extensive experimentation with the FZI production cell simulator in Karlsruhe and has been submitted for inspection to the Dagstuhl seminar on \"Practical Methods for Code Documentation and Inspection\" (May 1997).'),
(724,''),
(725,'Montages are a new way of describing all aspects of programming languages formally. Such specifications are intelligible for a broad range of people involved in programming language design and use. In order to enhance readability we combine visual and textual elements to yield specifications similar in structure, length, and complexity to those in common language manuals, but with a formal semantics. The formal semantics is based on Gurevich\'s Abstract State Machines (formerly called Evolving Algebras).'),
(726,'This paper presents the formal specification of the programming language Oberon. Using Montages we give a description of syntax, static, and dynamic semantics of all constructs of the language. The specification is arranged in five refinement steps, each of them results in a working sub-language of Oberon. The compactness and readability of the specification make us believe that it can be used for a reference manual.'),
(727,'We present here the transformation to C++ code of the refined ASM mode l for the production cell developed in the paper \"Integrating ASMs into the Softw are Development Life Cycle\" (see this volume) which serves as program documentation. This implementation is a refinement step and produces code which has been valida ted through extensive experimentation with the production cell simulator of FZI Karlsruhe.'),
(728,'In this paper, we discuss the use of a model checker in combination with the specification method of Abstract State Machines (ASMs). A schema is introduced for transforming ASM models into the language of a model checker. We prove that the transformation preserves the semantics of ASMs and provide a theoretical framework for a transformation tool. Experience with model-checking the ASM model of the Production Cell demonstrates that this approach offers effective support for verifying ASM specifications.'),
(729,'Existing works on the construction of correct compilers have at least one of the following drawbacks: (i) correct compilers do not compile into machine code of existing processors. Instead they compile into programs of an abstract machine which ignores limitations and properties of real-life processors. (ii) the code generated by correct compilers is orders of magnitudes slower than the code generated by unverified compilers. (iii) the considered source language is much less complex than real-life programming languages. This paper focuses on the construction of correct compiler backends which generate machine-code for real-life processors from realistic intermediate languages. Our main results are the following: (i) We present a proof approach based on abstract state machines for bottom-up rewriting system specifications (BURS) for back-end generators. A significant part of this proof can be parametrized with the intermediate and machine language. (ii) The performance of the code constructed by our approach is in the same order of magnitude as the code generated by non-optimizing unverified C-compilers.'),
(730,'With the development and diffusion of the Internet worldwide connection, a large amount of information is available to the users. Methods of information filtering and fetching are then required. This paper presents two approaches. The first concerns the information filtering system ProFile based on an adaptation of the generalized probabilistic model of information retrieval. ProFile filters the netnews and uses a scale of 11 predefined values of relevance. ProFile allows the user to update on--line the profile and to check the discrepancy between the assessment and the prediction of relevance of the system. The second concerns ABIS, an intelligent agent for supporting users in filtering data from distributed and heterogeneous archives and repositories. ABIS minimizes user s effort in selecting the huge amount of available documents. The filtering engine memorizes both user preferences and past situations. ABIS compares documents with the past situations and finds the similarity scores on the basis of a memory-based reasoning approach.'),
(731,'Knowledge has been lately recognized as one of the most important assets of organizations. Can information technology help the growth and the sustainment of organizational knowledge? The answer is yes, if care is taken to remember that IT here is just a part of the story (corporate culture and work practices being equally relevant) and that the information technologies best suited for this purpose should be expressly designed with knowledge management in view. This special issue of the Journal of Universal Computer Science contains a selection of papers from the First Conference on Practical Applications of Knowledge Management. Each paper describes a specific type of information technology suitable for the support of different aspects of knowledge management.'),
(732,'This paper describes an approach to capturing organisational memory, which serves to ground an analysis of human issues that knowledge management (KM) technologies raise. In the approach presented, teams construct graphical webs of the arguments and documents relating to key issues they are facing. This supports collaborative processes which are central to knowledge work, and provides a group memory of this intellectual investment. This approach emphasises the centrality of negotiation in making interdisciplinary decisions in a changing environment. Discussion in the paper focuses on key human dimensions to KM technologies, including the cognitive and group dynamics set up by an approach, the general problem of preserving contextual cues, and the political dimensions to formalising knowledge processes and products. These analyses strongly motivate the adoption of participatory design processes for KM systems.'),
(733,'A core concept in discussions about technological support for knowledge management is the Corporate Memory. A Corporate or Organizational Memory can be characterized as a comprehensive computer system which captures a company\'s accumulated know-how and other knowledge assets and makes them available to enhance the efficiency and effectiveness of knowledge-intensive work processes. The successful development of such a system requires a careful analysis of established work practices and available information-technology (IT) infrastructure. This is essential for providing a cost-effective solution which will be accepted by the users and can be evolved in the future. The current paper compares and summarizes our experiences from three case studies on Corporate Memories for supporting various aspects in the product life-cycles of three European corporations. Based on the conducted analyses and prototypical implementations, we sketch a general framework for the development methodology, architecture, and technical realization of a Corporate Memory.'),
(734,'This paper describes the realisation of and experiences with two complementary tools for the support of cooperative processes: electronic circulation folders and shared workspaces. Circulation folders support structured work processes, shared workspaces provide a working environment for less structured processes. Both approaches are complementary and their combined usage provides new and very flexible ways of telecooperation and cooperative knowledge management. The components are integrated in the POLITeam system which is developed for the support of cooperative processes between the separated ministries in Bonn and Berlin.'),
(735,'A great part of the product knowledge in manufacturing enterprises is only available in the form of natural language documents. The know-how recorded in these documents is an essential resource for successful competition in the market. From the viewpoint of knowledge management, however, documents have a severe limitation: They do not capture the wealth of knowledge contained in these documents, since the entire knowledge is not spelled out on the linguistic surface. In order to overcome this limitation, the notion of a document as a particular kind of realization of (or view on) the underlying knowledge is introduced. The paper discusses the major steps in realizing this approach to documents: Knowledge acquisition, knowledge representation, and techniques to automatically generate multilingual documents from knowledge bases. Further, the paper describes how the required product knowledge can be represented in a sharable and reusable way.'),
(736,'Knowledge of the cooperative work processes characterizing an organization is a fundamental patrimony not only for people involved in their automation but also for the whole organization in performing its everyday activities. The paper focuses on workflow technology as a set of tools both supporting coordination and enhancing management of the knowledge of work and learning processes within a group of people coordinating their own activities. The paper presents a framework for the construction of coordination mechanisms whose design principles and tools make them a technology enabling the sharing of knowledge about processes and the incremental learning of people within the organization. These claims are illustrated through a working example.'),
(737,'INFOrmer is an intelligent filtering system, currently being applied to the management of USENET News articles. An individual may have one or more profiles, each representing a long-term interest of that user. The user profile is then used to measure the relevance of incoming articles and filter out irrelevant documents. A user profile may be modified as a result of relevance feedback, so that it adjusts to users  changing interests. This paper discusses the architecture of INFOrmer and covers the profile/document representation and comparison techniques adopted within the system.'),
(738,'The class of 2-automatic paperfolding sequences corresponds to the class of ultimately periodic sequences of unfolding instructions. We first show that a paper-folding sequence is automatic iff it is 2-automatic. Then we provide families of minimal finite-state automata, minimal uniform tag sequences and minimal substitutions describing automatic paperfolding sequences, as well as a family of algebraic equations satisfied by automatic paperfolding sequences understood as formal power series.'),
(739,'We describe a new approach to lossy compression of silhouette-like images. By a silhouette-like image we mean a bi-level image consisting of black and white regions divided by a small number of closed curves. We use a boundary detection algorithm to express the closed curves by chain codes, and we express the chains as one function of one variable. We compress this function using WFA over two letter alphabet. Finally, we use arithmetic coding to store the automaton.      <hr align=\"left\" width=\"50%\">      1.) This work was supported by the National Science Foundation under Grant No. CCR-9417384. Preliminary version was presented in DCC 1997.'),
(740,'In a recent paper we introduced Parikh slender languages and series as a generalization of slender languages defined and studied by Andrasiu, Dassow, Paun and Salomaa. Results concerning Parikh slender series can be applied in ambiguity proofs of context-free languages. In this paper an algorithm is presented for deciding whether or not a given N-algebraic series is Parikh slender.'),
(741,'In this paper we discuss two novel ideas to improve teaching and information transfer between persons in general. The first aspect is centered around proposing to use the \"Tamagotchi craze\" for teaching purposes, the second deals with new ways of unobtrusively collecting data on the subjective satisfaction of persons with information and teaching material offered on the Web.'),
(742,'When adding n-bit 2-th complement numbers, the result can be outside the range representable with n bits. A well-known theorem justifies the common overflow logic: Let a,b <img src=\"a_note_on_correctness/abstract_images/img1.gif\"> {0,1}n be the 2-th complement representations of signed integers [a] and [b], respectively, and let c0 <img src=\"a_note_on_correctness/abstract_images/img1.gif\"> {0, 1} be the carry-in bit. Then, [a] + [b] + c0 <img src=\"a_note_on_correctness/abstract_images/img1.gif\"> {-2n-1,...,2n-1-1} if and only if cn = cn-1 , where ci denotes the carry-bit from position i - 1 to position i when adding the binary numbers a and b. We present a proof of this theorem which is much shorter than previous proofs. This simplification can save valuable time in computer science classes. With a small extension the proof even holds for d-th complement numbers. Although the proof technique is known by some specialists, nobody seems to have written it up. With this note, it is once documented in a precise form, thus avoiding re-invention.'),
(743,''),
(744,'It is demonstrated how a program making use of a single stack may be transformed, via memoization, into an equivalent one running in time proportional to the sum of variabilities at certain program points of the original program. This result generalizes Cook\'s linear time simulation of a deterministic two-way push-down automaton and also provides a lucid explanation of Cook\'s construction.     <br><br>     Obtaining an efficient transformed program depends on making good use of the stack to reduce variabilities at the critical program points. It is suggested to obtain such a program directly from a source program expressed in a non-deterministic language with invertible operations and annotated with a kind of \"cuts\" somewhat similar to cuts in a Prolog program.'),
(745,'We study here the degree-theoretic structure of set-theoretical splittings of recursively enumerable (r.e.) sets into differences of r.e. sets. As a corollary we deduce that the odering of wtt-degrees of unsolvability of differences of r.e. sets is not a distributive semilattice and is not elementarily equivalent to the ordering of r.e. wtt-degrees of unsolvability.'),
(746,'This paper presents an attack on Gong\'s proposed collisionful hash function. The weaknesses of his method are studied and possible solutions are given. Some secure methods that require additional assumptions are also suggested.'),
(747,'We introduce a generalization of Selman s P-selectivity that yields a more flexible notion of selectivity, called (polynomial-time) multi-selectivity, in which the selector is allowed to operate on multiple input strings. Since our introduction of this class, it has been used [HJRW] to prove the first known (and optimal) lower bounds for generalized selectivity-like classes in terms of EL2 , the second level of the extended low hierarchy. We study the resulting selectivity hierarchy, denoted by SH, which we prove does not collapse. In particular, we study the internal structure and the properties of SH and completely establish, in terms of incomparability and strict inclusion, the relations between our generalized selectivity classes and Ogihara s P-mc (polynomial-time membership-comparable) classes. Although SH is a strictly increasing infinite hierarchy, we show that the core results that hold for the P-selective sets and that prove them structurally simple also hold for SH. In particular, all sets in SH have small circuits, the NP sets in SH are in Low2 , the second level of the low hierarchy within NP, and SAT cannot be in SH unless P = NP. Finally, it is known that the P-selective sets are not closed under union or intersection. We provide an extended selectivity hierarchy that is based on SH and that is large enough to capture those closures of the P-selective sets, and yet, in contrast with the P-mc classes, is refined enough to distinguish them.'),
(748,''),
(749,'This is a discussion paper which presents a cryptographic solution for discretionary access control in object-oriented databases. Our approach is based on the use of pseudo-random functions and sibling intractable function families (SIFF). Each entity (object or class) in the object-oriented database model is associated with access keys that ensure secure access to that entity and all related entities. The main advantage of our approach is its ability to verify an access request during query processing. Pseudo-random functions and SIFF are applied in such a way that cryptographic keys can be generated from keys of related objects or users. The security of the system depends on the difficulty of predicting the output of pseudo-random functions and on finding extra collision for the sibling intractable function family. The authorization system supports ownership and granting/revoking of privileges.'),
(750,'The placement of rectangular objects without overlapping on a bounded surface is a generic problem that may have many applications. Space planning, chipset placement, cutting-stock problems, point-feature label placement, or the placement of articles on a newspaper page, are all instances of this more abstract problem.<br>     All these applications are concerned with the insertion of rectangular objects on a part of a bounded free surface. It is therefore important to be able to efficiently model the free space of the bounded surface.<br>     In this article we present a method to compute free space. The method is based on an iterative insertion process. Our algorithm neither depends on the size of the object to insert, nor on the method of placement. The first feature improves efficiency, while the second allows us to compare different placement methods, and to parameterize the placement system using resolution heuristics.'),
(751,'High resilient and high nonlinear Boolean functions are desirable for secure key generators in stream ciphers, for example. This paper first shows that there exists a tradeoff between resiliency and nonlinearity. Then we show a new simple design method for high resilient and high nonlinear Boolean functions. Our method gives higher non- linearity than [Zhang and Zheng 95] while their method gives larger resiliency than our method. Further, the proposed method provides a tradeoff between resiliency t and nonlinearity NF by using an intermediate parameter l. If we choose a large l, then a small t and a large NF are obtained. If we choose a small l, then a large t and a small NF are obtained.'),
(752,''),
(753,'The suspending semantic model for the execution of the MONSTR generali sed term graph rewriting language is defined. This is the canonical operational semantic model for the MONSTR language. Its correctness with respect to DACTL semantics is discussed, a nd a number of general theorems on the soundness of suspending executions with respec t to DACTL semantics are proved. General theorems are proved about the independence of susp ending primitive actions, which are useful in the verification of MONSTR systems.'),
(754,'The heuristics most of the current assignment schemes use is based on satisfying the following rule of thumb: keeping the processors busy leads to a \'good\' assignment. Such schemes are said to be work-greedy. This paper presents new bound s on the performance of work-greedy schemes, taking into account the degree of parall elism visible between the tasks and the inter-task communication delays.'),
(755,'In this paper we solve the following problem: Given a positive integer f and L weights (real numbers), find a partition <img src=\"optimum_huffman_forests/abstract_images/img1.gif\"> with f classes of the multiset of weights such that the sum of the costs of the optimum m-ary Huffman trees built for every class of <img src=\"optimum_huffman_forests/abstract_images/img1.gif\"> is minimum. An application to the optimal extendibility problem for prefix codes is proposed.      <hr align=\"left\" width=\"50%\">      1.)This work was done while the author has visited the Computer Science Department, University of Auckland, New Zealand.'),
(756,'Stack filters are a class of non-linear spatial operators used for suppression of noise in signals. In this work their design is formulated as an optimisation problem and a method that uses Genetic Algorithms (GAs) to perform the configuration is explained. Because of its computational complexity the process has been implemented as a distributed parallel GA using the Parallel Virtual Machine (PVM) software. We present the results of applying our stack filters to the restoration of magnetic resonance (MR) images corrupted with uniform, uncorellated, noise showing improved statistical performance compared with the median filter and indicating better retention of image details. The efficiency of the parallel implementation is examined, addressing both algorithmic and data decomposition, showing that execution times can be significantly reduced by distributing the task across a network of heterogeneous processors.'),
(757,''),
(758,'In this paper a characterization of the general relation between linear multisecret-sharing schemes and error-correcting codes is presented. A bridge between linear multisecret-sharing threshold schemes and maximum distance separable codes is set up. The information hierarchy of linear multisecret-sharing schemes is also established. By making use of the bridge several linear multisecret-sharing threshold schemes based on Reed-Solomon codes, generalized Reed-Solomon codes, Bossen-Yau redundant residue codes are described, which can detect and correct cheatings. The relations between linear multisecret-sharing threshold schemes and some threshold schemes for single-secret sharing are pointed out.'),
(759,'The high latency of memory operations is a problem in both sequential and parallel computing. Multithreading is a technique, which can be used to eliminate the delays caused by the high latency. This happens by letting a processor to execute other processes (threads) while one process is waiting for the completion of a memory operation. In this paper we investigate the implementation of multithreading in the processor-level. As a result we outline and evaluate a MultiThreaded VLIW processor Architecture with functional unit Chaining (MTAC), which is specially designed for PRAM-style parallelism. According to our experiments MTAC offers remarkably better performance than a basic pipelined RISC architecture and chaining improves the exploitation of instruction level parallelism to a level where the achieved speedup corresponds to the number of functional units in a processor.'),
(760,'This paper presents a genetic trajectory planning method of a robot manipulator producing the optimal trajectory between two end points. Genetic algorithm based methods seldom require a priori knowledge of a problem. Furthermore, they do not tend to fall into local optima and proceed toward the global optimum. However, they have difficulty in handling equality constraints of trajectory boundary conditions because they use probabilistic transition rules to find a solution. In this paper, we investigate the proper genetic trajectory parameterization and develop an efficient scheme for the implementation of genetic trajectory planner. We demonstrate the effectiveness and validity of the proposed approach through some simulation studies.'),
(761,'Current Computer Science (CS) research is primarily focused on solving engineering problems. Often though, promising attempts for solving a particular problem fail for non-avoidable reasons. This is what I call a negative result: something that should have worked does not. Due to the current CS publication climate such negative results today are usually camouflaged as positive results by non-evaluating or mis-evaluating the research or by redefining the problem to fit the solution. <br><br> Such publication behavior hampers progress in CS by suppressing some valuable insights, producing spurious understanding, and misleading further research efforts. Specific examples given below illustrate and back up these claims. <br><br> This paper is the announcement of a (partial) remedy: a permanent publication forum explicitly for negative CS research results, called the <I>Forum for Negative Results, FNR. </I>FNR will be a regular part of J.UCS.'),
(762,''),
(763,'The transitive coercing semantic model for the execution of the MONSTR generalised term graph rewriting language is defined. Of all the operational semantics for MONSTR that on e might consider, this one has the cleanest properties. Under intuitively obvious conditions fo r executions involving redexes permitted to overlap sufficiently to allow the programming of deterministic synchronisations, and despite the failure of exact subcommutativity, a Church-Rosser theorem is proved to hold up to markings and garbage.'),
(764,'The Gurevich\'s Abstract State Machine formalism is used to specify the well known <i>Kerberos Authentication System</i> based on the Needham-Schroeder authentication protocol. A complete model of the system is reached through stepwise refinements of <i>ASMs</i>, and is used as a basis both to discover the minimum assumptions to guarantee the <i>correctness</i> of the system and to analyse its <i>security</i> weaknesses. Each refined model comes together with a correctness refinement theorem.'),
(765,'<p>Based on the <i>ITU-T Recommendation Z.100</i> [27]---also known as SDL-92--- we define a formal semantic model of the dynamic properties of Basic SDL in terms of an <i>abstract SDL machine</i>. More precisely, we use the concept of <i>multi-agent realtime ASM</i> [17] as a semantic platform on top of which we construct our mathematical description. The resulting interpretation model is not only mathematically precise  but also reflects the common understanding of SDL in a direct and intuitive manner; it provides a <i>concise</i> and <i>understandable</i> representation of the complete dynamic semantics of Basic SDL. Moreover, the model can easily be <i>extended</i> and <i>modified</i>---a particularly important issue for an evolving technical standard.    <P>In this article, we consider all relevant aspects concerning the behavior of channels, processes and timers with respect to signal transfer operations and timer operations.  The model we obtain is intended as a basis for formal documentation as well as for executable high-level SDL specifications.</P>'),
(766,'In this article we present a process algebra in which the behaviour in the absence of certain actions can be specified. Processes of the form <img src=\"an_asynchronous_calculus_based/abstract_images/img1.gif\"> represent a behaviour which is specified by <img src=\"an_asynchronous_calculus_based/abstract_images/img2.gif\"> but only in an environment which cannot perform any action in S. If the environment can perform an action in S, the process is suspended. This is useful in specifying priority, time outs, interrupts etc. We present a few examples which illustrate the use of the extended calculus. A bisimulation relation induced by a labelled transition system is then considered. We present a few properties which form the basis for a sound and complete axiomatisation of a bisimulation equivalence relation. This requires an extension of the syntax. This is because the absence of information from the environment used in the operational semantics is captured syntactically. A comparison with other approaches is presented.'),
(767,''),
(768,'Known upper bounds on the number of required nodes (size) in the ordered binary and multiple-valued decision diagram (DD) for representation of logic functions are reviewed and reduced by a small constant factor. New upper bounds are derived for partial logic functions containing don t cares and also for complete Boolean functions specified by Boolean expressions. The evaluation of upper bounds is based on a bottom-up algorithm for constructing efficient ordered DDs developed by the author.'),
(769,'This paper generalizes the specification of Basic Interval Arithmetic Subroutines (BIAS) to support interval arithmetic on directed (i.e. proper and improper) intervals. This is due to our understanding that the arithmetic involving improper intervals will be increasingly used in future applications and the corresponding interval arithmetic implementations require no additional cost. We extend BIAS specification to be sufficiently precise and complete, to include everything a user needs, such as subroutine s purpose, name, method of invocation and details of its behaviour and communication with the environment. The specified interval arithmetic subroutines for directed intervals are consistent with conventional interval arithmetic and IEEE floating-point arithmetic.'),
(770,'Document archives contain large amounts of data to which sophisticated queries are applied. The size of archives and the complexity of evaluating queries makes the use of parallelism attractive. The use of semantically-based markup such as SGML makes it possible to represent documents and document archives as data types. We present a theory of trees and tree homomorphisms, modelling structured text archives and operations on them, from which it can be seen that: many apparently unrelated tree operations are homomorphisms, homomorphisms can be described in a simple parameterised way that gives standard sequential and parallel implementations for them, and certain special classes of homomorphisms have parallel implementations of practical importance. In particular, we develop an algorithm for path expression search, a novel powerful query facility for structured text, taking time logarithmic in the text size. This algorithm is the first example of a new algorithm discovered using homomorphic skeletons over data types.'),
(771,''),
(772,''),
(773,'A property <img src=\"persistency_of_confluence/abstract_images/img1.gif\"> of term rewriting systems (TRSs, for short) is said to be persistent if for any many-sorted TRS <img src=\"persistency_of_confluence/abstract_images/img2.gif\"> has the property <img src=\"persistency_of_confluence/abstract_images/img1.gif\"> if and only if its underlying unsorted TRS <img src=\"persistency_of_confluence/abstract_images/img3.gif\"> has the property <img src=\"persistency_of_confluence/abstract_images/img1.gif\">. This notion was introduced by H. Zantema (1994). In this paper, it is shown that confluence is persistent.      <hr align=\"left\" width=\"50%\">      1.) Proceedings of the First Japan-New Zealand Workshop on Logic in Computer Science, special issue editors D.S. Bridges, C.S. Calude, M.J. Dinneen and B. Khoussainov.'),
(774,'We examine, within the framework of Bishop\'s constructive mathematics, various classical methods for proving the existence of weak solutions of the Dirichlet Problem, with a view to showing why those methods do not immediately translate into viable constructive ones. In particular, we discuss the equivalence of the existence of weak solutions of the Dirichlet Problem and the existence of minimizers for certain associated integral functionals. Our analysis pinpoints exactly what is needed to find weak solutions of the Dirichlet Problem: namely, the computation of either the norm of a linear functional on a certain Hilbert space or, equivalently, the infimum of an associated integral functional.     <hr align=\"left\" width=\"50%\">     1.) Proceedings of the First Japan-New Zealand Workshop on Logic in Computer Science, special issue editors D.S. Bridges, C.S. Calude, M.J. Dinneen and B. Khoussainov.'),
(775,'We prove that any Chaitin &#937; number (i.e., the halting probability of a universal self-delimiting Turing machine) is wtt-complete, but not tt-complete. In this way we obtain a whole class of natural examples of wtt-complete but not tt-complete r.e. sets. The proof is direct and elementary. <hr align=\"left\" width=\"50%\"> 1.) Proceedings of the First Japan-New Zealand Workshop on Logic in Computer Science, special issue editors D.S. Bridges, C.S. Calude, M.J. Dinneen and B. Khoussainov.<br> 2.) Partially supported by AURC A18/XXXXX/62090/F3414056, 1997.<br> 3.) Partially supported by NSF Grant DMS-9500983.'),
(776,'Suppose that we have L messages coded by a prefix code (over an alphab et M with m letters) having a minimum weighted length. The problem addressed in this paper is the following: How to find s codewords for new messages so that by leaving unchanged the codification of the first L messages (by compatibility rea sons), the resulting extended code is still prefix (over M) and has a minimum weighted length? To this aim we introduce the notion of optimum extendible prefix code and then, by modifying Huffman s algorithm, we give an effcient algorithm to construct the opti mum extension of a non-complete prefix code, provided the initial code is optimal.      <hr align=\"left\" width=\"50%\">      1.) Proceedings of the First Japan-New Zealand Workshop on Logic in Computer Science, special issue editors D.S. Bridges, C.S. Calude, M.J. Dinneen and B. Khoussainov.'),
(777,'We construct a minimal automaton for an output-incomplete Moore automaton. The approach is motivated by physical interpretation of seeing deterministic finite automata as models for elementary particles. When compared to some classical methods our minimal automaton is unique up to an isomorphism and preserves also the undefined or unspecified behaviour of the original automaton.     <hr align=\"left\" width=\"50%\">     1.) Proceedings of the First Japan--New Zealand Workshop on Logic in Computer Science, special issue editors D.S. Bridges, C.S. Calude, M.J. Dinneen and B. Khoussainov.     2.)On leave from the Department of Mathematics, University of Turku, Finland.'),
(778,'The major results of Robertson and Seymour on graph well-quasi-ordering establish nonconstructively that many natural graph properties that constitute ideals in the minor or immersion orders are characterized by a finite set of forbidden sub- structures termed the <i>obstructions</i> for the property. This raises the question of what general kinds of information about an ideal are sufficient, or insufficient, to allow the obstruction set for the ideal to be effectively computed. It has been previously shown that it is not possible to compute the obstruction set for an ideal from a description of a Turing machine that recognizes the ideal. This result is significantly strengthened in the case of the minor ordering. It is shown that the obstruction set for an ideal in the minor order cannot be computed from a description of the ideal in monadic second-order logic.   <hr align=\"left\" width=\"50%\">   1.) Proceedings of the First Japan-New Zealand Workshop on Logic in Computer Science, special issue editors D.S. Bridges, C.S. Calude, M.J. Dinneen and B. Khoussainov.'),
(779,'We study the growth rate on the number obstructions (forbidden minors) for families of graphs that are based on parameterized graph problems. Our main result shows that if the defining graph problem is <i>NP</i>-complete then the growth rate on the number of obstructions must be super-polynomial or else the polynomial-time hierarchy must collapse to <IMG src=\"too_many_minor_order/abstract_images/img1.gif\">. We illustrate the rapid growth rate of parameterized lower ideals by computing (and counting) the obstructions for the graph families with independence plus size at most <IMG src=\"too_many_minor_order/abstract_images/img2.gif\">. <hr align=\"left\" width=\"50%\"> 1.) Proceedings of the First Japan-New Zealand Workshop on Logic in Computer Science, special issue editors D.S. Bridges, C.S. Calude, M.J. Dinneen and B. Khoussainov.'),
(780,'T-Codes are a class of variable-length codes. Their self-synchronization properties are useful in compression and communication applications where error recovery rather than error correction is at issue, for example, in digital telephony. T-Code s may be used without error correction or additional synchronization mechanisms. Typically, the representation of variable-length codes is a problem in computers based on a fixed-length word architecture. This presents a problem in encoder and decoder applications. The present paper introduces a fixed-length format for storing and handling variable-length T-Code codewords, the T-depletion codewords, which are derived from the recursive construction of the T-Code codewords. The paper further proposes an algorithm for the conversion of T-Code codewords into T-depletion codewords that may be used as a decoder for generalized T-Codes. As well as representing all codewords of a T-Code set (the leaf nodes in the set s decoding tree), the T-depletion code format also permits the representation of \"pseudo-T codewords\" --- strings that are not in the T-Code set. These strings are shown to correspond uniquely to all proper prefixes of T-Code codewords, thus permitting the representation of both intermediate and final decoder states in a single format. We show that this property may be used to store arbitrary finite and prefix-free variable-length codes in a compact fixed-length format.     <hr align=\"left\" width=\"50%\">     1.) The authors\' research is supported by the Department of Computer Science, the Division of Science and Technology (Tamaki Campus), and the Graduate Research Fund, all of The University of Auckland, the Centre for Discrete Mathematics and Theoretical Computer Science (CDMTCS) of the University of Auckland and the University of Waikato, and by the Deutsche Forschungsgemeinschaft (DFG).     2.) Proceedings of the First Japan-New Zealand Workshop on Logic in Computer Science, special issue editors D.S. Bridges, C.S. Calude, M.J. Dinneen and B. Khoussainov.'),
(781,'We present invariance characterizations of different types of random sequences. We correct Schnorr\'s original, incorrect characterization of Martin-Loef ran dom sequences, compare it with Schnorr s corresponding characterization of his own randomness concept, and give a similar, new characterization of Kurtz random sequences. That is, we show that an infinite sequence <img src=\"invariance_properties_of_random/abstract_images/img1.gif\"> is Kurtz random if and only if for every partial, computable, measure-invariant function <img src=\"invariance_properties_of_random/abstract_images/img2.gif\"> the sequence <img src=\"invariance_properties_of_random/abstract_images/img3.gif\"> is not recursive.      <hr align=\"left\" width=\"50%\">      1.) Proceedings of the First Japan-New Zealand Workshop on Logic in Computer Science, special issue editors D.S. Bridges, C.S. Calude, M.J. Dinneen and B. Khoussainov.'),
(782,'Every infinite binary sequence is Turing reducible to a random one. This is a corollary of a result of Peter Gacs stating that for every co-r.e. closed set with positive measure of infinite sequences there exists a computable mapping which maps a subset of the set onto the whole space of infinite sequences. Cristian Calude asked whether in this result one can replace the positive measure condition by a weaker condition not involving the measure. We show that this is indeed possible: it is sufficient to demand that the co-r.e. closed set contains a computably growing Cantor set. Furthermore, in the case of a set with positive measure we construct a surjective computable map which is more effective than the map constructed by Gacs.      <hr align=\"left\" width=\"50%\">      1 Proceedings of the First Japan-New Zealand Workshop on Logic in Computer Science, special issue editors D.S. Bridges, C.S. Calude, M.J. Dinneen and B. Khoussainov.'),
(783,'We investigate effectiveness of the completeness result for the logic with the Weak Law of Excluded Middle.     <hr align=\"left\" width=\"50%\">      1.) Proceedings of the First Japan-New Zealand Workshop on Logic in Computer Science, special issue editors D.S. Bridges, C.S. Calude, M.J. Dinneen and B. Khoussainov.     2.) Khoussainov acknowledges the support of Japan Advanced Institute of Science and Technology (JAIST) and of the University of Auckland Research Committee.'),
(784,'This paper deals, constructively, with two theorems on the sequential continuity of linear mappings. The classical proofs of these theorems use the boundedness of the linear mappings, which is a constructively stronger property than sequential continuity; and constructively inadmissable versions of the Banach-Steinhaus theorem.      <hr align=\"left\" width=\"50%\">      1.) Proceedings of the First Japan-New Zealand Workshop on Logic in Computer Science, special issue editors D.S. Bridges, C.S. Calude, M.J. Dinneen and B. Khoussainov.'),
(785,'This paper formalizes the communication of agents with modal operators in arrow logic. A communication between agents consists of an agent\'s utterance and the other agent\'s perception, thus, both of the utterance and the perception are regarded as parts of a communication channel between agents. Information is regarded as a propositional content of a sentence. An information channel where information flows can be considered to be a program, in the sense that it gets an utterance as an input and puts an output to be a perception of some agent. In the real situations, th ere are so called miscommunications. Thus, the communication channel as a program may add some noise on information indeterministically. We implement the noises are some modal operators on information. We try to formalize the communication channels in arrow logic. In that, we especially pay attention to the following three problems: channel bottleneck, unreliable channel, and reverse information. This paper\'s contribution is two-fold. First, we formalize the theory of information flow, based on situation semantics, in terms of arrow logic. Secondly, we propose the theory of communication channels between agents by using arrow logic, where, classical modal operators like knowledge, belief, and perception are distributed on various places on the communication channel. We discuss the satisfiability and the applicability of our formalization, using the test principles by Barwise on this information flow model.'),
(786,'In this paper we define and study public data structures, which are concurrent data structures in the shared memory environment, which enable access to an unknown (and possibly infinite) set of identical processes. Specific cases of such data structures (like counting networks and concurrent counters) have been studied recently, and such data structures seem to model concurrent systems like client-server applications, in which the identities of the clients, and sometimes also their number, are not known apriori. Specifically, we study the relation between wait-free and bounded wait-free public data structures - the former guarantees that every operation performed on the data structure always terminates, regardless of the relative speed of the processes, the latter guarantees that every such operation is terminated within a fixed number of steps. We present an example of a public data structure which is wait-free but not bounded wait-free, and then we show that if all the concurrent objects of the data structure are periodic, then wait-freedom implies bounded wait-freedom.'),
(787,'This paper describes local area network (LAN) access using public wide area data networks and problems that arise when using integrated services digital network (ISDN) technology [<A HREF=\"#0x811b9908_0x003b5c49\">Stallings 90</A>] [<A HREF=\"#0x811b9908_0x003b5c4a\">Thachenkary 93</A>]. To date mainly modem connections at serial lines with a terminal port have been the standard remote access technique. With ISDN it is foreseen that these modem lines will be replaced rather soon. This is mainly due to the fact that ISDN offers a more adequate bandwidth and is much more consistent from the point of view of access and embedding. This paper demonstrates in the main section a router-based solution for enhanced call management. One of the main advantages is the separation of the strategic module which defines the behavior and thus allows for a number of active connections exceeding the number of ports. It also addresses traffic and access control in the network environment.'),
(788,'This paper presents the Dortmund Family of Hypermedia Models (DFHM). Existing formal models for hypermedia mostly lack the flexibility and adaptability and, often not more than one existing system conforms to such a model. The DFHM overcomes this drawback by means of optional and alternative data types. The conformance of a hypermedia system to the DFHM can be conditionalised upon one member of the family. The DFHM has been formalised in VDM, but the aim of this paper is to give an informal overview of the main concepts. Therefore, any formalisms are omitted here. The first part of the paper deals with hypermedia fundamentals from a conceptual perspective. Apart from basic concepts, e.g. nodes and links, also structuring concepts, e.g. views, folders and others, are discussed in detail. Some examples are given to convey how models for existing hypermedia systems can be derived from the DFHM. The second part demonstrates the power of these concepts by introducing main features of a hypermedia system that has been developed for the use in educational settings. This hypermedia system bases upon a member of the DFHM.'),
(789,''),
(790,'Sequential algorithms given by Angluin (1987) and Schapire (1992) learn deterministic finite automata (DFA) exactly from Membership and Equivalence queries. These algorithms are feasible, in the sense that they take time polynomial in n and m, where n is the number of states of the automaton and m is the length of the longest counterexample to an Equivalence query. This paper studies whether parallelism can lead to substantially more efficient algorithms for the problem. We show that no CRCW PRAM machine using a number of processors polynomial in n and m can identify DFA in o(n/log n) time. Furthermore, this lower bound is tight up to constant factors: we develop a CRCW PRAM learning algorithm that uses polynomially many processors and exactly learns DFA in time O(n/log n).'),
(791,'The notion of a right to privacy of citizens in their communications is discussed in the context of an international movement by governments towards regulation of cryptography, and consideration of key forfeiture systems in national cryptography use. The authors argue that the right to privacy in communications networks is an issue of major importance, assuring freedom of the individual in national and global communications. Regulation and control of cryptography use on the Internet by national governments may lead to an imbalance in the citizen/government power relationship, with sequelae including unprecedented surveillance of citizens, disruption of international commerce due to lack of powerful cryptography (and lack of standardisation), human rights abuses by less democratic or non-democratic governments, and limiting of the political potential of an Internet global political system.'),
(792,'Two significant recent advances in cryptanalysis, namely the differential attack put forward by Biham and Shamir [BS91] and the linear attack by Matsui [Mat94a, Mat94b], have had devastating impact on data encryption algorithms. An eminent problem that researchers are facing is to design S-boxes or substitution boxes so that an encryption algorithm that employs the S-boxes is immune to the attacks. In this paper we present evidence indicating that there are many pitfalls on the road to achieve the goal. In particular, we show that certain types of S-boxes which are seemingly very appealing do not exist. We also show that, contrary to previous perception, techniques such as chopping or repeating permutations do not yield cryptographically strong S-boxes. In addition, we reveal an important combinatorial structure associated with certain quadratic permutations, namely, the difference distribution table of each differentially 2-uniform quadratic permutation embodies a Hadamard matrix. As an application of this result, we show that chopping a differentially 2-uniform quadratic permutation results in an S-box that is very prone to the differential cryptanalytic attack.'),
(793,''),
(794,'An integrated set of four subsequent single semester courses is being developed covering information technology in great width and with strong links to technical applications. The courses will become integral parts of a distance teaching university\'s curricula. Combining short phases of presence with interleaved media-based self study blocks, they are also especially well-suited for continuing education. An innovative approach is taken by integrating all course elements -- acoustic and written information, diagrams and figures, animations, simulations, video clips, and laboratory exercises -- in a single electronic document. Whereas up to now all this information is only accessible on different storage media, the new multimedia teaching programmes even replace printed material.'),
(795,'Starting from some basic characteristics of instructional video and TV steps in production and organization will be described in detail. Then the scope is extended to discuss more general design concepts in instructional filming. As video sequences frequently form part of multimedia instructional software the respective production and design concepts gain importance in software development as well.'),
(796,'A possibility to overcome the problem of organizing a diploma-thesis at a distance university with focus on practice in the design of Application Specific Integrated Circuits (ASICs) is presented with the aid of an example. A student of electrical engineering designed a fast arithmetical unit for use in a digital filter with a VHDL software package (ALLIANCE (see [laboratory 1994]) freely distributed on the internet. He did his work at home with a standard personal computer running a free Unix clone (Linux (see [Johnson 1993]) in summer 1993.'),
(797,'In any distance-education situation students and tutors are at a distance from each other at least in the sense that they are not in the same room while learning and teaching. This means that distance education relies on media. It has two constituent elements, on the one hand the presentation of learning matter, which can be described as one-way traffic, on the other hand interaction between students and tutors, which represents two-way traffic. Both are brought about by media. This was so a hundred years ago, when print, the written word and, occasionally, phonograph recordings exhausted the media repertoire, and does so now that a wealth of more and less sophisticated media are available. What, equipped with media of various kinds, distance education is capable of is the theme of this essay.'),
(798,'MediaPublishing is an application project within the BERKOM programme. More and more publications for different target groups are produced on multimedia basis. By using high speed communication networks and standardized protocols, distributed applications and scenarios for planning, editing, production and partially also for usage of multimedia publications became possible. In the realisation and test of such complex, distributed production chains for the development of media in distance teaching lies the goal of the project MediaPublishing. The first course produced with the editorial environments and communication facilities now in progress is \"Technical data security in communication networks\".'),
(799,'The use of computer conferences as \"Virtual Seminars\" has become a convenient way to allow spatially separated participants to interact under the purpose of acquiring specific knowledge in the area of distance education. In order to facilitate orientation, to indicate social meanings, and to structure the communicative processes, two different types of spatial metaphors have been applied in interface design of these telematic settings: large-scale metaphors depicting extended geographical areas (campus-sites, buildings) and small-scale metaphors depicitng rooms. Their adequateness crucially depends on the correspondence between the real world domain and software domain. Possible obstacles for this match stem from a lack of providing interactivity, from cluttering the interface with pseudorealistic details and from the specifities of the asynchronous and text-based communication modes.'),
(800,''),
(801,'The Internet facilitates access to a large amount of electronic information. However, in order to exploit the flood of information, sophisticated search facilities are needed which convert the inundation of electronic data coming from numerous sources into real knowledge. From this knowledge a whole range of users will benefit, from business people to casual surfers and shoppers on the Internet. <br><br>Intelligent agents or knowledge brokers play a vital role in realizing this vision. This paper presents a framework for knowledge brokers who search for information which is potentially available but stored in a way not always foreseen how the information will be exploited. More striking, the paper presents an architectural framework where the user can retrieve and combine knowledge uniformly, irrespective of where or how the knowledge-representing information is stored. <br><br>Lessons learned from a prototype implementation allow a discussion of shortcomings due to the emphasis of current information repositories and their interfaces, above all their poor support for knowledge combination and the difficulty of localizing the appropriate information repositories.'),
(802,'This paper considers a particular relationship defined overpairs of <I>n</I>-argument monotone Boolean functions. The relationship is of interest since we can show that if ( <I>g</I>, <I>h</I> ) satisfy it then for any <I>n</I>-argument monotone Boolean function  <I>f</I> there is a close relationship between the combinational and monotone network complexities of the function (<I>f</I>/\\<I>g</I>) \\/ h. We characterise the class of pairs of functions satisfying the relationship and show that it extends and encapsulates previous results concerning translations from combinational to monotone networks.'),
(803,'An -word <i>p</i> over a finite alphabet <i></i> is called <I>disjunctive</I> if every finite word over <img SRC=\"disjunctive_omega_words_and_real_numbers/images/img2.gif\"> occurs as a subword in <I>p</I>. A real number is called <I>disjunctive to base a</I> if it has a disjunctive <I>a</I>-adic expansion. For every pair of integers <I>a,b</I> <img SRC=\"disjunctive_omega_words_and_real_numbers/images/img3.gif\"> 2 such that there exist numbers disjunctive to base <I>a</I> but not to base <I>b</I> we explicitly construct very simple examples of such numbers. General versions of the following results are proved. If <img SRC=\"disjunctive_omega_words_and_real_numbers/images/img4.gif\"> is a strictly increasing sequence of positive integers with <img SRC=\"disjunctive_omega_words_and_real_numbers/images/img5.gif\"> for infinitely many <I>i</I> then <img SRC=\"disjunctive_omega_words_and_real_numbers/images/img6.gif\"> is disjunctive to base 2. The number <img SRC=\"disjunctive_omega_words_and_real_numbers/images/img7.gif\"> is disjunctive to base <I>a</I> if <I>a</I> is even and not a power of 2. The sum <img SRC=\"disjunctive_omega_words_and_real_numbers/images/img8.gif\">is disjunctive to base 6 if <I>c <img SRC=\"disjunctive_omega_words_and_real_numbers/images/img3.gif\"></I> 3 is odd.'),
(804,''),
(805,'An edge-flipping operation in a triangulation <i>T</i> of a set of points in the plane is a local restructuring that changes <i>T</i> into a triangulation that differs from <i>T</i> in exactly one edge. The edge-flipping distance between two triangulations of the same set of points is the minimum number of edge-flipping operations needed to convert one into the other. In the context of computing the rotation distance of binary trees Sleator, Tarjan, and Thurston show an upper bound of 2<i>n</i> - 10 on the maximum edge-flipping distance between triangulations of convex polygons with <i>n</i> nodes, n > 12. Using volumetric arguments in hyperbolic 3-space they prove that the bound is tight. In this paper we establish an upper bound on the edge-flipping distance between triangulations of a general finite set of points in the plane by showing that no more edge-flipping operations than the number of intersections between the edges of two triangulations are needed to transform these triangulations into another, and we present an algorithm that computes such a sequence of edge-flipping operations.'),
(806,'LATE is the acronym for Learning And Teaching Environment, a concept that pulls together a number of developments in a uniquely integrated way to provide - for the first time in the history of computing - an approach to computer assisted learning and teaching that offers a realistic alternative to more traditional methods. Above claim may sound exaggerated. However, the reader is asked to reserve judgement until having looked at the details of this paper. LATE is not just one other concept in the area of computer assisted instructional technologies - an area rich in projects with high-expectations but few real success stories. LATE is more: it is based on much experience with earlier undertakings, both negative and positive, and is positioned to be a definitive answer to educational needs as they can be provided by networked multimedia systems now and in the forseeable future. LATE is positioned as concept that can be turned into a fully functional and applicable system, open-ended to incorporate further modules, improvements and future computer and network technology. In this sense LATE can be aptly described by the longer acronym: IS THE LATEST, standing for Integrating Systematically Technologies Helping to Establish Learning And Teaching Environment Strategies.<br><br>    It has been over 30 years that computer-based solutions for educational purposes have been investigated and experimented with. Results have been consistently exasperating modest. Indeed some frustrated researchers and developers have started to seriously consider the possibility that the educational process - being highly oriented towards individual needs - may not be computerizeable at all to a large extent.<br><br>    The LATE concept as put forward in this paper - will show that above mentioned pessimism is - fortunately - not justified. Rather: it has taken a long time to effectively introduce computers into the educational process, but the LATE concept, once implemented, provides the ultimate answer. Hence our pun: LATE is better than never! And hence our request to all European funding agencies, and the European Commission in particular, to provide the financial assistance to turn LATE from a concept into a reality.<br><br>    LATE is not a new attempt at developing a computer assisted instruction (CAI) or computer based learning (CBL) system; it is not a system providing modern lecturing techniques; it is not another attempt to implement digital libraries; it should not be seen as a novel authoring environment, a distance teaching system, a networked help system, discussion forum, computer supported collaborative environment or a networked model of modern computer based decision rooms. LATE must not be understood as any of the above: LATE is the rich symbiosis of all of the above resulting in an environment of breadth and dimensions sofar unheard of.<br><br>    If this sounds too good to be true: do continue reading; above is not intended to give a project description, but to \"whet the appetite\" for reading what follows. There we will show that and how the symbiosis of the diversity of areas is indeed feasable when a modern WWW system with much extended functionality is used as a basis.'),
(807,''),
(808,'The introduction of new communication technologies such as the World Wide Web are creating unique opportunities for AEC project teams to develop new coordination and communication strategies. Of particular interest is the capability of teams to interact remotely in a virtual team environment. However, this evolution of project team interactions is introducing a diverse range of new issues in project management and process control which requires a new generation of management frameworks. This paper introduces the requirements of these management frameworks as developed through studies of interdisciplinary, virtual project teams working together over the World Wide Web.      <hr align=\"left\" width=\"50%\">      1.) This paper is an extension of \"Managing Interdisciplinary Project Teams Through the Web\", Paul S. Chinowsky amd Robin E. Goodmann, WebNet \'96: The First World Conference of the Web Society, San Francisco CA, October 1996'),
(809,'The explosive growth of the World Wide Web, and the resulting information overload, has led to a mini-explosion in World Wide Web search engines. This mini-explosion, in turn, led to the development of ProFusion, a meta search engine. Educators, like other users, do not have the time to evaluate multiple search engines to knowledgeably select the best for their uses. Nor do they have the time to submit each query to multiple search engines and wade through the resulting flood of good information, duplicated information, irrelevant information, and missing documents. ProFusion sends user queries to multiple underlying search engines in parallel, retrieves and merges the resulting URLs. It identifies and removes duplicates and creates one relevance-ranked list. If desired, the actual documents can be pre-fetched to remove yet more duplicates and broken links. ProFusion s performance has been compared to the individual search engines and other meta searchers, demonstrating its ability to retrieve more relevant information and present fewer duplicates pages. The system can automatically analyze queries to identify its topic(s) and, based on that analysis, select the most appropriate search engines for the query.      <hr align=\"left\" width=\"50%\">      1.) This paper is an extension of \"Information Fusion with ProFusion\", Susan Gauch and Guijun Wang, WebNet \'96: The First World Conference of the Web Society, San Francisco, CA, October 1996.      <a href=\"http://www.designlab.ukans.edu/ProFusion.html\">http://www.designlab.ukans.edu/ProFusion.html'),
(810,'Web and Internet technologies have traditionally been used to serve information across machines and among people. Recently, there has been a great deal of interest shown in using these information services to support <I>digital libraries</I>. Research in digital libraries is an interdisciplinary effort that must synthesize existing results from highly disparate fields. This paper examines two such contributing fields - information systems and orality-literacy studies - and applies them to a particular digital library domain, botanical taxonomic work. In trying to build digital libraries for botanical taxonomists, we show how two widely differing fields can each provide part of a solution neither can provide alone.'),
(811,'The first part of the paper identifies disadvantages of first generation Web publishing solutions that have to be overcome for professional publication providers. Using Hyper-G for distribution of electronic documents opens the way to the first fully-integrated professional publishing solution on the Web. User and group access rights as well as billing mechanisms are integrated into the server, links are bidirectional and annotations to existing documents are possible without changing the contents of documentsi themselves. Hyperlinks are supported in arbitrary document types besides hypertext which opens the way to new publishing paradigms beyond the paper-based approach. Naturally, a rich set of structure and search mechanisms is provided. On this basis, a set of tools has been developed that almost fully automatically supports electronic refereeing, automatic hyperlink creation, glossary links and table of contents generation. All the data prepared on a Hyper-G server can then be simply exported to CD-ROM, allowing hybrid Web/CD-ROM publications to be created without any additional effort.'),
(812,''),
(813,'Alter Egos represent people in Cyberspace. An Alter Ego is a kind of intelligent agent who is active in performing actions in behalf of the person it represents. How these Alter Egos can be modelled and constructed is discussed. In this context the question whether Alter Egos can be held responsible is studied. The tools we use for the modelling process are using Linguistic knowledge and are logically founded.      <hr align=\"left\" width=\"50%\">      1.) This paper is an extension of \"Linguistic Tools for Modelling Alter Egos in Cyberspace: Who is responsible?\", R.P. van de Riet and J.F.M. Burg, WebNet \'96: The First World Conference of the Web Society, San Francisco, CA, October 1996'),
(814,''),
(815,'The paper presented here discusses the need to develop a technologically oriented trauning model for K-12 teachers. This model would have, as its core, a context, process, and content emphasis. To implement the type of design discussed, a series of training modules that incorporate context, process, and content are developed. Each is patterned for specific school needs and rescources. Implementation strategies are also included.'),
(816,'This paper reports on the design and development of an electronic classroom and conference room which was opened in the city of Linz, Austria, in late 1996. This room was developed in a multidisciplinary approach and contains several unique features. After an introduction about the background, rationale for merging the concepts of classroom and conference room, and an overview about the four layers of the logical architecture, the paper emphasizes issues associated with the facilities installed. Then the attention is drawn upon a \"consumables\" layer which we propose to insert between such facilities and the usual software tools. A brief overview of the tools and contents layers completes the description.'),
(817,'Recently, there has been growing excitement over the use of electronic classrooms. This paper provides a carefully balanced view of the advantages of using various technological facilities to improve teaching and learning, while recognizing the continuing need for a human teacher who coordinates and controls the learning process. In particular, our view is that technology should not be intrusive and should serve as a \"means\" towards specific educational goals, not as an \"end\" in itself. To illustrate this view we offer as a \"case study\" our vision of a course that uses electronic classrooms to assist teachers and students.'),
(818,'Seventeen students completed a course in which no face-to-face meetings and no paper exchanged hand. All information was shared either on the WWW or by email. In the first few weeks, extensive email dialogue occurred about the method of learning but after that the students focused exclusively on the content of the course. The first readings and exercises gave the students much freedom of choice but that proved too much freedom for the students. They preferred the assignment to be highly focused. The automatic indexing of email by the Hypermail program needed to be augmented with manual indexing by the teacher. This indexing and generally managing the email was the most time consuming aspect of the teaching, but in the end this was less demanding on the teacher than would have been a traditional course. The students rated the course as an above-average learning experience.'),
(819,'Although the technology exists to provide collaborative distance learning and training through electronic networks and groupware, little is currently known about appropriate ways in which to structure these learning environments. This article describes two research projects using groupware for collaborative learning activities. The first was a graduate business course conducted entirely online with geographically dispersed individuals. The second project investigated the use of groupware for collaborative writing and problem solving at a military academy with undergraduate students. Results and conclusions are presented to inform others working with computer networks and groupware.'),
(820,'Firstly, we consider important concepts to be taken into account when thinking about the classroom organisation and the design of learning environments. The concept of classroom integrates different facets including at first interaction between children and a system of available knowledge carried out either by human beings or multi-media contents. A very important facet is that the classroom is also a context for social interaction. Knowledge acquisition by human beings makes sense only if we take into account the need for socialisation of knowledge and a process that allows the sharing of it. We then consider recent results in cognitive sciences especially in the study of learning theories and focus on why and how this is to be considered when designing and implementing educational technologies. We need to consider user-centered approaches and human factors in the classroom.<br><br>     We then describe the major characteristics of the classroom for future. The model we propose for the classroom is to be splitted in two parts : the kernel (as a set of real places where persons can meet physicaly) and the cloud (as a set of real or virtual places) reachable from the kernel via networks. The kernel plays a role of a fix point very useful for people having roots. The balance between activities taking place in the kernel and the cloud is then analysed. This leads us to discuss some basic principles underlying the global organisation. This kind of approach helps us to specify more precisely different points : teachers\' roles and constraints on teachers\' education. The need for mediatised resources meeting the needs in such schools leads us to propositions for methodologies to design educational resources.'),
(821,''),
(822,'In this paper, we sharpen the results of Gaeartner on the universality of partition-limited ET0L systems by showing that such deterministic systems characterize the recursively enumerable sets, and, furthermore, the propagating deterministic partition-limited ET0L systems characterize the programmed languages with appearance checking disallowing erasing productions. The main results of this paper have been announced in [10].'),
(823,'Variable-length T-Codes may be used to provide robust compression for data communication and storage on noisy channels or media. Over the past twelve years, a number of papers on T-Codes have been published in various journals and as technical reports. During this time, notation and scope of the T-Codes have changed considerably, giving rise to a more integrated theory of T-Codes as recursive codes. This paper presents all known core principles of T-Code theory by taking a recursive approach throughout. A sufficient condition for information sources, ensuring decoder self-synchronisation for the T-encoded symbol stream, is introduced. By example of a recursive program, the paper shows how a suitable T-Code set for encoding a given memoryless source can be found.'),
(824,'In this paper we discuss the issue of the minimal instruction set necessary for universal computation. Our computing model is a machine consisting of a processor with a single n-bit register and a separate memory of n-bit words. We show that four simple instructions are sufficient in order to evaluate any computable function. Such reduction of the instruction set can only be achieved by exploiting the properties of self-modifying programs. Then we prove that, surprisingly, conditional branching can be substituted by unconditional branching. This is the main result of this paper. Therefore any computable function can be computed using only the instructions LOAD, STORE, INC and GOTO (unconditional branching). We also show that externally stored looping programs using indirect addressing and no branches are as powerful as conventional computer programs.'),
(825,''),
(826,'Since early 1994 the introductory course 2L670, \"Hypermedia Structures and Systems\", has been available on World Wide Web, and is an optional part of the curriculum in computing science at the Eindhoven University of Technology. The course has since been completed by more than 200 students from three different universities, two in the Netherlands and one in Belgium. Since January 1st 1996 the course is also offered at two more Dutch universities and at the Dutch Open University.     <br><br>     In order to participate in this course the student only needs a World Wide Web browser. There is no need for separate e-mail, netnews, bulletin boards or ftp software. In this paper we present the technical environment for the latest edition of the course, which features automatic evaluation of small assignments, a repository for assignment work, a discussion system, and complete monitoring of each student\'s progress. We also reflect on the previous architecture, used for the first 200 students, and draw lessons from that experience.'),
(827,'This paper considers the framework in which World-Wide-Web based online courseware can be developed. This framework is illustrated using courseware developed for parallel computing, C programming, X-Window programming and computer vision.'),
(828,'A growing number of instructors are putting course resources on the World Wide Web (WWW) [<A HREF=\"/jucs_2_12/evaluating_and_improving_www/Rebelsky_S_A.html#0x811b9908_0x003b76dc\">Berners-Lee et al. 1994</A>], from simple course descriptions through traditional printed handouts to complete \"classroom-free\" classes ([<A HREF=\"/jucs_2_12/evaluating_and_improving_www/Rebelsky_S_A.html#0x811b9908_0x003b76dd\">Team Web 1995</A>] provides a broad sampling of such resources). However, there appears to be a paucity of evaluation of WWW based classroom resources. Do they help or do they hurt? Which materials are more valuable or less valuable? How do students react to the web?<p>This paper describes the design, evaluation, redesign, and re-evaluation of a number of course webs that incorporate a wide range of resources (including readings, notes, transcriptions, and traditional handouts) and media (including text, images, and audio). This paper generalizes student reactions to webs for two introductory Computer Science courses [<A HREF=\"/jucs_2_12/evaluating_and_improving_www/Rebelsky_S_A.html#0x811b9908_0x003b76de\">Rebelsky 1994</A>] [<A HREF=\"/jucs_2_12/evaluating_and_improving_www/Rebelsky_S_A.html#0x811b9908_0x003b76df\">Rebelsky 1996</A>], incorporating additional comments from students in advanced courses.<p><strong>Key Words:</strong> Multimedia Information Systems [Evaluation/Methodology], Computer Uses in Education, World-Wide Web, Hypertext Document Design and Preparation, Computer Science Education, Computer Literacy.'),
(829,'The development of a global ethical system is desirable to fulfil the societal promise of the technology of the Internet. The author attempts to construct such a \"pragmatic\" ethical system (in an age of relativism and ethical scepticism) using concepts drawn from a number of moral philosophers. The global ethics construct uses the quality of human rationality as a basis, co-existing with the essence of \"identity\" in religious, cultural, and personal terms for individuals. The potential of the constructed Internet \"global citizen\" is unquantifiable, but the concept represents an opportunity for global co-operation and mingling of ideas, personalities, cultures, and solutions to problems. It is an unconstrained scenario utilising key qualities of the technology including rapid information turnaround and unprecented individual access by vast numbers of the global population.'),
(830,''),
(831,'This paper is devoted to the software implementation of two mathematical methods which are often used in biological applications: interpolation and curve fitting in the presence of uncertainties in the input data given in the form of intervals. The methods involve model functions linear in their parameters and are formulated by means of simple expressions in terms of interval arithmetic allowing the computation of verified bounds for the interpolating/approximating functions. The methods are demonstrated for certain classes of nonlinear modelling functions finding applications in biology. A case study involving enzyme-catalysed reaction is considered. The numerical results are performed in the computer algebra system Mathematica, which supports interval-arithmetic computations.'),
(832,'The inexact information system is based on linguistic terms which have values lying in the interval [0,1]. Imprecision has advantages, because fuzzy sets avoid the rigidity of conventional mathematical reasoning and computer programming. Fuzzy quantifiers are made explicit by means of fuzzy logic. Many systems, for example, complex biological processes, cannot be programmed in a precise way. With fuzzy sets the implicit quantifiers can be easily translated into machine usable form. This paper discusses a method for the description of fuzzy quantifiers in formal languages. A comparison between approximate reasoning and the method of linear interpolation is made. Inexact information in biological and medical expert systems, and the reliability inferences based on it, are also discussed.'),
(833,'Biological sequence comparison is a time consuming task on a Von Neuman computer. The addition of dedicated hardware for parallelizing the comparison algorithms results in a reduction of several orders of magnitude in the execution time. This paper presents and compares different dedicated approaches, based on the parallelization of the algorithms on linear arrays of processors.'),
(834,'A molecular computational procedure in which manipulation of DNA strands may be harnessed to solve a classical problem in NP - the directed Hamiltonian path problem - was recently proposed [<A HREF=\"on_the_scalability_of/Mac_Donaill_D_A.html#0x811b9908_0x003b8198\">Adleman 1994</A>], [<A HREF=\"on_the_scalability_of/Mac_Donaill_D_A.html#0x811b9908_0x003b8199\">Gifford 1994</A>]. The procedure is in effect a massively parallel chemical analog computer and has a computational capacity corresponding to approximately<img SRC=\"on_the_scalability_of/images/3138634c.gif\"> CPU years on a typical 10 MFLOP workstation. In this paper limitations on the potential scalability of molecular computation are considered. A simple analysis of the time complexity function shows that the potential of molecular systems to increase the size of generally solvable problems in NP is fundamentally limited to<img SRC=\"on_the_scalability_of/images/3138639e.gif\"> . Over the chemically measurable picomolar to molar concentration range the greatest practical increase in problem size is limited to<img SRC=\"on_the_scalability_of/images/313863ba.gif\">'),
(835,''),
(836,'This is the first in a series of papers dealing with the implementation of an extended term graph rewriting model of computation (described by the DACTL language) on a distributed store architecture. In this paper we set out the high level model, and under some simple restrictions, prove an abstract packet store implementation correct modulo garbage. The abstract packet store model is compared to a more realistic and finegrained packet store model, more closely related to the properties of a genuine distributed store architecture, and the differences are used to inspire the definition of the MONSTR sublanguage of DACTL, intended for direct execution on the machine. Various alternative operational semantics for MONSTR are proposed to reflect more closely the finegrained packet store model, and the prospects for establishing correctness are discussed. The detailed treatment of the alternative models, in the context of suitable sublanguages of MONSTR where appropriate, are subjects for subsequent papers.'),
(837,'We show that it is decidable whether or not the set of coefficients of a given Q-algebraic sequence is finite. The same question is undecidable for Q-algebraic series. We consider also prime factors of algebraic series.'),
(838,'We continue the investigations begun in [11] on the relationships between several variants of the splicing operation and usual operations with formal languages. The splicing operations are defined with respect to arbitrarily large sets of splicing rules, codified as simple languages. The closure properties of families in Chomsky hierarchy are examined in this context. Several surprising results are obtained about the generative or computing power of the splicing operation. Many important open problems are mentioned.'),
(839,''),
(840,'This note collects some open problems in AIT which have been discussed during the Summer School \"Chaitin Complexity and Applications. Each problem comes with the name(s) of the person(s) who suggested it, and more details about the listed open problems can be found directly by contacting the correspondent author: G. J. Chaitin (email: <a href=\"chaitin@watson.ibm.com\">chaitin@watson.ibm.com</a>), F. Geurts (email: <a href=\"gf@info.ucl.ac.be\">gf@info.ucl.ac.be</a>), H. Juergensen (email: <a href=\"helmut@uwo.ca\">helmut@uwo.ca</a>), T. Odor (email: <a href=\"odor@math-inst.hu\">odor@math-inst.hu</a>), K. Svozil (email: <a href=\"svozil@tph.tuwien.ac.at\">svozil@tph.tuwien.ac.at</a>). This list will be periodically up-to-dated in the home-page of the CDMTCS at url <a href=\"http://www.cs.auckland.ac.nz/CDMTCS/researchreports/ait.openproblems.ps.gz\">http://www.cs.auckland.ac.nz/CDMTCS/researchreports/ait.openproblems.ps.gz</a>      <hr align=\"left\" width=\"50%\">      1.) C. Calude (ed.). The Finite, the Unbounded and the Infinite, Proceedings of the Summer School ``Chaitin Complexity and Applications\'\', Mangalia, Romania, 27 June -- 6 July, 1995.'),
(841,'Kraft\'s inequality [9] is essential for the classical theory of noiseless coding [1, 8]. In algorithmic information theory [5, 7, 2] one needs an extension of Kraft\'s condition from finite sets to (infinite) recursively enumerable sets. This extension, known as Kraft-Chaitin Theorem, was obtained by Chaitin in his seminal paper [4] (see also, [3, 2]). The aim of this note is to offer a simpler proof of Kraft-Chaitin Theorem based on a new construction of the prefix-free code.      <hr align=\"left\" width=\"50%\">      1.) C. Calude (ed.). The Finite, the Unbounded and the Infinite, Proceedings of the Summer School \"Chaitin Complexity and Applications\", Mangalia, Romania, 27 June - 6 July, 1995.<br>      2.) The work of the first author has been partially supported by Auckland University Research Grant A18/ XXXXX/62090/3414050.'),
(842,''),
(843,'Both empirical sciences and computations are fundamentally restricted to measurements/computations involving a finite amount of information. These activities deal with the FINITE - some finite precision numbers, coming out from measurements, or from calculations run for some finite amount of time. By way of contrast, as Leibniz expressed it, mathematics is the science of the INFINITE, which contains the concept of continuum. The related concepts of limit points, derivatives and Cantor sets also belong to mathematics, the realm of the infinite, and not to the world of the finite. One is then lead to wonder about the basis for the \"unreasonable effectiveness of mathematics in the natural sciences\" (Wigner (1960). This puzzling situation gave birth, over the centuries, to a very lively philosophical discussion between mathematicians and physicists. We intend to throw into the debate a few simple examples drawn from practice in numerical analysis as well as in finite precision computations. By means of these examples, we illustrate some aspects of the subtle interplay between the discrete and the continuous, which takes place in Scientific Computing, when solving some equations of Physics.<br><br>Is Nature better described by discrete or continuous models at its most intimate level, that is below the atomic level? With the theory of quantum physics, it seems that the question has received a significant push towards a discrete space. However one can argue equally that the time variable in Schrdinger\'s equation is continuous. We will not get involved in the scholarly dispute between the continuous and the discrete. Instead, we will show on simple examples taken from Scientific Computing, the subtlety of the interplay between the continuous and the discrete, which can take place in computations, be it with finite precision or exact arithmetic.        <hr align=\"left\" width=\"50%\">      1.) C. Calude (ed.). The Finite, the Unbounded and the Infinite, Proceedings of the Summer School \"Chaitin Complexity and Applications\", Mangalia, Romania, 27 June - 6 July, 1995.'),
(844,''),
(845,'In the spirit of Shannon s theory of secrecy systems we analyse several possible natural definitons of the notion of perfect secrecy, these definitions are based on arguments taken from probability theory, information theory, the theory of computational complexity, and the theory of program-size complexity or algorithmic information. It turns out that none of these definitions models the intuitive notion of perfect secrecy completely: Some fail because a cryptographic system with weak keys can be proven to achieve perfect secrecy in their framework, others fail, because a system which, intuitively, achieves perfect secrecy cannot be proven to do so in their framework. <br><br>To present this analysis we develop a general formal framework in which to express and measure secrecy aspects of information transmission systems. <br><br>Our analysis leads to a clarification of the intuition which any definition of the notion of perfect secrecy should capture and the conjecture, that such a definition may be impossible, that is, that only secrecy by degrees can be defined rigorously. <br><br>This analysis also leads to a clarification of what the cryptographic literature refers to as the one-time pad. On the basis of the arguments used for its strength in the literature, one has to distinguish between two quite different systems: the first kind uses randomly chosen strings of some given length, the second kind uses random strings, that is, patternless strings of some given length. The former achieves perfect secrecy in the sense of Shannon, but permits weak keys - like the all-zero key, the latter, while intuitively stronger, does not achieve perfect secrecy in any of the proposed senses. <br><br>Finally, the analysis exposes the need for a formal, non-operational, but mathematical definition of the notion of weak key.      <hr align=\"left\" width=\"50%\">      1.) C. Calude (ed.). The Finite, the Unbounded and the Infinite, Proceedings of the Summer School \"Chaitin Complexity and Applications\", Mangalia, Romania, 27 June - 6 July, 1995. The research reported in this paper was supported by the Natural Sciences and Engineering Council of Canada, Grant OGP0000243.'),
(846,'<hr align=\"left\" width=\"50%\">   1.) C. Calude (ed.). The Finite, the Unbounded and the Infinite, Proceedings of the Summer School \"Chaitin Complexity and Applications\", Mangalia, Romania, 27 June - 6 July, 1995.'),
(847,'The Connex Memory is a new memory structure proposed by G. Stefan as a hardware support for symbolic processing. The powerful set of memory access functions supported by the CM is expected to allow a faster and less resource consuming execution of functional languages on dedicated architectures. This paper presents an interpreter of Chaitin s Toy LISP written for a CM-based system with stack controller.      <hr align=\"left\" width=\"50%\">      1.) C. Calude (ed.). The Finite, the Unbounded and the Infinite, Proceedings of the Summer School \"Chaitin Complexity and Applications\", Mangalia, Romania, 27 June - 6 July, 1995.'),
(848,'This paper describes an implementation of Chaitin\'s ToyLisp [Chaitin  \'87] on a Connex Memory Machine (CMM) [Stefan  \'85]. The Connex Memory Machine has a smaller complexity than previous Universal Machines used to run Lisp programs, so the time and space used in running Lisp programs can be considerably decreased. A ToyLisp like language can be used with a CMM to construct a Lisp (Co)Processor for accelerating Lisp processing in conventional architectures.      <hr align=\"left\" width=\"50%\">      1.) C. Calude (ed.). The Finite, the Unbounded and the Infinite, Proceedings of the Summer School \"Chaitin Complexity and Applications\", Mangalia, Romania, 27 June - 6 July, 1995.'),
(849,'We discuss some effective characterizations of the prime elements in a polynomial ring and polynomial factorization techniques. We emphasize that some factorization methods are probabilistic, their efficiency justifies the experimental trend in mathematics. The possibility of an effective version of Hilbert\'s irreducibility theorem and the probabilistic techniques of Berlekamp will be also discussed. Finally, bounds on the heights of integer polynomials are used as tools for improving polynomial factorizations.      <hr align=\"left\" width=\"50%\">      1 C. Calude (ed.). The Finite, the Unbounded and the Infinite, Proceedings of the Summer School \"Chaitin Complexity and Applications\", Mangalia, Romania, 27 June - 6 July, 1995.'),
(850,'The agenda of quantum algorithmic information theory, ordered `top-down,  is the quantum halting amplitude, followed by the quantum algorithmic information content, which in turn requires the theory of quantum computation. The fundamental atoms processed by quantum computation are the quantum bits which are dealt with in quantum information theory. The theory of quantum computation will be based upon a model of universal quantum computer whose elementary unit is a two-port interferometer capable of arbitrary U(2) transformations. Basic to all these considerations is quantum theory, which is most conveniently expressible in Hilbert space.      <hr align=\"left\" width=\"50%\">      1.) C. Calude (ed.). The Finite, the Unbounded and the Infinite, Proceedings of the Summer School \"Chaitin Complexity and Applications\", Mangalia, Romania, 27 June - 6 July, 1995.'),
(851,''),
(852,'The paper contains completeness criterions for pseudosimple sets. Those criterions are constructed using effectivization of the definitions as well as extensionally bounded functions.'),
(853,'We prove that grammar systems with (prescribed or free) teams (of constant size at least two or arbitrary size) working as long as they can do, characterize the family of languages generated by (context-free) matrix grammars with appearance checking; in this way, the results in [<A HREF=\"Freund_R.html#0x811b9908_0x003b6520\">Paun, Rozenberg 1994</A>] are completed and improved.'),
(854,'We show that nonzero axioms add to the generative capacity of Lindenmayerian series generating systems. On the other hand, if nonzero axioms are allowed, nonterminals do not, provided that only quasiregular series are considered.'),
(855,'One of the problems that we experience with today\'s most widespread Internet Information Systems (like WWW or Gopher) is the lack of support for maintaining referential integrity. Whenever a resource is (re)moved, dangling references from other resources may occur.<p>This paper presents a scalable architecture for automatic maintenance of referential integrity in large (thousands of servers) distributed information systems. A central feature of the proposed architecture is the <strong>p-flood</strong> algorithm, which is a scalable, robust, prioritizable, probabilistic server-server protocol for efficient distribution of update information to a large collection of servers.<p>The <strong>p-flood</strong> algorithm is now implemented in the Hyper-G system, but may in principle also be implemented as an add-on for existing WWW and Gopher servers.<p><strong>Keywords:</strong> Hypertext, Link Consistency, Distributed Information System, Internet, Gopher, WWW, Hyper-G, Scalability, p-flood.'),
(856,'The connections among the various nonlinearity criteria is currently an important topic in the area of designing and analyzing cryptographic functions. In this paper we show a quantitative relationship between propagation characteristics and nonlinearity, two critical indicators of the cryptographic strength of a Boolean function. We also present a tight lower bound on the nonlinearity of a cryptographic function that has propagation characteristics.'),
(857,''),
(858,'As the Internet continues to experience exponential rates of growth, attention is shifting away from mainstream network services such as electronic mail and file transfer to more interactive information services. Current network information systems, whilst extremely successful, run into problems of fragmentation, consistency, scalability, and loss of orientation.<br><br>The development of \'second generation\' network information systems such as Hyper-G can help overcome these limitations. Of particular note are Hyper-Gs tightly-coupled structuring, linking, and search facilities, its projection of a seamless information space across server boundaries with respect to each of these facilities, and its support for multiple languages. The Harmony client for Hyper-G utilises two and three-dimensional visualisations of the information space and couples location feedback to search and link browsing operations, in order to reduce the likelihood of disorientation. This paper presents a comprehensive overview of Hyper-G and Harmony.'),
(859,'The World-Wide Web is the most talked-about distributed information system today. This paper does not touch on its workings, it tries to give a brief history and outlines the feelings provoked by the explosive adoption in all circles of WWW as the first vehicle on the Global Information Infrastructure.'),
(860,'Publishing the results of scientific research is the basis of the advancement of science, technology and medicine.'),
(861,'How the Internet Gopher system has evolved since its first released in 1991 and how Internet Gopher relates to other popular Internet information systems. Current problems and future directions for the Internet Gopher system.'),
(862,'WAIS (Wide Area Information Servers), a development of Thinking Machine Corporation, turned out to be one of the main search engines in connection with the World Wide Web (WWW). This article gives a short overview of WAIS, its history, its basics and some connected developments.'),
(863,''),
(864,'Given a biconnected network G with n nodes and a specific edge (r, s) of G, the st-numbering problem asks for an assignment of integers to the nodes satisfying the following condition: r is assigned the number 1 and s is assigned the number n and all other nodes are assigned numbers in such a way that every node (other than r and s) has a neighbour with smaller st-number and a neighbour with larger st-number. Since st-numbering exists iff G is biconnected, it serves as a powerful \"local characterization\" of the \"global\" property of the network. We present an efficient O(e) message complexity and O(n) time complexity algorithm for st-numbering a biconnected graph.'),
(865,'We show that it is decidable, given a number system <I>N</I>, whether or not there is an unambiguous number system equivalent to <I>N</I>.'),
(866,'A zero decomposition algorithm is presented and used to devise a method for proving theorems automatically in differential geometry and mechanics. The method has been implemented and its practical efficiency is demonstrated by several non-trivial examples including Bertrand s theorem, Schell s theorem and Kepler-Newton s laws.'),
(867,''),
(868,'For 30 years repeated attempts have been made to use computers to support the teaching and learning process, albeit with only moderate success. Whenever major attempts failed, some seemingly convincing reasons were presented the for less than satisfactory results. In the early days cost or even lack of suitable equipment was blamed, after colour graphics computers started to be widespread, production costs of interactive and graphically appealing material were considered the main culprits, when modern multimedia authoring techniques did not change the situation either, the lack of personalized feed-back, of network support and the difficulty of producing high quality simulations were seen as main obstacles. With networks now offering excellent multimedia presentation and communication facilities the final breakthrough of computers as ultimate teaching and learning tool is (once more) predicted. And once more results will be disappointing if one crucial component is again overlooked: good courseware must give both guidance to students but also provide a rich variety of background material whenever such is needed. It is the main claim of this paper that the advent of sizeable digital libraries provides one of the most significant chances for computer based training ever. We will argue that such libraries not only allow the efficient production of courseware but also provide the extensive background reservoir of material needed in many situations.'),
(869,'High speed devices for public key cryptography are of emerging interest. For this reason, the <img SRC=\"testing_a_high_speed/images/30c30836.gif\"> crypto chip was designed. It is an architecture capable of performing fast RSA encryption and other cryptographic algorithms based on modulo multiplication. Besides the modulo multiplication algorithm called FastMM, the reasons for its high computation speed are the As Parallel As Possible (APAP) architecture, as well as the high operation frequency. The <img SRC=\"testing_a_high_speed/images/30c30836.gif\"> crypto chip also contains on-chip RAM and a special-purpose control logic, enabling special features like encrypted key loading. However, this control mechanism influences to some extend testability of the MM data path which is the heart of the chip. For this reason, the <img SRC=\"testing_a_high_speed/images/30c3087b.gif\"> crypto chip has been designed to be able to evaluate the behaviour of the pure MM data path. In the following, we describe the strategies used with the <img SRC=\"testing_a_high_speed/images/30c3087b.gif\"> crypto chip for testing the MM data path under realistical conditions. In this context, analyzing control signal flow turns out to be the key action.<p>This work has been sponsored as part of the project Nr. P9384PHY \"Sichere Kommunikation bei hohen Geschwindigkeiten\" by the Austrian Science Foundation.'),
(870,'In this paper we attempt to compare features of WWW and Hyper-G, the first fully operable networked multimedia system that goes much beyond WWW and incorporates many features first proposed in Xanadu and later partially tested in systems such as Intermedia.'),
(871,''),
(872,'Numerical validation of computed results in scientific computation is always an essential problem as well on sequential architecture as on parallel architecture. The probabilistic approach is the only one that allows to estimate the round-off error propagation of the floating point arithmetic on computers. We begin by recalling the basics of the CESTAC method (Controle et Estimation Stochastique des Arrondis de Calculs). Then, the use of the CADNA software (Control of Accuracy and Debugging For Numerical Applications) is presented for numerical validation on sequential architecture. On parallel architecture, we present two solutions for the control of round-off errors. The first one is the combination of CADNA and the PVM library. This solution allows to control round-off errors of parallel codes with the same architecture. It does not need more processors than the classical parallel code. The second solution is represented by the RAPP prototype. In this approach, the CESTAC method is directly parallelized. It works both on sequential and parallel programs. The essential difference is that this solution requires more processors than the classical codes. These different approaches are tested on sequential and parallel programs of multiplication of matrices.'),
(873,''),
(874,'The effect of round-off errors on the numerical solution of the heat equation by finite differences can be theoretically determined by computing the mean error at each time step. The floating point error propagation is then theoretically time linear. The experimental simulations agree with this result for the towards zero rounding arithmetic. However the results are not so good for the rounding to the nearest artihmetic. The theoretical formulas provide an approximation of the experimental round-off errors. In these formulas the mean value of the assignment operator is used, and consequently, their reliability depends on the arithmetic used.'),
(875,'A binary representation of the rationals derived from their continued fraction expansions is described and analysed. The concepts \"adjacency\", \"mediant\" and \"convergent\" from the literature on Farey fractions and continued fractions are suitably extended to provide a foundation for this new binary representation system. Worst case representation-induced precision loss for any real number by a fixed length representable number of the system is shown to be at most 19% of bit word length, with no precision loss whatsoever induced in the representation of any reasonably sized rational number. The representation is supported by a computer arithmetic system implementing exact rational and approximate real computations in an on-line fashion.'),
(876,'In this paper we investigate an extension to Vuillemin\'s work on continued fraction arithmetic [Vuillemin 87, Vuillemin 88, Vuillemin 90], that permits it to evaluate the standard statistical distribution functions. By this we mean: the normal distribution, the <img src=\"exact_statistics_and_continued/images/337c108b.gif\" HEIGHT=13 WIDTH=12>-distribution, the <I>t</I>-distribution, and, in particular, the <I>F</I>-distribution. The underlying representation of non-rational computable real numbers is also as continued fractions, in the style of Vuillemin. This permits arbitrary accuracy over a range of values. The number of terms of a continued fraction that are used by the implementation is dynamically controlled by the accuracy demanded of the final answer. The use of a modern lazy functional language - Haskell - has considerably eased the programming task. Two features are of note. Firstly, the <I>type-class</I> structure allows one to augment the varieties of numbers supported by the language. Secondly, the <I>laziness</I> inherent in the Haskell\'s semantics, makes it very straightforward to dynamically control the accuracy of the intermediate evaluations.'),
(877,'The result of a simple floating-point computation can be in great error, even though no error is signaled, no coding mistakes are in the program, and the computer hardware is functioning correctly. This paper proposes a set of instructions appropriate for a general purpose microprocessor that can be used to improve the credibility and accuracy of numerical computations. Such instructions provide direct hardware support for monitoring events which may threaten computational integrity, implementing floating-point data types of arbitrary precision, and repeating calculations with greater precision. These useful features are obtained by the efficient implementation of high radix on-line arithmetic. The prevalence of super-scalar and VLIW processors makes this approach especially attractive.'),
(878,'We discuss two closely related interval arithmetic systems: i) the of directed (generalized) intervals studied by E. Kaucher, and ii) the syste intervals together with the outer and inner interval operations. A relation two systems becomes feasible due to introduction of special notations and a normal form of directed intervals. As an application, it has been shown that interval systems can be used for the computation of tight inner and outer in of ranges of functions and consequently for the development of software for computation of ranges of functions.'),
(879,'We develop a formal account of digit serial number representations by describing them as strings from a language. A prefix of a string represents an int erval approximating a number by enclosure. Standard on-line representations are shown to be a special case of the general digit serial representations. Matrices are introd uced as representations of intervals and a finite-state transducer is used for mapping str ings into intervals. Homographic and bi-homographic functions are used for representing basi c arithmetic operations on digit serial numbers, and finally a digit serial represen tation of floating point numbers is introduced.      <hr align=\"left\" width=\"50%\"><br>      1.) This work has been supported by The Danish Research Councils under the grant no.5.21.08.02.'),
(880,'Three algorithms providing rigourous bounds for the eigenvalues of a real matrix are presented. The first is an implementation of the bisection algorithm for a symmetric tridiagonal matrix using IEEE floating-point arithmetic. The two others use interval arithmetic with directed rounding and are deduced from the Jacobi method for a symmetric matrix and the Jacobi-like method of Eberlein for an unsymmetric matrix.'),
(881,'IEEE floating-point arithmetic standards 754 and 854 reflect the present state of the art in designing and implementing floating-point arithmetic units. A formalism applied to a standard non-trapping mode floating-point system shows incorrectness of some numeric and non-numeric results. A software emulation of decimal floating-point computer arithmetic supporting an enhanced set of exception symbols is reported. Some implementation details, discussion of some open questions about utility and consistency of the implemented arithmetic with the IEEE Standards are provided. The potential benefit for computations with infinite symbolic elements is outlined.'),
(882,''),
(883,'When driven by simple models of information processing, reading instruction focuses on basic decoding skills centering on words and sentences. Factoring in advanced cognitive studies adds at least two more dimensions. First, readers must learn a collection of strategies for constructing meaning from text. Second, and most importantly, readers must develop enough situational awareness to diagnose a text and know which strategy to deploy. Teaching intellectual crafts that involve not only base-line performative skills but also a repertoire of problem-solving heuristics, and the metacognitive maturity to orchestrate multi-leveled activities, works well in a master-apprentice model. However, one-on-one instruction is far too labor-intensive to be commonplace in the teaching of reading. This paper describes a computerized learning environment for teaching the conceptual patterns of critical literacy. While the full implementation of the software treats both reading and writing, this paper covers only the reading aspects of R-WISE (Reading and Writing in a Supportive Environment).'),
(884,'A new range reduction algorithm, called ModularRange Reduction (MRR), briefly introduced by the authors in [<A HREF=\"Daumas_M.html#0x811b9908_0x003b55ed\">Daumas et al. 1994</A>]   is deeply analyzed. It is used to reduce the arguments to exponential and trigonometric function algorithms to be within the small range for which the algorithms are valid. MRR reduces the arguments quickly and accurately. A fast hardwired implementation of MRR operates in time <img src=\"modular_range_reduction/images/2f76c369.gif\">(log(n)), where n is the number of bits of the binary input value. For example, with MRR it becomes possible to compute the sine and cosine of a very large number accurately. Web<A HREF=\"http://www.jucs.org/jucs_articles_by_category/b.2\"> pr</A>op<A HREF=\"http://www.jucs.org/jucs_articles_by_category/g.1.0\">ose t</A>wo possible architectures implementing this algorithm.'),
(885,'This surveys algorithms and circuits for integer division in special cases. These include division by constants, small divisors, exact divisors, and cases where the divisor and the number base have a special relationship. The related operation of remainder is also covered. Various prior techniques are treated in a common framework. Worked examples are provided together with examples of practical application.'),
(886,'Intelligent Tutoring Systems (ITS) have proven to be effective tools for teaching and training. However, ITSs have not become common in industrial and organisational settings, in part because their complexity has proven difficult to manage outside of the research lab. Minimalist ITSs are an attempt to bridge the gap between research and practical application, they simplify research techniques while striving to maintain as much pedagogic intelligence as possible. This paper describes one such system, SWIFT, that is an example of how a minimalist ITS can be delivered as a commercial product. We outline some of the issues facing designers of a minimalist system, and describe the ways that research techniques have been incorporated into four modules of SWIFT: adaptive testing, course planning, guidance, and diagnosis.'),
(887,'The classical halting probability <img src=\"imagesprobability_amplitude_of/images/2f659787.gif\">  introduced by Chaitin is generalized to quantum computations. (The quantum omega was invented in a meeting of G. Chaitin, A. Zeilinger and the author (K. S.) in a Viennese coffee house (Cafe Braeunerhof) in January 1991. Thus, the group should be credited for the original invention, whereas any blame should remain with the author.)'),
(888,''),
(889,'We investigate the generative capacity of the so-called conditional tabled eco-grammar systems (CTEG). They are a variant of ecogrammar systems, generative mechanisms recently introduced as models of the interplay between environment and agents in eco-systems. In particular, we compare the power of CTEG systems with that of programmed and of random context T0L systems and with that of ET0L systems. CTEG systems with one agent only (and without extended symbols) are found to be surprisingly powerful (they can generate non-ET0L languages). Representation theorems for ET0L and for recursively enumerable languages in terms of CTEG languages are also presented.      <hr align=\"left\" width=\"50%\"><br>      1.) Research supported by the Academy of Finland, Project 11281'),
(890,'In this paper, we present HOME, a new environment for distributed hypermedia. We mainly concentrate on the server side, and provide access to World-Wide Web clients through a gateway mechanism. Data and metadata are strictly separated in the distributed HOME server. The architecture is based on a layered approach with separate layers for raw data, multimedia characteristics and hypermedia structure. We briefly present some of the implementation aspects and emphasise distinctive characteristics of HOME. We conclude with a comparison with related research and our plans for the future.'),
(891,'Many fuzzy automaton models have been introduced in the past. Here, we discuss two basic finite fuzzy automaton models, the Mealy and Moore types, for lexical analysis. We show that there is a remarkable difference between the two types. We consider that the latter is a suitable model for implementing lexical analysers. Various properties of fuzzy regular languages are reviewed and studied. A fuzzy lexical analyzer generator (FLEX) is proposed.      <hr align=\"left\" width=\"50%\"><br>      1.) The work reported here has been supported by the Natural Sciences and Engineering Research Council of Canada grants OGP0041630 and the Project 11281 of the Academy of Finland.'),
(892,'The attempt by Unisys to obtain royalties from the Lempel Zev Welch Graphics Interchange Format specification through Compuserve has wide implications for the Internet. Increased activity in the US software patents area is likely to result in damage to progress of the software arts and the Internet, and to generate upscaled protest from Internet users. The LZW GIF case highlights the Internet culture in favour of free and unfettered development. Clarification of this important principle will have a major effect on the future of the Internet.'),
(893,'We show that some widely accepted criteria for cryptographic functions, including the strict avalanche criterion (SAC) and the propagation criterion, have various limitations in capturing properties of vital importance to cryptographic algorithms, and propose a new criterion called GAC to measure the global avalanche characteristics of cryptographic functions. We also introduce two indicators related to the new criterion, one forecasts the sum-of-squares while the other the absolute avalanche characterist- ics of a function. Lower and upper bounds on the two indicators are derived, and two methods are presented to construct cryptographic functions that achieve nearly optimal global avalanche characteristics.'),
(894,''),
(895,'A translation of the pi-calculus into the MONSTR graph rewriting language is described and proved correct. The translation illustrates the heavy cost in practice of faithfully implementing the communication primitive of the pi-calculus and similar process calculi. It also illustrates the convenience of representing an evolving network of communicating agents directly within a graph manipulation formalism, both because the necessity to use delicate notions of bound variables and of scopes is avoided, and also because the standard model of graphs in set theory automatically yields a useful semantics for the process calculus. The correctness proof illustrates many features typically encountered in reasoning about graph rewriting systems, and particularly how serialisation techniques can be used to reorder an arbitrary execution into one having stated desirable properties.'),
(896,'Changing relative performance of processors, networks, and disks makes it necessary to reconsider algorithms using these three resources. As networks get faster and less congested topologies emerge, it becomes important to use network resources more aggressively to obtain good performance. Substitution of local disk accesses by accesses to remote memory can lead to better balanced resource usage and thus to faster systems. In this work we address the issue of file caching in a networked file system configuration. Distributed block-level in-memory caches are considered. We show that carefully constructed distributed concepts can lead to lower server load and better overall system performance than centralized concepts. Oversimplification, although aimed at gaining performance for single components, may deteriorate overall performance as a result of unbalanced resource usage.'),
(897,'Much of the confusion that surrounds electronic personal assistants arises from the open-ended complexity of their development. In this paper we categorise some of their more common uses before suggesting several thought-provoking extensions.'),
(898,'We present two examples of microworlds built into the Smalltalk environment for the purpose of teaching the main concepts of object oriented programming (OOP) and of the Smalltalk programming language. Thee distinguishing features of our microworlds are that each of them presents the student with a sequence of environments. These environments introduce one OOP concept after another, and disclose the Smalltalk environment and language in a step-by-step fashion. The starting environment does not require any programming and does not encourage the user to use Smalltalk tools, the last environment must be programmed in Smalltalk and discloses the major Smalltalk tools. The intended use of our microworlds is for the introductory part of a course on OOP, to be followed by a detailed presentation of the language. An extension of the presented approach would make the method suitable for teaching basics of computer programming in a computer literacy course.'),
(899,''),
(900,'We introduce Wang cubes with colored faces that are a generalization of Wang tiles with colored edges. We show that there exists an aperiodic set of 21 Wang cubes, that is, a set for which there exists a tiling of the whole space with matching unit cubes but there exists no periodic tiling. We use the aperiodic set of 13 Wang tiles recently obtained by the first author using the new method developed by the second. Our method can be used to construct an aperiodic set of <I>n</I>-dimensional cubes for any <I>n</I> <img src=\"an_aperiodic_set_of/images/3371e2fb.gif\"> 3.'),
(901,'We propose a new hypermedia data model, called CHM for Contained HyperMedia. Our model is based on set-oriented data structuring, with a strong emphasis on automatic maintenance of link integrity. In this paper, the CHM model is presented in detail: both data structuring, navigational facilities and authoring support are presented. We will also explain how we have integrated support for the CHM model in Home, our Hypermedia Object Management Environment, publicly accessible through the World-Wide Web.'),
(902,'We report about a new way of producing hypermedia documents for supporting teaching at universities. A computer held lecture is automatically converted into the core of a multimedia document and linked together with papers, textbooks, animations and simulations. As an electronic substitute of the blackboard we have used the whiteboard wb of the Mbone toolset and have transmitted the lecture also to remote locations. Our experiments demonstrate that classroom lecturing, distance teaching, and the production of educational hypermedia can be successfully integrated.'),
(903,''),
(904,'A new internal structure for simple polygons, the  straight skeleton, is introduced and discussed. It is composed of pieces of angular bisectores which partition the interior of a given n-gon P in a tree-like fashion into n monotone polygons. Its straight-line structure and its lower combinatorial complexity may make the straight skeleton preferable to the widely used medial axis of a polygon. As a seemingly unrelated application, the straight skeleton provides a canonical way of constructing a polygonal roof above a general layout of ground walls.'),
(905,'We propose constraints as the appropriate computational constructs for the design of agents with the task of selecting, merging and managing electronic information coming from such services as Internet access, digital libraries, E-mail, or on-line information repositories. Specifically, we introduce the framework of Constraint-Based Knowledge Brokers, which are concurrent agents that use so-called signed feature constraints to represent partially specified information and can flexibly cooperate in the management of distributed knowledge. We illustrate our approach by several examples, and we define application scenarios based on related technology such as Telescript and workflow management systems.'),
(906,'An <i>n</i>-dimensional vector of natural numbers is said to be prime if the greatest common divisor of its components is one. A word is said to be Parikh prime if its Parikh vector is prime. The languages of Parikh prime and of Parikh non-prime words are investigated (they are neither semilinear nor slender, hence are not context-free or D0L languages, both of them can be generated by matrix grammars with appearance checking). Marking in the plane the points identified by prime (2-dimensional) vectors, interesting patterns of non-marked (\"free\") points appear (they are similar to the territories in the game of GO). The shape of such possible territories is investigated (with an exhaustive analysis of tro-, tetro-, pento- and hexominoes). Some open problems are formulated (both concerning the mentioned languages and the \"GO territories theory\").      <hr align=\"left\" width=\"50%\"><br>      1.) Research supported by the Academy of Finland, project 11281, and the ESPRIT Basic Research Working Group ASMICS II.'),
(907,'Constraint satisfaction is very common in many artificial intelligence applications. This paper presents results from parallelizing constraint satisfaction in a special application --- the algorithm for qualitative simulation QSim [Kuipers 94]. A parallel-agent based strategy (PAB) is used to solve the constraint satisfaction problem (CSP). Two essential steps of PAB are studied in more detail to achieve a good performance of the parallel algorithm. Partitioning heuristics to generate independent parts of the overall search space are investigated. Sequential CSP algorithms are compared in order to reveal the most efficient one for QSim. The evaluation of these heuristics and algorithms is based on runtime measurements using CSPs traced from QSim. These runtimes allow a best- and worst-case estimation of the expected speedup of the parallel algorithms. The comparison of sequential CSP algorithms leads to following strategy for solving partitioned problems. Less complex problems are solved with simple backtracking, and more complex models are solved with graph-directed backjumping (GBJ).'),
(908,'We describe a Markov process which models the sequential allocation for two adjacent tables coexisting in memory by growing towards each other. The tables are expected to fill at the same rate, random deletions and insertions are allowed.      <hr align=\"left\" width=\"50%\"><br>      1.) 1991 Mathematics Subject Classification. Primary 60J20, Secondary 62M05, 68P05.'),
(909,''),
(910,'In this paper we make a first attempt at systematically investigating levels of anonymity required in networked computer systems: we feel it is often overlooked that beyond such obvious cases as identified by means of a password  or  anonymous use  there are many other levels of anonymity, identification and authenticity necessary in various applications.'),
(911,'Chaitin s algorithmic definition of random strings - based on the complexity induced by self-delimiting computers - is critically discussed. One shows that Chaitin s model satisfy many natural requirements related to randomness, so it can be considered as an adequate model for finite random objects. It is a better model than the original (Kolmogorov) proposal. Finally, some open problems will be discussed.'),
(912,'High-radix division, developing several quotient bits per clock, is usually limited by the difficulty of generating accurate high-radix quotient digits. This paper describes techniques which allow quotient digits to be inaccurate, but then refine the result. We thereby obtain dividers with slightly reduced performance, but with much simplified logic. For example, a nominal radix-64 divider can generate an average of 4.5 to 5.5 quotient bits per cycle with quite simple digit estimation logic. The paper investigates the technique for radices of 8, 16, 64 and 256, including various qualities of digit estimation, and operation with restricted sets of divisor multiples.'),
(913,'We show how to implement an <img src=\"on_implementing_erew_work/images/2f1ea70e.gif\">-processor EREW PRAM workoptimally on a 2-dimensional n-sided mesh of trees, consisting of n processors, n memory modules, and <img src=\"on_implementing_erew_work/images/2f1ea754.gif\">nodes. Similarly, we prove that an <img src=\"on_implementing_erew_work/images/2f1ea776.gif\">-processor EREW PRAM can be implemented work-optimally on a 3-dimensional n-sided mesh of trees. By the work-optimality of implementations we mean that the expected routing time of PRAM memory requests is <img src=\"on_implementing_erew_work/images/2f1ea7cb.gif\"> per simulated PRAM processor with high probability. Experiments show that on relatively small<img src=\"on_implementing_erew_work/images/2f1ea7ea.gif\"> and <img src=\"on_implementing_erew_work/images/2f1ea7fb.gif\">the cost per simulated PRAM processor is 1.5-2.5 in the 2-dimensional case, and 2-3 in the 3-dimensional case. If at each step at most 1/3\'th of the PRAM processors make a reference to the shared memory, then the simulation cost is approximately 1. We also compare our work-optimal simulations to those proposed for coated meshes.'),
(914,'We consider generative mechanisms producing languages by starting from a finite set of words and shuffling the current words with words in given sets, depending on certain conditions. Namely, regular and finite sets are given for controlling the shuffling: strings are shuffled only to strings in associated sets. Six classes of such grammars are considered, with the shuffling being done on a left most position, on a prefix, arbitrarily, globally, in parallel, or using a maximal selector. Most of the corresponding six families of languages, obtained for finite, respectively for regular selection, are found to be incomparable. The relations of these families with Chomsky language families are briefly investigated.'),
(915,''),
(916,'This paper describes a work-in-progress: a computerized learning environment for teaching the conceptual patterns of scientific reasoning. BROCA (Basic Research, Observations, Critical Analysis) is theory-driven, combining two very powerful conceptual models of thinking. The first -- drawn from cognitive psychology and information theory -- focuses on the mental manipulations by which data becomes information and information becomes knowledge. The second theoretical construct comes from rhetoric and describes the intellectual activities carried out in prewriting, drafting, and revision by an expert writing. As an interactive \"cognitive tool,\" BROCA provides scaffolding (through visual algorithms and adaptive prompting) to help a fledgling thinker practice the robust patterns of scientific reasoning.'),
(917,'We describe a novel text compressor which combines Ziv-Lempel compression and arithmetic coding with a form of vector quantisation. The resulting compressor resembles an LZ-77 compressor, but with no explicit phrase lengths or coding for literans. An examination of the limitations on its performance leads to some predictions of the limits of LZ-77 compression in general, showing that the LZ-77 text compression technique is already very close to the limits of its performance.'),
(918,'We describe new methods for the estimation of the bounds of the coefficients of proper divisors of integer polynomials in one variable. There exist classes of poly-nomials for which our estimates are better than those obtained using the polynomial measure or the 2-weighted norm.'),
(919,'We describe a robust method for spatial registration, which relies on the coarse correspondence of structures extracted from images, avoiding the establishment of point correspondences. These structures (tokens) are points, chains, polygons and regions at the level of intermediate symbolic representation (ISR). The algorithm recovers conformal transformations (4 affine parameters), so that 2-dimensional scenes as well as planar structures in 3D scenes can be handled. The affine transformation between two different tokensets is found by minimization of an exponentially decreasing distance function. As long as the tokensets are kept sparse, the method is very robust against a broad variety of common disturbances (e.g. incomplete segmentations, missing tokens, partial overlap). The performance of the algorithm is demonstrated using simple 2D shapes, medical, and remote sensing satellite images. The complexity of the algorithm is quadratic on the number of affine parameters.'),
(920,''),
(921,'In this paper we claim that the navigational and structural tools currently available on the Internet are not sufficient to fully exploit the tremendous power of the largest information and communication ressource mankind has ever had. We contend that current hypermedia systems and its most prominent specimen WWW do not have enough functionality to provide the power that is needed. We explain important features that are absent, claim that \"second generation\" hypermedia systems incorporating such features are essential and mention a first such second generation hypermedia system called Hyper-G, which is just becoming available and is starting to be used for a wide variety of applications.'),
(922,''),
(923,'\"Hypermedia\" is a term that is widely misunderstood and misused, often loosely associated with hype about new applications of computers to multimedia. The term \"multimedia\" is also frequently used with too narrow a meaning. Few people appreciate the likely consequences of the new technology in all its ramifications. This paper is an attempt to show what impact the new technology will have on many aspects of life. One thing is certain: the impact will be tremendous and irresistible. Before long, organisations won\'t be able to afford not to use hypermedia.<p>We believe that the definition of \"multimedia\" must be generalised from the usual one (a mix of text, pictures, graphics, animations, video and sound) to include 3D objects, 3D models of scenes of arbitrary complexity, interactive movies, diagrams, maps, CAD drawings and much more. A hypermedia system can be defined as multimedia, with links, embedded in a network, i.e. a networked system supporting the storage and retrieval of linked multimedia and the real-time transfer of this data among the terminals in the network.<p>It can be argued that hypermedia systems can revolutionise work, leisure, and lifelong learning.<p><br><p>Applications of hypermedia discussed in this paper include:<p><ul> <li>Administration: A fully integrated system such as the one proposed will mean efficient data processing and valuable statistical data. <li>Electronic orientation and information displays: Electronic guided tours, public information kiosks and publicity dissemination with archive facilities. <li>Electronic Personal Assistants: Hypermedia systems will turn into powerful personal digital assistants as they make information and communication available when needed. <li>General Information and Communication Systems: Distributed information systems for purposes such as businesses, schools and universities, museums, libraries, health systems.... <li>Lecturing: A system going beyond the traditional to empowerboth teachers and learners. <li>Libraries: A further step towards fully electronic library systems. <li>Directories of all kinds: Staff, telephone and all sorts of generic directories. <li>Research: Material can now be accessed from databases all around the world. The effects of networking and computer supported collaborative work are discussed and examples of new scientific visualisation programs are quoted. </ul><p>The paper concludes with a section entitled [<A HREF=\"applications_and_impact_of/Lennon_J.html#0x811b9908_0x003b79cd\">Future Directions</A>].'),
(924,'In the first part of this paper we argue that the terms \"multimedia\" and \"hypermedia\" need redefining to reflect latest developments in converging technology. We propose new forms of \"interactive\" and \"annotated\" movies, to be created using advanced digital techniques. In the second half of the paper we suggest that some of the problems that users of Internet are currently experiencing are due to \"first generation\" systems, but that \"second generation\" answers have already emerged.'),
(925,'In this paper we first discuss briefly why electronic journals today have a rather moderate success. We then describe J.UCS - the Journal of Universal Computer Science - an electronic journal that is the prototype for electronic publishing in the future. Using Hyper-G for distribution it provides all search and navigation mechanisms of large scale hypermedia systems and therefore makes it easy to locate interesting articles. Readers can perform variable scope searches to find papers, then they can browse them on screen either in hypertext mode or in high quality PostScript mode, or they can get high quality PostScript documents for printing. Even PostScript documents provide full hyperlink support when reading them on screen. Articles in J.UCS can be accessed very fast using a wide net of servers distributed all over the world. J.UCS also supports annotations to existing articles informing the readers of new research results or errors. Writing articles for J.UCS is very easy using PostScript as the main submission format, even standard hyperlinks such as literature references are generated automatically. We close this paper with a short comparison of J.UCS to other electronic journals and with an outlook on future developments.'),
(926,''),
(927,''),
(928,'Advances in technology now make it possible to integrate hundreds of cores (e.g. general or special purpose processors, embedded memories, application specific components, mixed-signal I/O cores) in a single silicon die. The large number of resources that have to communicate makes the use of interconnection systems based on shared buses inefficient. One way to solve the problem of on-chip communications is to use a Network-on-Chip (NoC)-based communication infrastructure. Such interconnection systems offer new degrees of freedom, exploration of which may reveal significant optimization possibilities: the possibility of arranging the computing and storage resources in an NoC, for example, has a great impact on various performance indexes.  The paper addresses the problem of topological mapping of intellectual properties (IPs) on the tiles of a mesh-based NoC architecture. The aim is to obtain the Pareto mappings that maximize performance and minimize power dissipation. We propose a heuristic technique based on evolutionary computing to obtain an optimal approximation of the Pareto-optimal front in an efficient and accurate way. At the same time, two of the most widely-known approaches to mapping in meshbased NoC architectures are extended in order to explore the mapping space in a multi-criteria mode. The approaches are then evaluated and compared, in terms of both accuracy and efficiency, on a platform based on an event-driven trace-based simulator which makes it possible to take account of important dynamic effects that have a great impact on mapping. The evaluation performed on both synthesized traffic and real applications (an MPEG-4 codec) confirms the efficiency, accuracy and scalability of the proposed approach.'),
(929,'In this paper, we propose a methodology based on genetic programming to automatically generate hardware designs of substitution boxes necessary for many cryptosystems such as DES encryption system. We aim at evolving minimal hardware specifications, which minimise both space (i.e. required gate number), response time (i.e. encryption and decryption time) and dissipated power. We compare our results against existing and well-known designs, which were produced by human designers using conventional methods.'),
(930,'This paper presents three variants of Genetic Programming (GP) approaches for intelligent online performance monitoring of electronic circuits and systems. Reliability modeling of electronic circuits can be best performed by the stressor  susceptibility interaction model. A circuit or a system is considered to be failed once the stressor has exceeded the susceptibility limits. For on-line prediction, validated stressor vectors may be obtained by direct measurements or sensors, which after pre-processing and standardization are fed into the GP models. Empirical results are compared with artificial neural networks trained using backpropagation algorithm and classification and regression trees. The performance of the proposed method is evaluated by comparing the experiment results with the actual failure model values. The developed model reveals that GP could play an important role for future fault monitoring systems.'),
(931,'The paper concerns the design of evolutionary   algorithms and pattern search methods on two circuit design   problems: the multi-objective optimization of an Operational   Transconductance Amplifier and of a fifth-order leapfrog filter. The   experimental results obtained show that evolutionary algorithms are   more robust and effective in terms of the quality of the solutions   and computational effort than classical methods. In particular, the   observed Pareto fronts determined by evolutionary algorithms has a   better spread of solutions with a larger number of nondominated   solutions when compared to the classical multi-objective   techniques.'),
(932,'This work analyses two heuristic algorithms based on the genetic evolution theory applied to direct sequence code division multiple access (DS/CDMA) com munication systems. For different phases of an evolutionary algorithm new biological processes are analyzed, specially adapted to the multiuser detection (MuD) problem in multipath fading channels. Monte Carlo simulation results show that the detection based on evolutionary heuristic algorithms is a viable option when compared with the optimum solution (ML  maximum likelihood), even for hostile channel conditions and severe system operation. Additionally, a comparative table is presented considering the relation between bit error rate (BER) and complexity as the main analyzed figure of merit. Each algorithm complexity is determined and compared with others based on the required number of computational operations to reach de optimum performance and also the spent computational time.'),
(933,''),
(934,''),
(935,'In this paper we describe a technique for   monitoring and checking temporal logic assertions augmented with   real-time and time-series constraints, or Metric Temporal Logic   Series (MTLS). The method is based on Remote Execution and   Monitoring (REM) of temporal logic assertions. We describe the   syntax and semantics of MTLS and a monitoring technique based on   alternating finite automata that is efficient for a large set of   frequently used formulae and is also an on-line technique. We   investigate the run-time data-structure size for several interesting   assertions taken from the Kansas State specification   patterns.'),
(936,'Karatsuba discovered the first algorithm that accomplishes multiprecision integer multiplication with complexity below that of the grade-school method. This algorithm is implemented nowadays in computer algebra systems using irreversible logic. In this paper we describe reversible circuits for the Karatsuba\'s algorithm and analyze their computational complexity. We discuss garbage disposal methods and compare with the well known Bennett\'s schemes. These circuits can be used in reversible computers which have the advantage of being very efficient in terms of energy consumption. The algorithm can also be used in quantum computers and is an improvement of previous circuits for the same purpose described in the literature.'),
(937,'In this paper, we remind previous results about   the tilings {<I>p,q</I>} of the hyperbolic plane. As   proved in [Margenstern and Skordev 2003a], these tilings are   combinatoric, a notion which we recall in the introduction. It   turned out that in this case, most of these tilings also have the   interesting property that the language of the splitting associated   to the tiling is regular. In this paper, we investigate the   consequence of the regularity of the language by providing   algorithms to compute the path from a tile to the root of the   spanning tree as well as to compute the coordinates of the   neighbouring tiles. These algorithms are linear in the coordinate of   the given node.'),
(938,'This paper surveys some of the work that was   inspired by Wagner\'s general technique to prove completeness in the   levels of the boolean hierarchy over NP and some related results. In   particular, we show that it is DP-complete to decide whether or not   a given graph can be colored with exactly four colors, where DP is   the second level of the boolean hierarchy. This result solves a   question raised by Wagner in 1987, and its proof uses a clever   reduction due to Guruswami and Khanna. Another result covered is due   to Cai and Meyer: The graph minimal uncolorability problem is also   DP-complete. Finally, similar results on various versions of the   exact domatic number problem are discussed.'),
(939,'Much development work is ongoing addressing technologies and their application in the health domain, in order to achieve solutions that are non-invasive to every day life and work. As with many previous phases of informatics to support health, currently the developments are in islands and there is considerable untapped potential for synergy. Much research development is happening in other domains and show potential for health reversioning and deployment once proven. This paper explores some of the technological, societal and domain-specific issues surrounding this emerging concept of pervasiveness. It concludes that pervasive support to care is emerging but further work on minimizing risk and marketing the concept to professionals and laypeople is necessary to ensure an effective deployment.'),
(940,'To many people, home is a sanctuary. For those people who need special medical care, they may need to be pulled out of their home to meet their medical needs. As the population ages, the percentage of people in this group is increasing and the effects are expensive as well as unsatisfying. We hypothesize that many people with disabilities can lead independent lives in their own homes with the aid of at-home automated assistance and health monitoring. In order to accomplish this, robust methods must be developed to collect relevant data and process it dynamically and adaptively to detect and/or predict threatening long-term trends or immediate crises.  The main objective of this paper is to investigate techniques for using agent-based smart home technologies to provide this at-home health monitoring and assistance. To this end, we have developed novel inhabitant modeling and automation algorithms that provide remote health monitoring for caregivers. Specifically, we address the following technological challenges: 1) identifying lifestyle trends, 2) detecting anomalies in current data, and 3) designing a reminder assistance system. Our solution approaches are being tested in simulation and with volunteers at the UTA\'s MavHome site, an agent-based smart home project.'),
(941,'More than 60,000 people die suddenly each year in France due to cardiac arrhythmias. The current techniques used to diagnose cardiac arrhythmias such as HOLTER, R.TEST and telemetry system are partially efficient owing to the limitation of the duration of monitoring. This paper presents a new system dedicated to real-time cardiac arrhythmias tele-assistance and monitoring. This system is generally composed of 4 main configurable elements: wireless ECG sensor, local access unit, remote centre server, and remote surveillance terminal. The main technical challenges of this system include three aspects: a real-time automatic ECG diagnostic algorithm, an embedded real-time multi-task operating system, and a real-time reliable telemedicine communication protocol. This paper gives our solutions to these problems and specifies the technical details. Currently, this system has been evaluated on thirty patients at the CHRU of Gabriel Montpied hospital (Clermont-Ferrand, France) and also been used to test the athletes\' cardiac status during the physical exercises. The performance results show that this system meets fully the requirements of real-time cardiac monitoring and diagnosing application and can be used as a long-term cardiac healthcare equipment.'),
(942,'Telemedicine applications on a medical practitioner\'s mobile device should be context-aware. This can vastly improve the effectiveness of mobile applications and is a step towards realising the vision of a ubiquitous telemedicine environment. The nomadic nature of a medical practitioner emphasises location, activity and time as key context-aware elements. An intelligent middleware is needed to effectively interpret and exploit these contextual elements. This paper proposes an agent-based architectural solution called Context-Aware Mobile Medical Devices (CAMMD). This framework can proactively communicate patient records to a portable device based upon the active context of its medical practitioner. An expert system is utilised to cross-reference the context-aware data of location and time against a practitioner\'s work schedule. This proactive distribution of medical data enhances the usability and portability of mobile medical devices.  The proposed methodology alleviates constraints on memory storage and enhances user interaction with the handheld device. The framework also improves utilisation of network bandwidth resources. An experimental prototype is presented highlighting the potential of this approach.'),
(943,'The lack of appropriate and accurate information on the ability of a frail individual to accomplish specific task oriented activities can place the individual at risk or result in the allocation of costly and unnecessary care. Although there have been previous attempts to use computer technology to obtain this information, they have proved to be costly and complex and therefore not widely used. However, a behavioral monitoring technology, based on smart-home and telemedicine applications, has been developed that obtains more accurate and timely information on the ability of frail individuals to accomplish specific tasks in their own residences than any other existing method. During a twelve month pilot study, this system has been used by care providers to assess the status of their clients, respond to immediate needs and alter overall care plans, thus resulting in better care and greater peace of mind for the individual.'),
(944,'The convergence of need between improved clinical care and post genomics research presents a unique challenge to restructuring information flow so that it benefits both without compromising patient safety or confidentiality. The CLEF project aims to link-up heath care with bioinformatics to build a collaborative research platform that enables a more effective biomedical research. In that, it addresses various barriers and issues, including privacy both by policy and by technical means, towards establishing its eventual system. It makes extensive use of language technology for information extraction and presentation, and its shared repository is based around coherent \"chronicles\" of patients\' histories that go beyond traditional health record structure. It makes use of a collaborative research workbench that encompasses several technologies and uses many tools providing a rich platform for clinical researcher.'),
(945,'This paper proposes a Human-centered Pervasive Computing  System Model (HPC), a Layered Architectural Analysis and Design Method  (LAAD) and a Waterfall Prototyping Process Model (WPP). Based on the HPC  model and the LAAD method, a pervasive computing based multimodal tele-home  healthcare system is designed and partly implemented using the Waterfall  Prototyping Process. The design and implementation issues are discussed  in more detail. Some testing results are presented.'),
(946,''),
(947,'Individuals vary in survival chances due to   differences in genetics, environmental exposures, and   gene-environment interactions. These chances, as well as the   contribution of each factor to mortality, change as individuals get   older. In general, human physiological systems are constructed by   collecting more than one part to perform either single or multiple   functions. In addition, the successive times between failures are   not necessarily identically distributed. More generally, they can   become smaller (an indication of deterioration). However, if any   critical deterioration is detected, then the decision of when to   take the intervention, given the costs of diagnosis and   therapeutics, is of fundamental importance. At the time of the   decision, the degree of future physiological system deterioration,   which is likely to be uncertain, is of primary interest for the   decision maker. This paper develops a possible Web-based decision   support system by considering the sensitivity analysis as well as   the optimal prior and posterior decisions for aging chronic   diseases. The proposed design of Bayesian decision support systems   facilitates the effective use of the computing capability of   computers and provides a systematic way to integrate the expert\'s   opinions and the sampling information which will furnish decision   makers with valuable support for quality decision making.'),
(948,'Privacy is a complex social process that will persist in one form or another as a fundamental feature of the substrate into which ubiquitous computing (ubicomp) is threaded. Hospitals are natural candidates for the deployment of ubicomp technology while at the same time face significant privacy requirements. To better understand the privacy issues related to the use of ubicomp we place our efforts in understanding the contextual information relevant to privacy and how its interplay shapes the perception of privacy in a hospital. The results indicate that hospital workers tend to manage privacy by assessing the value of the services provided by a ubicomp application and the amount of privacy they are willing to concede. For ubicomp applications to better deal with this issue we introduce the concept of Quality of Privacy (QoP) which allows balancing this trade-off in a similar way as that of Quality of Service (QoS) does for networking applications. We propose an architecture that allows designers to identify different levels of QoP based on the user\'s context. Finally, we identify the main privacy risks of a location-aware application and we extend its architecture exemplifying the use of QoP to manage those risks.'),
(949,'Ambient Intelligent (AmI) vision is a new concept materialized by the Six Framework Program of the European Community. It involves three key technologies: Ubiquitous Computing, Ubiquitous Communications and Natural Interfaces. With the increase in context aware applications it is important to keep these technologies in mind. In this paper we present a context aware application in a conference site based on the identification process using RFID. Furthermore the highlights of this proposal are based on the \"ws\" concepts. Three environments are modelled applying the \"who\" to the \"when\" and \"where\" to reach the \"what\". In this sense certain services are offered to the conference attendees, some of which are characteristics of this technology and others are the result of a context aware application, the visualization services named \"Mosaics of Information\".'),
(950,'<I>Ambient Intelligence</I>   scenarios describe situations in which multitude of devices and   agents live together. In this kind of scenarios is frequent to see   the appearance of conflicts when modifying the state of a device as   for example a lamp. Those problems are not as much of sharing of   resources as of conflict of orders coming from different   agents. This coexistence must deal also with the desire of privacy   of the different users over their personal information such as where   they are, what their preferences are or to whom this information   should be available.  When facing incompatible orders over the state   of a device it turns necessary to make a decision. In this paper we   propose a centralised mechanism based on prioritized FIFO queues to   decide the order in which the control of a device is granted. The   priority of the commands is calculated following a policy that   considers issues such as the commander\'s role, command\'s type,   context\'s state and commander-context and commander-resource   relations.  Finally we propose a set of particular policies for   those resources that do not adjust to the general policy. In   addition we present a model pretending to integrate privacy through   limiting and protecting contextual information.'),
(951,'An interesting new application domain for handheld devices may be represented by Ambient Intelligence (AmI), where they can be used as intermediaries between us and our surrounding environment. Thus, the devices, which always accompany us, will behave as electronic butlers who assist us in our daily tasks, by interacting with the smart objects (everyday objects augmented with computational services) in our whereabouts. In order to achieve such goal, this paper proposes an AmI-enabling framework providing two main functions: a) facilitate the development and deployment of smart objects and b) transform mobile devices into universal remote controllers of those smart objects.'),
(952,'Lately, wireless networks have gained acceptance for home networking. Low cost installation, flexibility and no fixed infrastructures have made it possible home environments rapidly to adopt this technology. In this paper we introduce the use of mobile ad-hoc networks (MANETs) for large in-home environments, such as hospitals, government buildings, office and industrial buildings, etc. Thus, we define an information gathering mechanism in order to provide a context aware QoS framework, relaxing some restrictions that are inherited from traditional ad-hoc networks scenarios (battlefield, catastrophic disaster, etc.) to better fit the specific characteristics of this new application field. In particular, we propose an adaptative QoS architecture oriented to provide context-aware quality of service to the traffic generated in a smart-building network.'),
(953,'The growth of sensor networks during the last years is a fact and within this field, wireless sensor networks are growing particularly as there are many applications that demand the use of many nodes, even hundreds or thousands. More and more applications are emerging to solve several problems in data acquisition and control in different environments, taking advantage of this technology. In this context, hardware design of the sensor network node becomes critical to satisfy the hard constraints imposed by wireless sensor networks, like low power consumption, low size and low cost. Moreover, these nodes must be capable of sensing, processing and communicating physical parameters, becoming true smart sensors in a network. With this goal in mind, we propose a modular architecture for the nodes, composed of four layers: communication, processing, power supply and sensing. The purpose is to minimize the redesign effort as well as to make the node flexible and adaptable to many different applications. In a first prototype of the node, we present a node with a mixed design based on a microcontroller and an FPGA for the processing layer and Bluetooth technology for communications.'),
(954,'In recent times, interest in Ambient Intelligence (or AmI) has increased considerably. One of the main challenges in the development of these systems is to improve their modularization in order to achieve a high degree of reusability, adaptability and extensibility. This will help us to deal with the heterogeneity and evolution of the environments in which AmI devices exit. An example would be to easily adapt existing applications when new communication technologies appear. Current approaches apply component technologies to achieve these goals, but more should be done. Our research focuses on applying aspect technologies to components in order to improve AmI application modularization. We present the benefits of aspect technologies with regard to reusability and adaptability, by showing the limitations of PCOM, a component-based AmI middleware platform. We will show a study comparing DAOPAmI, our own component and aspect-based AmI middleware platform and PCOM.'),
(955,'This paper describes OCP (Open Context Platform), a middleware which provides support for management of contextual information and merging of information from different sources. The host system is thus endowed with proactive capacities which, in turn, provide a certain environmental intelligence [8]. The approach consists of a modelling of contextual information which is based on Semantic Web derived technologies and a description of the structured merging in the form of decision rules. The latter serve to classify situations of interest and subsequently to trigger off the relevant actions at each moment. Elsewhere, the underlying architecture in OCP is designed so that it can function both by centralizing all the contextual information from a central server and by distributing it among consumers and producers of this type of information.'),
(956,''),
(957,'In ad-hoc networks, mobile devices communicate via wireless links without the aid of any fixed networking infrastructure. These devices must be able to discover services dynamically and share them safely, taking into account ad-hoc net-works requirements such as limited processing and communication power, decentralised management, and dynamic network topology, among others. Legacy solutions fail in addressing these requirements. </P>  <P>In this paper, we propose a service discovery protocol with security features, the Secure Pervasive Discovery Protocol. SPDP is a fully distributed protocol in which services offered by devices can be discovered by others, without a central server. It is based on an anarchy trust model, which provides location of trusted services, as well as protection of confidential information, secure communications, or access control.</P>'),
(958,'Everyone realizes how powerful the few big Web search engine companies have become, both in terms of financial resources due to soaring stock quotes and in terms of the still hidden value of the wealth of information available to them. Following the common belief that \"information is power\" the implications of what the data collection of a de-facto monopolist in the field like Google could be used for should be obvious. However, user studies show that the real implications of what a company like Google can do, is already doing, and might do in a not too distant future, are not explicitly clear to most people.</P>      <P>Based on billions of daily queries and an estimated share of about     49% of the total Web queries [Colburn, 2007], allows predicting     with astonishing accuracy what is going to happen in a number of     areas of economic importance. Hence, based on a broad information     base and having the means to shift public awareness such a company     could for instance predict and influence the success of products     in the market place beyond conventional advertising or play the     stock market in an unprecedented way far beyond mere time series     analysis. But not only the mining of information is an interesting     feature; with additional services such as Google Mail and on-line     communities, user behavior can be analyzed on a very personal     level. Thus, individual persons can be targeted for scrutiny and     manipulation with high accuracy resulting in severe privacy     concerns.</P>      <P>All this is compounded by two facts: First, Google\'s initial     strategy of ranking documents in a fair and objective way     (depending on IR techniques and link structures) has been replaced     by deliberatively supporting or ignoring sites as economic or     political issues are demanding [Google Policy: Censor,     2007]. Second, Google\'s acquisition of technologies and     communities together with its massive digitization projects such     as [Burright, 2006] [Google Books Library, Project, 2006] enable     it to combine information on issues and persons in a still more     dramatic way. Note that search engines companies are not breaking     any laws, but are just acting on the powers they have to increase     shareholder value. The reason for this is that there are currently     no laws to constrain data mining in any way. We contend that     suitable internationally accepted laws are necessary. In their     absence, mechanisms are necessary to explicitly ensure web content     neutrality (which goes beyond the net neutrality of [Berners-Lee,     2006]) and a balanced distribution of symbolic power [see Couldry,     2003]. In this paper we point to a few of the most sensitive     issues and present concrete case studies to support our point. We     need to raise awareness to the threat that a Web search engine     monopoly poses and as a community start to discuss the     implications and possible remedies to the complex problem.'),
(959,'In this paper the analysis of the data structures used in a symbolic computation system, called Kenzo, is undertaken. We deal with the specification of the inheritance relationship since Kenzo is an object-oriented system, written in CLOS, the Common Lisp Object System. We show how the order-sorted algebraic specification formalism can be adapted, through the \"inheritance as coercion\" metaphor, in order to model the simple inheritance between structures in Kenzo.'),
(960,'Component Based Software Engineering has now   emerged as a discipline for system development. After years of   battle between component platforms, the need for means to abstract   away from specific implementation details is now recognized. This   paves the way for model driven approaches (such as the OMG MDA) but   also for the more older Architectural Description Language (ADL)   paradigm. In this paper we present Korrigan, a true ADL (in the   [MT00] sense), which provides interesting features: fully formal   behaviours and data types, expressive component gluing mechanisms   through the use of temporal logic, yet ensuring the specification   readability thanks to graphical notations.'),
(961,'Nowadays, many daily human activities such as education, trade, talks, etc are done by using the Internet. In such things as registration on Internet web sites, hackers write programs to make automatic false registration that waste the resources of the web sites while it may also stop it from functioning. Therefore, human users should be distinguished from computer programs. To this end, this paper presents a method for distinction of Persian and Arabic-language users from computer programs based on Persian and Arabic texts. Our proposed algorithm is based on adding a background to the image of a meaningless Persian/Arabic randomly generated word. This method relies on the difficulty of automatic separation of background from Persian/Arabic writing, due to the presence of many diacritical dots and signs.</Pgt;  <P>In this method, the image of a random meaningless Persian or Arabic word is shown to the user and he is asked to type it. Considering that the presently available Persian and Arabic OCR programs cannot identify these words, the word can be identified only by a Persian or Arabic-language user. This method also can be used to prevent program attacks, resource waste and performance reduction. The proposed method has been implemented by the Java language. The generated words are tested, using ReadIris and Omnipage OCR systems. These OCR systems were unable to recognize these words.'),
(962,''),
(963,'Several techniques for implementing Prolog in a efficient manner have been devised since the original interpreter, many of them aimed at achieving more speed. There are two main approaches to efficient Prolog implementation: (1) compilers to bytecode and then interpreting it (emulators) or (2) compilers to native code. Emulators have smaller load/compilation time and are a good solution for their simplicity when speed is not a priority. Compilers are more complex than emulators, and the difference is much more acute if some form of code analysis is performed as part of the compilation, which impacts development time. Generation of low level code promises faster programs at the expense of using more resources during the compilation phase. In our work besides using an mixed execution mode, we design an optimizing compiler that using type feedback profiling, dynamic compilation and dynamic deoptimization for improving the performance of logic programming languages.'),
(964,'Emerging interaction paradigms, such as service-oriented computing, and new technological challenges, such as exogenous component coordination, suggest new roles and application areas for process algebras. This, however, entails the need for more generic and adaptable approaches to their design. For example, some applications may require similar programming constructs coexisting with different interaction disciplines. In such a context, this paper pursues a research programme on a coinductive rephrasal of classic process algebra, proposing a clear separation between structural aspects and interaction disciplines. A particular emphasis is put on the study of interruption combinators defined by natural co-recursion. The paper also illustrates the verification of their properties in an equational and pointfree reasoning style as well as their direct encoding in Haskell.'),
(965,'Modern Java Compilers, such as Sun\'s HotSpot compilers, implement a number of optimizations, ranging from high-level program transformations to low-level architecure dependent operations such as instruction scheduling. In a Just-in-Time (JIT) environment, the impact of each optimization must be weighed against its cost in terms of total runtime. Towards better understanding the usefulness of individual optimizations, we study the main optimizations available on Sun HotSpot compilers for a wide range of scientific and non-scientific benchmarks, weighing their cost and benefits in total runtime. We chose the HotSpot technology because it is state of the art and its source code is available.'),
(966,'This paper defines algorithms to automatically detect five types of bad smells that occur in aspect-oriented systems, more specifically those written using the AspectJ language. We provide a prototype implementation to evaluate the detection algorithms in a case study, where bad smells are detected in three well-known AspectJ systems.'),
(967,'Program slicing is a well known family of techniques used to identify code fragments which depend on or are depended upon specific program entities. They are particularly useful in the areas of reverse engineering, program understanding, testing and software maintenance. Most slicing methods, usually oriented towards the imperative or object paradigms, are based on some sort of graph structure representing program dependencies. Slicing techniques amount, therefore, to (sophisticated) graph transversal algorithms. This paper proposes a completely different approach to the slicing problem for functional programs. Instead of extracting program information to build an underlying dependencies\' structure, we resort to standard program calculation strategies, based on the so-called Bird-Meertens formalism. The slicing criterion is specified either as a projection or a hiding function which, once composed with the original program, leads to the identification of the intended slice. Going through a number of examples, the paper suggests this approach may be an interesting, even if not completely general, alternative to slicing functional programs.'),
(968,'Design patterns have been developed to cope with the vast space of possible different designs within object-oriented systems. One of those classic patterns is the Visitor Pattern, used for representing an operation to be performed on the elements of an object structure. In general, the order in which the objects are visited is crucial. We present a mapping from the Visitor Pattern to a context free grammar that defines the set of all such visit sequences, a given Visitor can perform. The language defined by this grammar is the language of the Visitor Design Pattern and the mapping encodes knowledge about the class hierarchy and the implementation of the accept methods of a Visitor Design Pattern. It is general enough to model complications that occur when traversing arbitrary object structures, and also properly represents cases such as lack of a common base class, multiple inheritance, and inheritance from concrete classes. Due to its particular design, the grammar can also be used as precise documentation for a Visitor Design Pattern.'),
(969,'In a mobile language, computations can move   between locations in a network to better utilise resources, e.g., as   in a computational GRID. <I>Mobile Haskell</I>, or   mHaskell, is a small extension of Concurrent Haskell that enables   the construction of distributed mobile software by introducing   higher order communication channels called <I>Moble   Channels</I> (MChannels). mHaskell only provides <I>weak   mobility</I>, i.e. the ability to start new computations on   remote locations. This paper shows how <I>strong   mobility</I>, i.e. the ability to migrate running threads   between locations, can be implemented in a language like mHaskell   with weak mobility, higher-order channels and first-class   continuations. Using Haskell\'s high level features, such as   higher-order functions, type classes and support for monadic   programming, strong mobility is achieved without any changes to the   runtime system, or built-in support for continuations. Strong   mobility is illustrated with examples and a mobile agent case   study.'),
(970,'Most modern computer applications should run on heterogeneous platforms and, moreover, objects and respective code should be easily interchangeable between distinct platforms at runtime. This paper describes a runtime platform based on distributed and cooperating virtual machines named Virtuosi. A unified object model permits easy inter-operation between applications written on different languages. All applications must be compiled to a standard runtime code format so they all can run on any platform where an implementation of the virtual machine exists. A novel code format which is entirely based on instances of the classes that define the object model itself is employed. A proper programming language has been defined, a corresponding compiler implemented, a virtual machine that includes a class loader, a code interpreter, a single-threaded execution control and a distributed object store implemented and tested through example applications.'),
(971,'PEWS is a language for the implementation of web service interfaces. PEWS programs can be used for the description of both individual and composed web services. Individual web services can be built up from Java programs. Composed web services are built from simpler services. PEWS operators describe the allowed workflow of the web service, i.e.the order in which the operations of the web service will be executed. In this paper we analyze the expressiveness of PEWS programs. This is done by the systematic evaluation of the language. Our evaluation is based on a framework composed by workflow patterns. We also compare PEWS with other interface description languages. This comparison is based on the workflow behavior of the languages.'),
(972,'UML may be used to describe both the structure and behavior of objectoriented systems using a combination of notations. For the modeling of the dynamic behavior, a number of different models are offered such as interaction, state and activity diagrams. Although compositional techniques for modeling computational processes demand means of composing elements both in non-atomic or atomic ways, UML seems to lack compositional constructs for defining atomic composites. We discuss proper extensions for diagrams that are able to cope with the concept of atomic composition as the basic element for describing transactions (in our settings the term \"transaction\" denotes a certain operation of a system that might be atomically composed by many, possibly concurrent, operations). Atomic compositions are then formally defined through a special morphism between automata in a domain called Nonsequential Automata.'),
(973,'The concept of versioning was initially proposed for controlling design evolution on computer aided design and software engineering. On the context of database systems, versioning is applied for managing the evolution of different elements of the data. Modern database systems provide not only powerful data models but also complex query languages that have evolved to include several features from complex programming languages. While most related work focuses on different aspects of the concepts, designing models, and processing of versions efficiently, there is yet to be a formal definition of a query language for database systems with versions control. In this work we propose a query language, named Versioned Object Query Language (VOQL), that extends ODMG Object Query Language (OQL) with new features to recover object versions. We provide a precise definition of VOQL through a type system and we prove it safe in relation to a small-step operational semantics. Finally, we validate the proposed definition by implementing an interpreter for VOQL.'),
(974,''),
(975,'We outline the Berlin Brain-Computer Interface   (BBCI), a system which enables us to translate brain signals from   movements or movement intentions into control commands. The main   contribution of the BBCI, which is a non-invasive EEG-based BCI   system, is the use of advanced machine learning techniques that   allow to adapt to the specific brain signatures of each user with   literally no training. In BBCI a calibration session of about 20min   is necessary to provide a data basis from which the individualized   brain signatures are inferred. This is very much in contrast to   conventional BCI approaches that rely on operand conditioning and   need extensive subject training of the order 50-100 hours. Our   machine learning concept thus allows to achieve high quality   feedback already after the very first session. This work reviews a   broad range of investigations and experiments that have been   performed within the BBCI project. In addition to these general   paradigmatic BCI results, this work provides a condensed outline of   the underlying machine learning and signal processing techniques   that make the BBCI succeed. In the first experimental paradgm we   analyze the predictability of limb movement long before the actual   movement takes place using only the movement intention measured from   the pre-movement (readiness) EEG potentials. The experiments include   both off-line studies and an online feedback paradigm. The limits   with respect to the spatial resolution of the somatotopy are   explored by contrasting brain patterns of movements of left   vs. right hand rsp. foot. In a second conplementary paradigm   voluntary modulations of sensorimotor rhythms caused by motor   imagery (left hand vs. right hand vs. foot) are translated into a   continuous feedback signal. Here we report results of a recent   feedback study with 6 healthy subjects with no or very little   experience with BCI control: half of the subjects achieved an   information transfer rate above 35 bits per minute   (bmp). Furthermore one subject used the BBCI to operate a mental   typewriter in free spelling mode. The overall spelling speed was   4.5-8 letters per minute including the time needed for the   correction errors.'),
(976,'Data assimilation is a method of combining an imperfect simulation model and a number of incomplete observation data. Sequential data assimilation is a data assimilation in which simulation variables are corrected at every time step of observation. The ensemble Kalman filter is developed for a sequential data assimilation and frequently used in geophysics. On the other hand, the particle filter developed and used in statistics is similar in view of ensemble-based method, but it has different properties. In this paper, these two ensemble based filters are compared and characterized through matrix representation. An application of sequential data assimilation to tsunami simulation model with a numerical experiment is also shown. The particle filter is employed for this application. An erroneous bottom topography is corrected in the numerical experiment, which demonstrates that the particle filter is useful tool as the sequential data assimilation method.'),
(977,'In this paper, we survey efforts devoted to discovering interesting exceptions from data in data mining. An exception differs from the rest of data and thus is interesting and can be a clue for further discoveries. We classify methods into exception instance discovery, exception rule discovery, and exception structured-rules discovery and give a condensed and comprehensive introduction.'),
(978,'We give a brief introduction to probabilistic encryptions. This serves as an example how randomness plays a pivotal role in cryptographic systems that satisfy advanced security concepts.'),
(979,'Pseudorandom number generators are widely used in the area of simulation. Defective generators are still widely used in standard library programs, although better pseudorandom number generators such as the Mersenne Twister are freely available.</P>  <P>This manuscript gives a brief explanation on pseudorandom number generators for Monte Carlo simulation. The existing definitions of pseudorandomness are not satisfactorially practical, since the generation of sequences satisfying the definitions is sometimes impossible, somtimes rather slow. As a compromise, to design a fast and reliable generator, some mathematical indices are used as measures of pseudorandomness, such as the period and the higher-dimensional equidistribution property. There is no rigorous justification for the use of these indices as measures of pseudorandomness, but experiences show their usefulness in choosing pseudorandom number generators.'),
(980,'Shor\'s algorithms for the integer factorization and the discrete logarithm problems can be regarded as a negative effect of the quantum mechanism on publickey cryptography. From the computational point of view, his algorithms illustrate that quantum computation could be more powerful. It is natural to consider that the power of quantum computation could be exploited to withstand even quantum adversaries. Over the last decade, quantum cryptography has been discussed and developed even from the computational complexity-theoretic point of view. In this paper, we will survey what has been studied in quantum computational cryptography.'),
(981,'Combinatorial property testing, initiated   formally by Goldreich, Goldwasser, and Ron in [Goldreich et   al. (1998)] and inspired by Rubinfeld and Sudan in [Rubinfeld and   Sudan 1996], deals with the relaxation of decision problems. Given a   property <I>P</I> the aim is to decide whether a given   input satisfies the property <I>P</I> or is   <I>far</I> from having the property. A property   <I>P</I> can be described as a language, i.e., a   nonempty family of binary words. The associated property to a family   of boolean functions <I>f</I> =   (<I>f<sub>n</sub></I>) is the set of   1-inputs of <I>f</I>. By an attempt to correlate the   notion of testing to other notions of low complexity property   testing has been considered in the context of formal   languages. Here, a brief summary of results on testing properties   defined by formal languages and by languages implicitly represented   by small restricted branching programs is provided.'),
(982,'NP-complete problems cannot have efficient algorithms unless P = NP. Due to their importance in practice, however, it is useful to improve the known exponential-time algorithms for NP-complete problems. We survey some of the recent results on such improved exponential-time algorithms for the NP-complete problems satisfiability, graph colorability, and the domatic number problem. The deterministic time bounds are compared with the corresponding time bounds of randomized algorithms, which often run faster but only at the cost of having a certain error probability.'),
(983,'In this paper we give an introduction to the connection between complexity theory and the study of randomized algorithms. In particular, we will define and study probabilistic complexity classes, survey the basic results, and show how they relate to the notion of randomized algorithms.'),
(984,''),
(985,'Behavioural specifications allow to focus only on the\"observable\" behaviour of objects. These observations are made through \"observable contexts\" which are particular terms with a hole to be filled in with an object. We consider behavioural specifications based on the observation of a specified set of linear terms. The set of observable contexts is often infinite; therefore, we give an algorithm for computing some special contexts that we call \"covering contexts\", and show that they are sufficient for proving that two terms are behaviourally equal.'),
(986,'Fingerprint image enhancement is a common and critical step in fingerprint recognition systems. To enhance the images, most of the existing enhancement algorithms use filtering techniques that can be categorized into isotropic and anisotropic according to the filter kernel. Isotropic filtering can properly preserve features on the input images but can hardly improve the quality of the images. On the other hand, anisotropic filtering can effectively remove noise from the image but only when a reliable orientation is provided. In this paper, we propose a ridge orientation estimation and verification algorithm which can not only generate an orientation of ridge flows, but also verify its reliability. Experimental results show that, on average, over 51 percent of an image in the NIST-4 database has reliable orientations. Based on this algorithm, a hybrid fingerprint enhancement algorithm is developed which applies isotropic filtering on regions without reliable orientations and anisotropic filtering on regions with reliable orientations. Experimental results show the proposed algorithm can combine advantages of both isotropic and anisotropic filtering techniques and generally improve the quality of fingerprint images.'),
(987,'When dealing with remote systems, it is desirable that these systems are capable of operation within acceptable levels with minimal control and maintenance. In terms or transmission of telemetry information, a prediction-based compression scheme has been introduced. This paper studies the influence of some typical transmission and network errors on the encoded residue stream produced by a number of predictors used in the scheme, with the intention of identifying the more fault tolerant architecture that may be preferred as predictors. Classical linear predictors such as FIR and lattice filters, as well as a variety of feedforward and recurrent neural networks are studied. The residue streams produced by these predictors are subjected to two types of commonly occurring transmission noise, namely gaussian and burst. The noisy signal is decoded at the receiver and the magnitude of error, in terms or MSE and MAE are compared. Hardware failures in the input receptor and multiplier are also simulated and the performance of various predictors is compared. Overall, it is found that even small low-complexity neural networks are more resilient to faults due to the characteristics of their parallel architecture and distributed storage/processing characteristics.'),
(988,'We cast a new look on time-varying distributed H systems. In their original definition, where only new strings are passed to the next component, this language definition in itself is already enough to obtain computational completeness. Here, we consider two types of time-varying H systems with weaker language definitions, based on the usual definition of splicing systems: The next generation of strings consists of the union of all existing strings and the newly created strings. We show that if all strings, both old and new, are passed to the next component these systems are regular in power. If however, the new strings pass to the next component and the existing ones remain accessible to the current one, we prove that systems with 4 components are already computationally complete.'),
(989,''),
(990,''),
(991,'Abstract This paper presents a branching schema for the solving of a wide range of interval constraint satisfaction problems defined on any domain of computation, finite or infinite, provided the domain forms a lattice. After a formal definition of the branching schema, useful and interesting properties, satisfied by all instances of the schema, are presented. Examples are then used to illustrate how a range of operational behaviors can be modelled by means of different schema instantiations. It is shown how the operational procedures of many constraint systems (including cooperative systems) can be viewed as instances of this branching schema. Basic directives to adapt this schema to solving constraint optimization problems are also provided.'),
(992,'The need for processing biological information is rapidly growing, owing to the masses of new information in digital form being produced at this time. Old methodologies for processing it can no longer keep up with this rate of growth. The methods of Artificial Intelligence (AI) in general and of language processing in particular can offer much towards solving this problem. However, interdisciplinary research between language processing and molecular biology is not yet widespread, partly because of the effort needed for each specialist to understand the other one\'s jargon. We argue that by looking at the problems of molecular biology from a language processing perspective, and using constraint based logic methodologies we can shorten the gap and make interdisciplinary collaborations more effective. We shall discuss several sequence analysis problems in terms of constraint based formalisms such Concept Formation Rules, Constraint Handling Rules (CHR) and their grammatical counterpart, CHRG. We postulate that genetic structure analysis can also benefit from these methods, for instance to reconstruct from a given RNA secondary structure, a nucleotide sequence that folds into it. Our proposed methodologies lend direct executability to high level descriptions of the problems at hand and thus contribute to rapid while efficient prototyping.'),
(993,'Bisimulation can be defined in a simple way using coinductive methods, and has rather pleasant properties. Ready similarity was proposed by Meyer et al. as a way to weakening the bisimulation equivalence thus getting a semantics defined in a similar way, but supported for more reasonable (weaker) observational properties.</P>  <P>Global bisimulations were introduced by Frutos et al. in order to study different variants of non-determinism getting, in particular, a semantics under which the internal choice operator becomes associative. Global bisimulations are defined as plain bisimulations but allowing the use of new moves, called global transitions, that can change the processes not only locally in its head, but anywhere.</P>  <P>Now we are continuing the study of global bisimulation but focusing on the way different semantics can be characterised as global bisimulation semantics. In particular, we have studied ready similarity, on the one hand because it was proposed as the strongest reasonable semantics weaker than bisimulation; on the other hand, because ready similarity was not directly defined as an equivalence relation but as the nucleus of an order relation, and this open the question whether it is also possible to define it as a symmetric bisimulation-like semantics.</P>  <P>We have got a simple and elegant characterisation of ready similarity as a global bisimulation semantics, that provides a direct symmetric characterisation of it as an equivalence relation, without using any order as intermediate concept. Besides, we have found that it is not necessary to start from a simulation based semantics to get an equivalent global bisimulation. What has proved to be very useful is the axiomatic characterisation of the semantics. Following these ideas we have got also global bisimulation for several semantics, including refusals and traces. That provides a general framework that allows to relate both intensional and extensional semantics.'),
(994,'The size and complexity of software systems are   continuously increasing, which makes them difficult and   labor-intensive to develop, test and evolve. Since concurrent   systems are particularly hard to verify by hand, achieving effective   and automated verification tools for concurrent software has become   an important topic of research. <I>Model checking</I> is   a popular automated verification technology which allows us to   determine the properties of a software system and enables more   thorough and less costly testing. In this work, we improve the   <I>model-checking</I> methodology previously developed   for the <I>timed concurrent constraint programming   language</I> <tt>tccp</tt> so that more   sophisticated, real-time properties can be verified by the   model-checking tools. The contributions of the paper are twofold. On   the one hand, we define a timed extension of the   <tt>tccp</tt> semantics which considers an explicit,   discrete notion of the passing of time. On the other hand, we   consistently define a real-time extension of the linear-time   temporal logic that is used to specify and analyze the software   properties in <tt>tccp</tt>. Both extensions fit into   the <tt>tccp</tt> framework perfectly in such a way that   with minor modifications any <tt>tccp</tt> model checker   can be reused to analyze real-time properties. Finally, by means of   an example, we illustrate the improved ability to check real-time   properties.'),
(995,'Functional-logic programming amalgamates some of   the main features of both functional and logic styles into a single   paradigm. Nevertheless, negation is a widely investigated feature in   logic programming that has not received much attention in such   programming style. It is not difficult to incorporate some kind of   <I>negation as finite failure</I> for ground goals, but   we are interested in a <I>constructive</I> version able   to deal with non-ground goals. With this aim, in previous works we   have built a formal framework for checking <I>(finite) failure   of reduction</I>. In this paper we adapt it for implementing a   prototype for a functional-logic language with <I>constructive   failure</I> as the natural counterpart to negation in logic   programming.'),
(996,'We present a novel approach to the verification of functional-logic programs. For our verification purposes, equational reasoning is not valid due to the presence of non-deterministic and partial functions. Our approach transforms functionallogic programs into Maude theories and then uses the Rewriting Logic logical framework to verify properties of the transformed programs. We propose an inductive proving method based on the length of the computation on the Rewriting Logic framework to cope with the non-deterministic and non-terminating aspects of the programs. We illustrate the application of the method on various examples, where we analyze the sequence of steps to be performed by the proof in order to get expertise for the automatization of the process. Then, since the proposed transformation process is also amenable of automatization, we will obtain a tool for proving properties of CRWL programs. Another advantage of our methodology, that distinguish it from other approaches, is that it does not confuse the original functional-logic program with the subjects we want to talk about in the properties, but it allows the equational definition of observations on top of the transformed programs which simplifies the obtained proofs.'),
(997,'We present a tutorial of the ITP tool, a rewriting-based theorem prover that can be used to prove inductive properties of membership equational specifications. We also introduce membership equational logic as a formal language particularly adequate for specifying and verifying semantic data structures, such as ordered lists, binary search trees, priority queues, and powerlists. The ITP tool is a Maude program that makes extensive use of the reflective capabilities of this system. In fact, rewritingbased proof simplification steps are directly executed by the powerful underlying Maude rewriting engine. The ITP tool is currently available as a web-based application that includes a module editor, a formula editor, and a command editor. These editors allow users to create and modify their specifications, to formalize properties about them, and to guide their proofs by filling and submitting web forms.'),
(998,'The eXtensible Markup Language (XML) is considered as the format of choice for the exchange of information among various applications on the Internet. Since XML is emerging as a standard for data exchange, it is natural that queries among applications should be expressed as queries against data in XML format. This use gives rise to a requirement for a query language expressly designed for XML resources. World Wide Web Consortium (W3C) convened to create the XQuery language, concretely, a typed functional language for querying XML documents. One key aspect of the XQuery language is the use of the XPath language as basis for handling the structure of an XML document. In this paper, we present a proposal for the representation of XML documents by means of a logic program. Rules and facts can be used for representing the document schema and the XML document itself. In addition, we study how to query by means of the XPath language against a logic program representing an XML document. It evolves the specialization of the logic program with regard to the XPath expression. This specialization technique is based on the well-known transformation technique called Magic Sets and studied for deductive databases. The bottom-up evaluation of the speciali'),
(999,'Multi-adjoint logic programming represents a   very recent, extremely flexible attempt for introducing fuzzy logic   into logic programming. In this setting, the execution of a goal   w.r.t. a given program is done in two separate phases. During the   operational one, <I>admissible steps</I> are   systematically applied in a similar way to classical resolution   steps in pure logic programming, thus returning a computed   substitution together with an expression where all atoms have been   exploited. This last expression is then interpreted under a given   lattice during the so called interpretive phase, hence returning a   value which represents the fuzzy component (<I>truth   degree</I>) of the computed answer.</P>   <P>On the other hand, unfolding is a well known transformation rule widely used in declarative programming for optimizing and specializing programs, among other applications. In essence, it is usually based on the application of operational steps on the body of program rules. The novelty of this paper consists in showing that this process can also be made in terms of interpretive steps. We present two strongly related kinds of unfolding (operational and interpretive), which, apart from exhibiting strong correctness properties (i.e. they preserve the semantics of computed substitutions and truth degrees) they are able to significantly simplify the two execution phases when solving goals.'),
(1000,'Plagiarism in the sense of \"theft of intellectual property\" has been around for as long as humans have produced work of art and research. However, easy access to the Web, large databases, and telecommunication in general, has turned plagiarism into a serious problem for publishers, researchers and educational institutions. In this paper, we concentrate on textual plagiarism (as opposed to plagiarism in music, paintings, pictures, maps, technical drawings, etc.). We first discuss the complex general setting, then report on some results of plagiarism detection software and finally draw attention to the fact that any serious investigation in plagiarism turns up rather unexpected side-effects. We believe that this paper is of value to all researchers, educators and students and should be considered as seminal work that hopefully will encourage many still deeper investigations.'),
(1001,'We investigate behavioral institutions and   refinements in the context of the object oriented paradigm. The   novelty of our approach is the application of generalized abstract   algebraic logic theory of hidden heterogeneous deductive systems   (called <I>hidden k-logics</I>) to the algebraic   specification of object oriented programs. This is achieved through   the <I>Leibniz congruence</I> relation and its   combinatorial properties.</P>    <P>We reformulate the notion of hidden <I>k</I>-logic as well as the behavioral logic of a hidden <I>k</I>-logic as institutions. We define refinements as hidden signature morphisms having the extra property of preserving logical consequence. A stricter class of refinements, the ones that preserve behavioral consequence, is studied. We establish sufficient conditions for an ordinary signature morphism to be a behavioral refinement.'),
(1002,'We present two probabilistic leader election algorithms for anonymous unidirectional rings with FIFO channels, based on an algorithm from Itai and Rodeh [Itai and Rodeh 1981]. In contrast to the Itai-Rodeh algorithm, our algorithms are finite-state. So they can be analyzed using explicit state space exploration; we used the probabilistic model checker PRISM to verify, for rings up to size four, that eventually a unique leader is elected with probability one. Furthermore, we give a manual correctness proof for each algorithm.'),
(1003,'The seminal algorithm developed by Ron Cytron,   Jeanne Ferrante and colleagues in 1989 for the placement of   <I></I>-nodes in a control flow graph is still widely used   in commercial compilers. Placing <I></I>-nodes is necessary when   converting a program representation to Static Single Assignment   (SSA) form. This paper shows that if a variable <I>x</I> is defined in   a set of basic blocks <I>A(x)</I>, then the iterated   join set of <I>A(x)</I> can be decomposed into the   computation of the iterated join set of a disjoint collection of   subsets of <I>A(x)</I>. We use this result to show that   some join set computations are redundant. We measured the number of   redundant computations in the Open Research Compiler (ORC) in a   selection of SPEC 2000 benchmarks.'),
(1004,''),
(1005,'To date, one of the main aims of the World Wide   Web has been to provide users with information. In addition to   private homepages, large professional information providers,   including news services, companies, and other organisations have set   up web-sites. With the development and advance of recent   technologies such as wikis, blogs, podcasting and file sharing this   model is challenged and community-driven services are gaining   influence rapidly. These new paradigms obliterate the clear   distinction between information providers and consumers. The lines   between producers and consumers are blurred even more by services   such as Wikipedia, where every reader can become an author,   instantly. </P>  <P>This paper presents an overview of a broad selection of current technologies and services: blogs, wikis including Wikipedia and Wikinews, social networks such as Friendster and Orkut as well as related social services like del.icio.us, file sharing tools such as Flickr, and podcasting. These services enable user participation on the Web and manage to recruit a large number of users as authors of new content. It is argued that the transformations the Web is subject to are not driven by new technologies but by a fundamental mind shift that encourages individuals to take part in developing new structures and content. The evolving services and technologies encourage ordinary users to make their knowledge explicit and help a collective intelligence to develop.'),
(1006,'In this paper a distributed algorithm is   proposed that realises mutual exclusion among n nodes in a computer   network.  There is no common or global memory shared by the nodes   and there is no global controller.  The nodes of the network   communicate among themselves by exchanging messages only.  The   proposed algorithm is based on queue migration and achieves a   message complexity of O(n) per mutual exclusion invocation.  Under   heavy load, the number of required messages approaches 2 per mutual   exclusion.'),
(1007,'Characterising liveness using a structure based approach is a key issue in theory of Petri nets. In this paper, we introduce a <i>structure causality relation</i> from which a topological characterisation of liveness in Petri nets is defined. This characterisation relies on a controllability property of siphons and allows to determine the borders of the largest abstract class of Petri nets for which equivalence between liveness and deadlock-freeness holds. Hence, interesting subclasses of P/T systems, for which membership can be easily determined, are presented. Moreover, this paper resumes, from a new point of view, similar results related to this issue and, provides a unified interpretation of the causes of the non-equivalence between liveness and deadlock-freeness.'),
(1008,'Exploratory data analysis over foreign language   text presents virtually untapped opportunity. This work incorporates   Nave Bayes classifier with Case-Based Reasoning in order to   classify and analyze Arabic texts related to fanaticism. The Arabic   vocabularies are converted to equivalent English words using   conceptual hierarchy structure. The understanding process operates   at two phases. At the first phase, a discrimination network of   multiple questions is used to retrieve explanatory knowledge   structures each of which gives an interpretation of a text according   to a particular aspect of fanaticism. Explanation structures   organize past documents of fanatic content. Similar documents are   retrieved to generate additional valuable information about the new   document.  In the second phase, the document classification process   based on Na&#239;ve Bayes is used to classify documents into their   fanatic class. The results show that the classification accuracy is   improved by incorporating the explanation patterns with the Nave   Bayes classifier.'),
(1009,''),
(1010,'As enterprises worldwide race to improve real-time management to improve productivity, customer services and flexibility, huge resources have been invested into enterprise systems (ESs). All modern ESs adopt an n-tier client-server architecture, which includes several application servers to hold users and applications. As in any other multi-server environment, the load distributions, and user distributions in particular, become a critical issue in tuning system performance.</P>  <P>In stateful ESs, a user who logs onto an application server and stays connected to the server for an entire working session, which can last for days, evokes each application.  Therefore, admitting a user onto an application server affects not only current but also future performance of that server. Although the n-tier architecture may involve web servers, there is little in the literature in Distributed Web Server Architectures that considers the effects of distributing users instead of individual requests to servers.</P>  <P>The algorithm proposed in this paper gives specific suggestions in user distributions and the minimal number of servers required based on the application reusability threshold.  A heuristic version of the algorithm is also presented to improve the performance. The paper also discusses how to apply association rules to predict new user behavior when distributing users in the run-time. The distributions recommended by the algorithms are compared against the Round-Robin distributions on a set of real data derived from a mid size company. The result shows that the user distributions suggested have better performance than Round Robin distributions.'),
(1011,'In this paper we give a possible model for handling uncertain information. The concept of fuzzy knowledge-base will be defined as a quadruple of any background knowledge, defined by the proximity of predicates and terms; a deduction mechanism: a fuzzy Datalog program; a connecting algorithm, which connects the background knowledge with the program and a decoding set of the program, which help us to determine the uncertainty level of the results. Evaluation strategies will also be presented.'),
(1012,'Recently a growing demand has arisen for methods for the development of small- and medium scale Web Information Systems (WIS). Web applications are being built in a rapidly changing environment where requirements are usually unstable. Short-time design and implementation are needed in response to the new technologies. Our work focuses rather on the design and construction of Web applications, than management. Flexibility is a major requirement in such applications, and also in a database-backed environment for the structure and presentation of the sites.</P>  <P>We propose a systematic design method for Web applications which takes into account the data-oriented aspects of the application. The method is based on a UML profile adapted to the problem domain by means of stereotypes as well as a strategy for generating code templates from such models. We provide a method to derive the navigation model from the structural model of a Web application. We will also show guidelines for the development of the Data Layer of data-oriented Web application. Moreover, why to divide the business logic layer into two parts: the pure application logic for managing the workflow of the application and the storage logic responsible for the data structures.</P>  <P>Rapid development is enabled by providing roundtrip engineering capabilities with support for automatic code generation. We will show the role of XML: why to use XML to support both the reuse of content and context-dependent delivery.  </P>  <P>An advantage of the proposed methodology is that several steps can be performed is a semi-automatic way providing rapid development and prototyping.'),
(1014,'ENUM is a technology based on a procedure that assigns a sequence of traditional telephone numbers to Internet domain names. It specifies a rule that makes it possible to relate a domain to a telephone number without any risk of ambiguity. This domain can then be used to identify various communication services like fax, mobile phone numbers, voice-mail systems, e-mail addresses, IP telephone addresses, web pages, GPS coordinates, call diverts or unified messaging. In our paper we deal with three main problem areas in connection with the business model of the ENUM service and with the introduction of new services, i.e. the questions of tariffs, legal regulations and financial return. For the ENUM procedure to spread out in use specific services have to be implemented that can exploit the advantages of the ENUM and efficient methods have to be elaborated to base existing services on ENUM. We will outline the two new services invented by our group and that we have implemented in our project.'),
(1015,'The primary aim of the present paper is to modify the performance model of Boseand Cheng [1] to a more realistic case when external visits are also allowed to the remote Web servers and the Web servers have limited buffer. We analyze how many parameters affect theperformance of a Proxy Cache Server. Numerical results are obtained for the overall response time with and without a PCS. These results show that the benefit of a PCS depends on variousfactors. Several numerical examples illustrate the effect of visit rates, visit rates for the external users , and the cache hit rate probability on the mean response times.'),
(1016,'On the field of potato research and breeding, there are several possibilities for the application of modern digital image processing and data collection/analysing techniques. One of the most obvious methods is the multi/hyper spectral analysis. In our experiments research were done in the visible as well as in the infra, near infra and thermal wavelength. For more advanced analysis we developed a multi/hyper-spectral analysis method (spectral fractal dimension measurement and application). In the following we summarize its basic elements and the developed integrated information system of potato research and breeding.'),
(1017,'CQL, Continuous Query Language is suitable for data stream queries. Sometimes it is better if the queries operate on relational databases and data streams simultaneously. The execution of a CQL query takes a long time (several hours, days or even more). It is not clear what kind of semantics is suitable for the user when the database is updated during the execution of a CQL query. In this paper we give a short description of CQL, a characterization of update-problems, and we offer possible suggestions for the semantic extension of CQL. After the expansion, the CQL would be suitable for solving much more practical problems. The parallel usage of continuous data streams and updatable databases would be settled.'),
(1018,'Imperative programming languages (such as Java) are the most widespread programming languages recently. Besides being quite easy to get familiar with them, they are also perfectly suitable for business software development. Although the productivity of imperative languages is much acclaimed, some problems are much easier to solve in a logical language. The paper introduces a Java language extension called Japlo, which fits the Prolog language constructs into Java harmonically. Blurring the borders between the representatives of these two paradigms, the author aims at making the logical programming tools more easily available for Java programmers. Japlo does not only provide a foreign language interface to Prolog programs, but you can write Prolog rules within the language relying on some basic concepts (static typing, expression orientation) and the grammatic structure of Java.'),
(1019,'We examined the periodicity of the childhood leukaemia in Hungary using seasonal decomposition time series. Between 1988 and 2000 the number of annually diagnosed leukaemia (incidence) was analysed. The time series of the number of patients between the age of 0 and 18 years and the data series divided at the median were analysed. From the time series the seasonal components, trends, the seasonally adjusted series, the moving averages and the data series of the random components were examined. The seasonal components of the dates of diagnosis revealed that a higher percent of the peaks fell within the winter months than in the other seasons. This proves the seasonal occurrence of the childhood leukaemia in Hungary. These data seem to highlight the role of the environmental effects (viral infections, epidemics, etc.) on the onset of the disease.We examined the periodicity of the childhood leukaemia in Hungary using seasonal decomposition time series. Between 1988 and 2000 the number of annually diagnosed leukaemia (incidence) was analysed. The time series of the number of patients between the age of 0 and 18 years and the data series divided at the median were analysed. From the time series the seasonal components, trends, the seasonally adjusted series, the moving averages and the data series of the random components were examined. The seasonal components of the dates of diagnosis revealed that a higher percent of the peaks fell within the winter months than in the other seasons. This proves the seasonal occurrence of the childhood leukaemia in Hungary. These data seem to highlight the role of the environmental effects (viral infections, epidemics, etc.) on the onset of the disease.'),
(1020,'Models have played an important role in scientific research for a long time. The crop models try to simulate the functioning of the atmosphere-soil-plant system with the help of computers. These models can be effective tools in research, education and solving agricultural and environmental protection related problems. The 4M package includes a crop model and several accessories that help to operate the model. The 4M crop model is a daily-step, deterministic model that simulates the water and nutrient balance of the soil, the soil-plant interactions and the plant development and growth. To mention some examples: (1) The package can be used in education to carry out \'zero-cost\' virtual agricultural experiments and to challenge and enhance the system oriented thinking of the students. (2) In research it can be used for designing experiments and estimating the present and future characteristics of the investigated system. (3) In practical applications the package can be used to prepare agro-technological advise (fertilization, irrigation, etc.) for farmers, and to carry out economical analyses on farm level.'),
(1021,'The internet is an unlimited growing and most comfortable information and communication base. In this context, web portals on the internet play an important role created for combining communication facilities and information data bases. An increasing number of portals involve web-based map applications to integrate geographic information for the representation of economical and other records. Web-based map applications are typically designed in a client-server architecture. Different solutions exist using especially HTML and JavaScript, Flash or Java applets. We discuss these possibilities and point out an alternative solution. Particularly, we show how to use Java Applet Technology and Asynchronous JavaScript and XML (Ajax) in this field.  Finally, we report about our results as a complete, flexible map service solution which is available on the internet. An adapted kind of solution will be used by a geo-based marketing portal for offering information about the economic situation, social and cultural aspects and natural facts of regions to support joint-ventures inside the European Union.'),
(1022,'RSA is one of the oldest and until now one of the most widely used public key cryptographic systems, which is based on the modular raising to power. In this article it is pointed out that most of the essential properties of the RSA can be read out from the number of the modulo n roots of the polynomial mentioned in the title of this article. The results explain almost all of the properties taken into account at the choice of the parameters of the RSA. By the help of the polynomial it is pointed out how the modulus and the exponent must be chosen so that the modular raising to power realizes a secure cryptosystem. The article investigates also the role of the choice of the parameters related to the success of the cycling attack. The article conveys a unified point of view for the examination of a lot of the number theoretic problems arising with respect to the RSA.'),
(1023,'In this paper we show how the distance functions generated by neighborhood sequences provides flexibility in image processing algorithms and image database retrieval. Accordingly, we present methods for indexing and segmenting color images, where we use digital distance functions generated by neighborhood sequences to measure distance between colors. Moreover, we explain the usability of neighborhood sequences within the field of image database retrieval, to find similar images from a database for a given query image. Our approach considers special distance functions to measure the distance between feature vectors extracted from the images, which allows more flexible queries for the users.'),
(1024,'Higher education courses are increasingly created as student organized collections of interrelated modules. At the same time, frequent change of subject matter and knowledge in its background must be handled. Above and other factors created and recognized a need for efficient computer based course management. Conventional computer aided teaching methods are not suitable to organize, manage, and communicate the comprehensive course information any more. The authors considered an analogy with well- organized computer descriptions of interrelated objects in the form of comprehensive integrated models in product engineering. Modeling and management of information serve engineering activities during lifecycle of product. Relevant advanced characteristics of integrated product descriptions are process orientation, definition of application oriented building elements called as features, and assistance of decisions by knowledge representations. The authors considered higher education course as one kind of product and proposed a course model. They focused to integrating student, teacher, and institute demand driven characteristics of modeling. Model is developed for application by course procedures. While conventional virtual education systems concentrate to computer mediated distance education, the authors considered arbitrary mix of campus and distance styles of education. In this paper, the authors first give an introduction in their approach to classroom modeling by a comparison of conventional distance education, conventional virtual classroom, and the proposed model based virtual classroom. Next, functional elements of the proposed course modeling and components of virtual classroom are explained. Conflicts as consequences of inappropriate capability or breaking of human intent are analyzed. Following this, initial conditions for definition of course module and construction of course module using modification by features are detailed. Finally, future work for implementation of the modeling in an experimental system composed by professional product lifecycle management (PLM) system, configurable virtual teaching environment, its virtual classroom extension and virtual classroom extension to the engineering modeling system is concluded.'),
(1025,'A mathematical model is proposed to allow expressive representation of endocrine systems by graphic means. The differential equations exactly describing the system can be formulated easily and automatically by the graphic model. Different kinds of software are supposed to solve these equations easily. Chaotic operational range can be found by fitting the parameters of equations. The results can account for some endocrine diseases and would be able to help the therapy.'),
(1026,'A sequence of increasing translation invariant   subspaces can be defined by the Haar-system (or generally by   wavelets). The orthogonal projection to the subspaces generates a   decomposition (multiresolution) of a signal. Regarding the rate of   convergence and the number of operations, this kind of decomposition   is much more favorable then the conventional Fourier   expansion.</P> <P>In this paper, starting from Haar-like   systems we will introduce a new type of multiresolution. The   transition to higher levels in this case, instead of dilation will   be realized by a two-fold map. Starting from a convenient scaling   function and two-fold map, we will introduce a large class of   Haar-like systems. Besides others, the original Haar system and   Haar-like systems of trigonometric polynomials, and rational   functions can be constructed in this way. We will show that the   restriction of Haar-like systems to an appropriate set can be   identified by the original Haar-system.</P>  <P>Haar-like rational functions are used for the approximation of rational transfer functions which play an important role in signal processing [Bokor1 1998, Schipp01 2003, Bokor3 2003, Schipp 2002].'),
(1027,'Multiple hop routing in mobile ad hoc networks can minimize energy consumption and increase data throughput. Yet, the problem of radio interferences remain. However if the routes are restricted to a basic network based on local neighborhoods, these interferences can be reduced such that standard routing algorithms can be applied.</P>  <P>We compare different network topologies for these basic networks, i.e. the <B>Yao-graph</B> (aka. -graph) and some also known related models, which will be called the <B>SymmY-graph</B> (aka. YS-graph), the <B>SparsY-graph</B> (aka. YY-graph) and the <B>BoundY-graph</B>. Further, we present a promising network topology called the <B>HL-graph</B> (based on <B>H</B>ierarchical <B>L</B>ayers).</P>  <P>We compare these topologies regarding degree, spanner-properties, and communication features. We investigate how these network topologies bound the number of (uni- and bidirectional) interferences and whether these basic networks provide energy-optimal or congestion-minimal routing. Then, we compare the ability of these topologies to handle dynamic changes of the network when radio stations appear and disappear. For this we measure the number of involved radio stations and present distributed algorithms for repairing the network structure.'),
(1028,'In this paper a comparison is provided between the membrane computing systems and the graphical interfaces of operating systems. A membrane computing system is a computing model using massive parallelism inspired by the functioning of living cells. The graphical schemes of these computing devices look like the windows of a graphical operating system representing programs running parallel on the computer. Both similarities and differences of membrane-systems and graphical operating systems are detailed as well as some possible simulation methods.'),
(1029,'As we know the Cauchy distribution plays an important role in Probability Theory and Statistics. In this paper, we investigate the estimation of the location and the scale parameter. Both the one-dimensional problem and the multidimensional problem are studied for large sample. In the one-dimensional case, we give two algorithms for the estimation. The first one is an iterative method for which we prove the convergence and we show that the rate of convergence is geometric. The second algorithm provides an exact solution to the problem. In the multidimensional case, we give an algorithm analogous to the one-dimensional case. Computer experiments show that the rate of convergence is similar to the one-dimensional iterative algorithm.'),
(1030,'For exact examination of human information processing ability we elaborated an information-theory-based model and a new measuring method. Starting from the theoretical background of the well-known Hick-Hyman law and analysing the data acquired during the Soyuz-Salyut space missions, an important fact was derived. When examining the model by reducing the <I>H(X)</I> input entropy of the stimulus signals (symbols) and approaching to the 0bit there was an interesting effect: through the reduction of <I>H(X)</I> input entropy towards zero the ratio of (processed information)/(input entropy) was increasing and its value approached to 1, while at non-zero entropy this ratio was less than 1. Therefore a phase-change-like effect occurred below <I>H(X)</I>=1bit. Consequently the zerobit measurement could not be involved into the determination of Information Processing Ability (<I>IPA</I>) while the traditional measurements were performed only at two distinct values of entropy: at 0bit and at a not too high <I>H(X)</I> value (e.g. 2 bits/symbol).'),
(1031,'This study represents a survey approach in order to analyse the information society in a regional setting. The first part deals with a current problematic issue: the human resources of the information society, which is a neglected research area of the information society.  The second part of the paper will point out the main characteristics of the human adaptation of the information society. My aim is to accentuate the importance of the \'human-interface gap. These days the accessibility is strongly connected to the education and to the fact that several jobs do not require competence in digital or information literacy. Focusing on the human aspect has become a central issue of the higher education, and one of its prominent tasks, besides others, is to prepare the next generation for the challenges of the information society.'),
(1032,'The purpose of the present paper is to give an   expansion of the results of Michiharu Kudo and Anish Mathuria. We   present the base-protocol and formulate three properties of the   protocol with modal logic tools. After that we expand the   baseprotocol and prove four new properties. We prove that the third   trusted partner can not read the message of the sender until a   predetermined time.'),
(1033,'Optimal synthesis of workflow structures, the formerly undefined problem, has been introduced. Mathematical programming model is presented for determining the cost optimal workflow system of a given workflow problem. On the basis of a methodology developed for process network synthesis, effective solvers are available for the systematic synthesis of workflow systems.'),
(1034,'Introduction of the multi-degree linear   education system consisting of BSc, MSc and PhD programs in the   Hungarian higher education, according to the goals and requirements   of the Bologna Process, necessitates reviewing, enhancing and   changing the three-level subject-structures consisting of   fundamental, foundation enlarging and special subjects. In the   course of the last 50 years the quantity of the number of subject   areas and the number of engineering branches have been increasing in   technological faculties around the world and they have already   reached the limits of rationality. Recently there is an increasing   demand for integrated and interdisciplinary special branches such as   Systems Engineering. After having surveyed the paradigm changes in   the progress of engineering sciences, the paper gives a brief   summary on the concept, formation and significance of Systems   Engineering. The paper also deals with Production Information   Engineering as a characteristic field of Systems Engineering, which   offers important application possibilities for IT-based system   integration.'),
(1035,'Our aim is to discuss what, when and, how deep   logic should be taught in the computer science education in   connection with the so called <quot;>Bologna   process</quot;>. We survey the spread of logic in the computer   science education. We draw special attention to the process   resulting by 1987 in a comprehensive school in the international   logic research called <quot;>Computer Science   Logic</quot;>. It includes the investigation of research   problems arising among others on such fields as programming theory,   data- and knowledge base theory and, artificial intelligence as   well. New directions have emerged during the problem solving, the   earlier disciplines of classical logic like lambda calculus, type   theory, model theory, modal logic, temporal logic has come back into   the main scope. The results of these researches have a great impact   on the development of computer science. The research results and the   ever increasing role of logic should be obviously reflected by the   education. We explain our conception concerning this issue as   well.'),
(1036,'In this paper we present the structure and the achieved results of the RD project IKTA-4, 6/2001 <quot;>MEDIP - Platform independent software system for medical image processing<quot;> supported by the Hungarian Ministry of Education. The aim of the project was to develop a software background for our basic and applied research in the field of medical imaging that can be used in clinical routine, as well. Realization was based on the experience of information technology and medical imaging research university teams and a company specialized on software and hardware developing for nuclear medicine. The aims also reflect some former research and development activities of the participants. Thus some of them are well experienced in registration, segmentation and image fusion techniques. These experiences were also considered in the determination of the main purposes. The capabilities of the provided software library were demonstrated through test applications from the fields of orthopedics, oncology and nuclear medicine.'),
(1037,''),
(1038,'In this paper we present research works on   non-intuitive and low-efficient negotiations between agents in agent   based system. We find recommendation techniques as a suitable method   of making negotiation smarter and more efficient. We have introduced   into the negotiation thread the improvements in the form of hybrid   recommendation. Hybrid recommendation is composed of three basic   elements: demographic, collaborative and content-based. On the base   of a negotiation algorithm we have studied how recommendation   methods can improve the whole process of finding mutually acceptable   agreements between agents. The proposed methodology is presented   using a travel agency and its client negotiation.'),
(1039,'In this paper the problem of parameter estimation of an input-output system is discussed. It is assumed that the system is described by the relation known with accuracy to some parameters. The possible noisy observations of system are described. The estimation algorithm based on maximum likelihood method is proposed. The presented approach is illustrated by analytical examples.'),
(1040,'Collecting, distributing and sharing knowledge   in a knowledge-explicit way is a significant task for any company.   However, collecting decisional knowledge in the form of <I>formal   decision events</I> as the fingerprints of a company is an utmost   advance.  Such decisional fingerprint is called decisional DNA.  Set   of experience knowledge structure can assist on accomplishing this   purpose.  In addition, Ontology-based technology applied to set of   experience knowledge structure would facilitate distributing and   sharing companies\' decisional DNA.  Such possibility would assist in   the development of <I>an e-decisional community</I>,   which will support decision-makers on their overwhelming job.  The   purpose of this paper is to explain the development of .an OWL   decisional Ontology built upon set of experience, which would make   decisional DNA, that is, explicit knowledge of formal decision   events, a useful element in multiple systems and technologies, as   well as in the construction of the e-decisional community.'),
(1041,'This paper describes a method of using Petri net   P-invariants in system diagnosis. To model this process a net   oriented fault classification is presented. Hence, the considered   discrete event system is modelled by a live, bounded, and reversible   place-transition Petri net. The notions of D-partition of the set   of places P of a given place-transition net N and net   k-distinguishability are first introduced. Next these two notions   are extended to the set of all vertices, i.e. places and transitions   of N. So the problem of fault identification of the vertices of N   is transformed as a problem of fault identification of the places of   a new net N called a net simulator of N. Any transition in N is   assumed to be fault-free. Then the corresponding net place   invariants are computed. The system k-distinguishability measure is   obtained in a unique way from the place-invariant matrix. For a   large value of k, the system model is extended by using some set of   additional places called test points and at the same time preserving   the original net properties. To obtain a 1-distinguishable net the   notion of a marked graph component is used. It is shown a sufficient   condition for 1-distinguishability of an arbitrary place-transition   net and a corresponding algorithm is presented. Next two different   diagnosis test strategies are discussed, i.e. combinational and   sequential fault diagnosis. Corresponding (single) place and   transition fault models are introduced. The complexity of the   proposed method depends on the effectivity of the existing   algorithms for computation of the P-cover, i.e. the set of   P-invariants covering N. The proposed approach can be extended for   higher level Petri nets, e.g such as coloured nets or also to design   self-diagnosable circuit realisations of Boolean interpreted Petri   nets. Several examples are given.'),
(1042,'The idea of automating e-commerce transactions   attracts a lot of interest among researchers and IT practitioners,   and multi-agent systems are claimed to be one of promising software   technologies for achieving this goal. Since price negotiations are   one of crucial aspects of e-commerce transactions, in this paper we   present a rule-based implementation of automated price negotiations   utilized in a multi-agent system that models an e-commerce   environment. We start by summarizing state-of-the-art in rule-based   approaches to automated negotiations. We follow with a brief   description of the conceptual architecture of our system and a   simplified scenario that involves multiple buyer agents   participating in multiple English auctions performed in parallel.  A   detailed discussion of the design and implementation of price   negotiations, using JADE and JESS, and presentation of sample   experiments complete the paper.'),
(1043,'In this paper we propose an application of data mining methods in the prediction of the availability and performance of Internet paths. We deploy a general decision-making method for advising the users in further usage of Internet path at particular time and date. The method is based on the clustering and tree classification data mining techniques. The usefulness of our method for prediction the Internet path behavior has been confirmed in real-life experiment. The active Internet measurements were performed to gather the end-to-end latency and packet routing information. The knowledge gathered has been analyzed using a professional data mining package via neural clustering and decision tree algorithms. The results show that the data mining can be efficiently used for the purpose of the forecasting the network behavior. We propose to build a network performance monitoring and prediction service based on proposed data mining procedure. We address our approach especially to the non-networkers of such networking frameworks as Grid and overlay networks who want to schedule their network activity but who want to be left free from networking issues to concentrate on their work.'),
(1044,'A method for determining consensus of hierarchical incomplete ordered partitions and coverings of sets is presented in this chapter. Incomplete ordered partitions and coverings are often used in expert information analysis. These structures should be useful when an expert has to classify elements of a set into given classes, but referring to several elements he does not know to which classes they should belong. The hierarchical ordered partition is a more general structure than incomplete ordered partition. In this chapter we present definitions of the notion of hierarchical incomplete ordered partitions and coverings of sets. The distance functions between hierarchical incomplete ordered partitions and coverings are defined. We present also algorithms of consensus determining for a finite set of hierarchical incomplete ordered partitions and coverings.'),
(1045,'In this paper the author considers some problems   related to attribute dependencies in consensus determining. These   problems concern the dependencies of attributes representing the   content of conflicts, which cause that one may not treat the   attributes independently in consensus determining. It is assumed   that attribute values are represented by intervals. In the paper the   author considers the choice of proper distance function. Next, the   limitations guarantying determining a correct consensus despite   treating the attributes independently are presented. Additionally,   the algorithm of calculating the proper consensus in cases when   these limitation are not met is introduced.'),
(1046,'Entrainment, a physical phenomenon in which one   individual\'s expressed information synchronizes with another\'s and   vice versa, can be observed between two communicators who are   interacting naturally. In this study, we focused on the \"rate of   utterances\" as communicators\' expressed information and then   conducted an experiment to observe whether or not entrainment   naturally occurs in the rate of utterances in speech dialogs between   users and an auto response system. Specifically, participants were   asked to read given dialog scripts with an auto response system that   replied with different rates of utterances. The results revealed   that 1) when the system\'s rate of utterances increased, the   participants produced faster rates of utterances, 2) when the   system\'s rates decreased, participants spoke at slower rates. These   results suggest entrainment in the rate of utterances naturally   occurs in speech dialogs between participants and an auto response   system.'),
(1047,''),
(1048,'We present a concept and implementation of a   computational support for spatial memory management and describe its   temporal evolution. Our essential idea is to use an immersible globe   that consists of a global space and a conical space, thereby   providing arbitrary memory space for humans. We developed a   sustainable knowledge globe (SKG) for constructing a memory space,   and proposed a system called Contents Garden to expand the SKG into   immersive space. We carried out the user study of SKG to evaluate   its effectiveness. We also proposed and discussed three perceptual   operations on Contents Garden to improve the operativity of the   SKG.'),
(1049,'Generating composite human motion such as   locomotion and gestures is important for interactive applications,   such as interactive storytelling and computer games. In interactive   story environments, CG characters do not merely stand in one   position. Rather, they should be able to compose gestures and   locomotion based on the discourse of the story and the locations of   objects in the scene. Thus, in the present paper, we propose a   conversational locomotion model for CG character. We constructed a   conversational locomotion network for a virtual environment. A   multi-path searching algorithm calculates the optimal walking path,   which uses node activation from the story locations and conversation   units. The CG character also locally adjusts its position so that it   does not block the referenced object from the users sight. We have   applied the proposed technique to an interactive 3D movie system and   have demonstrated composite motion of the locomotion and   conversation of a CG character, which improves the immersion of the   viewer in the story environment.'),
(1050,'We propose a concept of real-time human proxy   for avatar-based communication systems, which virtualizes a human in   the real world in real-time and which lets the virtualized human   behave as if he/she was present at a distant place.  For estimating   RHP, we apply it to a simple game and a virtual classroom system.   The experimental results shows us that RHP is useful for   avatar-based communication.'),
(1051,'In this paper, we present a computational approach to understanding and augmenting the conversational knowledge process. We introduce the concept of the conversation quantization, a technique of approximating a continuous flow of conversation by a series of conversation quanta that represent points of the discourse. To investigate what the nature of conversation quanta is, we attempt to extract conversation quanta from two types of the meeting videos by hand. As a result, we have obtained some profitable suggestions about conversation quanta.'),
(1052,'This article presents an architecture that   encrypts data with the AES algorithm. This architecture can be   implemented on the Xilinx Virtex II FPGA family, by applying   pipelining and dynamic total reconfiguration (DTR). The originality   of our implementation is that it computes sequentially in the FPGA   the Key and Cipher part of the AES algorithm. This dynamic   reconfiguration implementation allows a good optimization of logic   resources with a high throughput. This architecture employs only   11619 slices allowing a considerable economy of the resources and   reaching a maximum throughput of 44 Gbps.'),
(1053,'Motion estimation from image sequences is a   complex problem which requires high computing resources and is   highly affected by changes in the illumination conditions in most of   the existing approaches. In this contribution we present a high   performance system that deals with this limitation. Robustness to   varying illumination conditions is achieved by a novel technique   that combines a gradient-based optical flow method with a   non-parametric image transformation based on the Rank transform. The   paper describes this method and quantitatively evaluates its   robustness to different illumination changing patterns. This   technique has been successfully implemented in a real-time system   using reconfigurable hardware. Our contribution presents the   computing architecture, including the resources consumption and the   obtained performance. The final system is a real-time device capable   to computing motion sequences in real-time even in conditions with   significant illumination changes. The robustness of the proposed   system facilitates its use in multiple potential application   fields.'),
(1054,'The development of processors with full custom technology has some disadvantages, such as the time used to design the processors and the cost of the implementation. In this article we used the programmable circuits FPGA such as an option of low cost for the development and implementation of Self-Timed (ST) systems. In addition it describes the architecture and the modules that compose the Asynchronous Microprocessor of Centralized Control (AMCC), and reviews the results of the occupation in the implementation of the FPGA.  The operation of this processor only requires of an external pulse to the input of the first asynchronous control block, and with this pulse the sequence of request-recognition of the control unit begins, that it activates the cycle search and it begins the process of execution of the instructions, without the need of having a clock feeding the system. Once concluded the program, the microprocessor stops and include inherently the stoppable clock feature; i.e., circuit is stopped if it is not required (minimal dynamic consumption). Until it is activated again by an external request signal.'),
(1055,'Some sensor systems are characterized by multiple simultaneous aperiodic emissions, with low signal-to-noise ratio and asynchronous detection. In these systems, complementary sets of sequences can be used to encode emissions, due to their suitable auto-correlation and cross-correlation properties. The transmission of a complementary set can be accomplished by interleaving the sequences of the set, generating a macro-sequence which is easily transmitted by a BPSK modulation. The detection of the macrosequence can be performed by means of efficient correlation algorithms with a notably decreased computational load and hardware complexity. This work presents a new hardware design in configurable logic of an efficient correlator for macrosequences generated from complementary sets of sequences. A generic implementation has been achieved, so the configuration can be changed according to the requirements of the application. The developed correlator has been tested in an ultrasonic pulse compression system in which real-time is needed. However, it is applicable in any multi-sensor or communication system where the goal is to make simultaneous emissions from different independent sources, minimizing mutual interference.'),
(1056,'This paper presents a survey of the   characteristics of a vision system implemented in a   reconfigurable/programmable chip (FPGA). System limitations and   performance have been evaluated in order to derive specifications   and constraints for further vision system synthesis. The system   hereby reported has a conventional architecture. It consists in a   central microprocessor (CPU) and the necessary peripheral elements   for data acquisition, data storage and communications. It has been   designed to stand alone, but a link to the programming and debugging   tools running in a digital host (PC) is provided. In order to   alleviate the computational load of the central microprocessor, we   have designed a visual co-processor in charge of the low-level image   processing tasks. It operates autonomously, commanded by the CPU, as   another system peripheral. The complete system, without the sensor,   has been implemented in a single reconfigurable chip as a SOPC. The   incorporation of a dedicated visual co-processor, with specific   circuitry for low-level image processing acceleration, enhances the   system throughput outperforming conventional processing   schemes. However, time-multiplexing of the dedicated hardware   remains a limiting factor for the achievable peak computing   power. We have quantified this effect and sketched possible   solutions, like replication of the specific image processing   hardware.'),
(1057,'Nowadays, the information security has achieved a great importance, both when information is sent through a non-secure network (as the Internet) and when data are stored in massive storage devices. The cryptographic algorithms are used in order to guarantee the security of data sent or stored. A lot of research is being done in order to improve the performance of the current cryptographic algorithms, including the use of FPGAs. In this work we present an implementation of the IDEA cryptographic algorithm using reconfigurable hardware (FPGAs). In addition, in order to improve the performance of the algorithm, partial and dynamic reconfiguration has been used to implement our final circuit. This fact allows us to obtain a very high encryption speed (14.757 Gb/s), getting better results than those found in the literature.'),
(1058,'Sequences of data-dependent tasks, each one   traversing large data sets, exist in many applications (such as   video, image and signal processing applications). Those tasks   usually perform computations (with loop intensive behavior) and   produce new data to be consumed by subsequent tasks. This paper   shows a scheme to pipeline sequences of data-dependent loops, in   such a way that subsequent loops can start execution before the   completion of the previous ones, which achieves performance   improvements. It uses a hardware scheme with decoupled and   concurrent data-path and control units that start execution at the   same time. The communication of array elements between two loops in   sequence is performed by special buffers with a data-driven,   fine-grained scheme. Buffer elements are responsible to flag the   availability of each array element requested by a subsequent loop   (i.e., a ready protocol is used to trigger the execution of   operations in the succeeding loop). Thus, the control execution of   following loops is also orchestrated by data availability (in this   case at the array element grain) and out-of-order produced-consumed   pairs are permitted. The concept has been applied using Nau, a   compiler infrastructure to map algorithms described in Java onto   FPGAs. This paper presents very encouraging results showing   important performance improvements and buffer size reductions for a   number of benchmarks.'),
(1059,''),
(1060,'This paper describes the specification, validation and verification of system and soft-ware requirements using the SCR tabular method and tools. An example is presented to illustrate the SCR tabular notation, and an overview of each of the ten tools in the SCR toolset is presented.'),
(1061,'This paper gives a tutorial introduction to the   ideas behind system development usingthe B-Method. Properly handled,   the crucial relationship between requirements and formal model leads   to systems that are correct by construction. Some industrial   successes are outlined.'),
(1062,'This paper overviews the application of formal   verification techniques to hardware ingeneral, and to floating-point   hardware in particular. A specific challenge is to connect the usual   mathematical view of continuous arithmetic operations with the   discrete world, in a credible andverifiable way.'),
(1063,'This paper introduces model checking, originally   conceived for checking finite statesystems. It surveys its evolution   to encompass finitely checkable properties of systems with unbounded   state spaces, and its application to software and other   systems.'),
(1064,'This paper outlines the emergence of formal   techniques, explaining why they wereslow to take on an industrially   acceptable form. The contemporary scene, in which formal techniques   are increasingly packaged within tools usable by a wide variety of   engineers, is reviewed,as are the promising prospects for the   future.'),
(1065,'This paper overviews the Verification Grand Challenge, a large scale multinationalintiative designed to significantly increase the interoperability, applicability and uptake of formal development techniques. Results to date are reviewed, and next steps are outlined.'),
(1066,'This paper surveys the whys, and the wherefores of using formal methods in an industrial context. Evidence is presented that the benefits of using formal techniques, though not an automatic consequence of their adoption, can be considerable.'),
(1067,'This paper reviews the current practice of   software engineering and outlines someprospects for developing a   more holistic and formally grounded approach.'),
(1068,''),
(1069,'This introductory paper gives some historical   background to the emergence of formalmethods, overviews what   subsequently happened, and surveys prospects for the future. Brief   introductions to the remaining papers in the Special Issue are   given.'),
(1070,'Nowadays, advanced E-Learning systems are generally pedagogy-aware. Commonly, these systems include facilities for defining so-called learning scenarios that reflect sophisticated pedagogical approaches such as collaborative writing or project-oriented learning. To support different learning activities from such scenarios the technological infrastructure of these systems must be appropriately adjusted and configured. Usually, this configuration process is laced with a number of difficulties. Most of these difficulties are caused by the fact that scenario capturing is achieved through informal user-developer dialogues. Typically, the result of such informal dialogues contains inconsistent and incomplete information because of misunderstandings and the complexity of the interactions within a scenario. Consequently, the configuration of the system is suboptimal and a number of iterations are required in order to achieve better results. In this paper an approach to improve this situation is presented. This approach is based on a general formal representation model for describing learning scenarios. A particular formal description of a concrete learning scenario is obtained through a user dialogue with a wizard tool. At the next step, this formal description might be automatically processed to facilitate configuration process. The paper is concluded with some experiences gained by applying this approach in two E-Learning projects.'),
(1071,'The recent interest in isolating real roots of   polynomials has revived interest in computing sharp upper bounds on   the values of the positive roots of polynomials. Until now Cauchy\'s   method was the only one widely used in this   process. &#350;tefnescu\'s recently published theorem offers an   alternative, but unfortunately is of limited applicability as it   works <I>only</I> when there is an   <I>even</I> number of sign variations (or changes) in   the sequence of coefficients of the polynomial under   consideration. In this paper we present a more general theorem that   works for any number of sign variations <I>provided</I>   a certain condition is met. We compare the method derived from our   theorem with the corresponding methods by Cauchy and by Lagrange for   computing bounds on the positive roots of polynomials. From the   experimental results we conclude that it would be advantageous to   extend our theorem so that it works without any restrictive   conditions.'),
(1072,'The WWW is currently experiencing a   revolutionary growth due to its increasing participative community   software applications. This paper highlights an emerging application   development paradigm on the WWW, called mashup. As blogs have   enabled anyone to become a publisher, mashups stimulate web   development by allowing anyone to combine existing data to develop   web applications. Current applications of mashups include tracking   of events such as crime, hurricanes, earthquakes, meta-search   integration of data and media feeds, interactive games, and as an   organizer for web resources. The implications of this emerging web   integration and structuring paradigm remains yet to be explored   fully. This paper describes mashups from a number of angles,   highlighting current developments while providing sufficient   illustrations to indicate its potential implications. It also   highlights the role of mashups in complementing and enhancing   digital journals by providing insights into the quality academic   content, extent of coverage, and the enabling of expanded   services. We present pioneering initiatives for the Journal of   Universal Computer Science in our efforts to harness the collective   intelligence of a collaborative scholarly network.'),
(1073,''),
(1074,'We investigate the computational power of C++   compilers. In particular, it is known that any partial recursive   function can be computed at compile time, using the template   mechanism to dene primitive recursion, composition, and   minimalization.  We show that polynomial time computable functions   can be computed at compile-time using the same mechanism, together   with template specialization.'),
(1075,'Payment systems need to address a number of   security issues in order to be an effective and secure means of   transferring payments across the Internet. To be accessible to a   wider audience, they also need to be easy to use for their end-users   (customers and merchants).</P>  <P>Trying to address these issues, we created the Internet Payment System (IPS). IPS tries to combine the advantages of several existing payment systems. While strong emphasis is made on the mobility and ease of use for its customers, IPS still retains strong security properties. It achieves privacy, integrity, authentication and non-repudiation by using different cryptographic algorithms and techniques. To demonstrate that the protocol satisfies the desired security properties, we use a recently proposed tool for formal verification, called AVISPA.'),
(1076,'The issue addressed in this paper focuses on the   design and implementation of an advanced information system for   Cultural Organizations, which serves as a platform for the   exploitation through e-Commerce web services and, in parallel, the   protection of copyright and digital rights management of the   cultural content. The main components of the information system are:   (a) Digital Image Library, which offers specialized services, (b)   copyright protection and digital rights management of digitized   material and (c) the E-Commerce web services, supported by advanced   technologies, for the proper exploitation of the digital cultural   content. The work described in this contribution focuses on   digitized material of Cultural Heritage and is deployed at several   cultural organizations.'),
(1077,'We present a new type of sat problem called the   <I>k</I>-GD-SAT, which generalizes   <I>k</I>-sat and GD-sat. In <I>k</I>-GD-SAT,   clause lengths have geometric distribution, controlled by a   probability parameter <I>p</I>; for <I>p</I>   = 1, a <I>k</I>-GD-SAT problem is a   <I>k</I>-SAT problem. We report on the phase transition   between satisfiability and unsatisfiability for randomly generated   instances of <I>k</I>-GD-SAT. We provide theoretical   analysis and experimental results suggesting that there is an   intriguing relationship (linear in the parameter   1/<I>p</I>) between crossover points for different   parameters of <I>k</I>-GD-SAT. We also consider a   relationship between crossover points for <I>k</I>-SAT   and <I>k</I>-GD-SAT and provide links between these   values.'),
(1078,'We consider the extension of fair event system   specifications by concepts of access control (prohibitions, user   rights, and obligations). We give proof rules for verifying that an   access control policy is correctly implemented in a system, and   consider preservation of access control by refinement of event   systems. Prohibitions and obligations are expressed as properties of   traces and are preserved by standard refinement notions of event   systems. Preservation of user rights is not guaranteed by   construction; we propose to combine implementation-level user rights   and obligations to implement high-level user rights.'),
(1079,'Web Services enable the creation of complex business activities through the cooperation of independently developed software programs. However, Web Services incur the risk of long delays and locked data when using the classical distributed transaction strategy, and the risk of inconsistency when using the compensating transactions strategy. If the benefits of Web Services are to be fully realized, a better strategy must be employed. In this paper we describe an extended transactions strategy that can be used in conjunction with existing Web Services infrastructures, and that is compatible with existing business practices. The strategy exploits local transactions and commutativity of local actions at each data item to achieve global atomicity for business activities.'),
(1080,'The Database Management System (DBMS) used to be   a commodity software component, with well known standard interfaces   and semantics. However, the performance and reliability expectations   being placed on DBMSs have increased the demand for a variety   add-ons, that augment the functionality of the database in a wide   range of deployment scenarios, offering support for features such as   clustering, replication, and self-management, among others. A well   known software engineering approach to systems with such   requirements is reflection. Unfortunately, standard reflective   interfaces in DBMSs are very limited. Some of these limitations may   be circumvented by implementing reflective features as a wrapper to   the DBMS server. Unfortunately, these solutions comes at the expense   of a large development effort and significant performance   penalty.</P>   <P>In this paper we propose a general purpose DBMS reflection architecture and interface, that supports multiple extensions while, at the same time, admitting efficient implementations. We illustrate the usefulness of our proposal with concrete examples, and evaluate its cost and performance under different implementation strategies.'),
(1081,''),
(1082,''),
(1083,'Exception handling is a powerful mechanisms for   dealing with failures at runtime. It simplifies the development of   robust programs by allowing the programmer to implement recovery   actions and tolerate non-fatal errors. Yet, exception handling is   difficult to get right!  The complexity of correct exception   handling is a major cause for incorrect exception handling. It is   therefore important to reduce the complexity of writing exception   handling code while, at the same time, making sure it is   correct. Our approach is to use atomic blocks for exception handling   combined with optional compensation actions.'),
(1084,'The impact of the Internet on Society also   affects learning at University. Students use not only printed books   and their own notes, but also the information available on the   Net. WebQuests are learning tools that help the students use the   Internet, but under the supervision of the lecturer, who has   previously selected the most interesting sites to visit. An   experience of using WebQuests with first year Computer Science   students is shown, as well as the good results obtained both in the   improvement of examination results and in the positive attitude of   the students when using WebQuests.'),
(1085,'The modelling of educational processes and their   operational support is a key aspect in the construction of more   effective e-learning applications. Instructional models are usually   described by means of an educational modelling language (EML). The   EML used can be one of the available standards (e.g. IMS Learning   Design), the customization of a standard to meet a specific   application profile, or even a domain-specific EML specifically   designed to better fit the very particular needs of a learning   scenario. In this paper we present <e-LD>, a general authoring and   operationalization architecture capable of dealing with all these   possibilities in a highly modular and flexible way. We also outline   a specific implementation of <e-LD> based on standard XML   technologies and workflow management systems, and we describe how   this implementation can be used to support IMS Learning   Design.'),
(1086,'This paper presents a project that provides both, to professors and to students, a tool that is useful for studying, teaching and learning how pipelines work and how they can be scheduled in an easy and widespread way. The project is called PipeSim, and features static and dynamic pipelines with a very attractive, dynamic and intuitive interface. It is well known that pipeline and pipeline-scheduling are very relevant concepts in computer science studies and it is very important that students can learn these in an easy and reliable way. The simulator makes easy both working in depth about pipeline scheduling and working slowly paying attention in the different stages of the scheduling. However, we designed the simulator knowing that principal users would be students with no experience, so both the execution and the presentation of the results have been carefully developed. In addition to this, to check the success of PipeSim, a survey has been made among some students that used the simulator. Results reveal that this kind of applications has a great acceptance among students, thought they consider that simulators are complements to the lessons given by the professor and never a substitute for them.'),
(1087,'The activities developed in this paper were aimed at providing an awareness of the elements that should be considered in quality learning objects instructional design for e-learning systems. We thus propose our own LO definition taking into account aggregation level number 2. On this basis, we analyze cognitive theories for promoting learning and we explain issues relating to the LO characteristics that help to improve their quality for suitable management. To achieve this we propose an instructional design based on an ontological model which explains the relationship between the instructional design elements and a specific classification to improve their management.'),
(1088,'<I>Educational Modeling   Languages</I> (EMLs) have been proposed to support the   modeling of educational units. Currently, there are some EML   proposals devoted to provide a computational base, enabling the   software processing and execution of educational units\' models. In   this context, flexibility is a key requirement in order to support   alternatives and changes . This paper presents a   <I>Perspective-oriented Educational Modeling Language</I> (PoEML)   that simplifies and facilitates the modeling of alternatives and the   performance of changes. The key point of the proposal is the   separation of the modeling in several concerns that can be managed   almost independently. As a result, changes at each concern can be   performed without affecting to other concerns, or affecting in   controlled ways.'),
(1090,'This paper describes the motivation, strategies,   and implementation details that lead to the creation of online   graduate-level degree programs in the Department of Electrical    Computer Engineering at the University of New Mexico. It also   presents some of the benefits as well as the challenges encountered   when designing and implementing these programs. The paper concludes   with a discussion of lessons learned and the future directions of   the program.'),
(1091,'Handheld devices are becoming more and more popular in education. Educational simulation and modelling are not new soil, but for handhelds they are still much under explored. Due to the difficulties teachers usually face in developing computer models and simulations and the lack of adequate tools for building them, we developed an authoring-tool for handheld educational simulation and modelling, called Sim-H (SIMulation for Handhelds). Sim-H is made by several modules each one relating to a type of simulation application that can be used in an educational context. One of these modules, that we describe thoroughly herein, is the Handheld Model Editor, a modelling tool for handheld devices that can be used to build models to use as such or as the core of educational handheld simulations.'),
(1092,'Collaborative learning environments require carefully crafted designs  both technical and social. This paper presents a model describing how to design socio-technical environments that will promote collaboration in group activities. A game was developed based on this model. This tool was used to conduct experiments for studying the collaborative learning process. Testing with this system revealed some strengths and weaknesses, which are being addressed in the on-going research.'),
(1093,'The Reusable Learning Objects Centre   for Excellence in Teaching and Learning (RLO-CETL) is a five-year   project (2005-2010) involving staff from three universities (London   Metropolitan, Cambridge University and the University of Nottingham)   in a collaborative programme of development, deployment and   evaluation of a range of multimedia learning objects that can be   stored in repositories, accessed over the Web, and integrated into   course delivery. One of the goals of the RLO-CETL is to provide   sustainable and reproducible processes that will allow sector-wide   collaboration, so as part of the internal formative evaluation of   the RLO-CETL, we are concerned to analyse its character, boundaries   and evolution, and how this develops in relation to individual and   institutional contexts, priorities, structures. In this paper, we   present some of the results of mapping tasks in which   twenty-eight participants (who included lecturers, tutors, students,   multimedia developers, administrators, evaluators and managers)   represented and talked about the networks of people with whom they   communicated. There are aspects of the maps that indicate how the   network of the RLO-CETL interacts and overlaps with institutional   and individual networks.'),
(1094,'Writing applications are currently designed for   desktop personal computers. Mobile devices like PDAs or smart phones   are increasingly being used for mobile applications such as access   to information sources or local work on the device, but they are   seldom used for collaborative tasks.  Here we present AULA and AWLA,   two applications that put mobile devices and collaborative   educational environments together inside and outside the   classroom. They are designed under the paradigm of collaborative   composition writing in language learning courses, in particular   English as a Foreign Language (EFL).'),
(1095,''),
(1096,'Mapping the specification IMS Learning Design   and the Course Management System Moodle is a logical step forward on   interoperability between eLearning systems and specifications in   order to increase the best acceptance of the specifications into the   widespread world of the eLearning systems and to ensure the   standardization of the outputs from the systems to be used in   others. IMS Learning Design and Moodle look for a common   understanding focused on the integration of information packages   modelled by each part in the other. The final goal aims at Moodle   playing an IMS LD package. A second step will map a Moodle course to   be used in any IMS LD complaint tool. The Unit of Learning in IMS LD   and the course in Moodle become the perfect couple where to find   several elements that should match each other. This paper shows how   to make this understanding, mapping related elements in both to get   a list of pairs easy to translate from one to another, and to define   also a list of requirements for this protocol.'),
(1097,''),
(1098,'Non-functional requirements such as performance,   program size, and energy consumption significantly affect the   quality of software systems. Small devices like PDAs and mobile   phones have little memory, slow processors, and energy   constraints. The C programming language has been the choice of many   programmers when developing application for small devices. On the   other hand, the need for functional software correctness has derived   several specification languages that adopt the Design by Contract   (DBC) technique. In this work we propose a specification language   for C, called CML (C Modeling Language), focused on non-functional   requirements. CML is inspired on the Design By Contract   technique. An additional contribution is a verification tool for   hard real-time systems. The tool is the first application developed   for CML. The practical usage of CML is presented through a case   study, which is a real application for a vehicle monitoring   system.'),
(1099,'Most automated reasoning tasks with prac tical   applications can be automatically reformulated into a constraint   solving task. A constraint programming platform can thus act as a   unique, underlying engine to be reused for mu ltiple automated   reasoning tasks in intelligent agents and systems. We identify six   key requirements for such platform: expressive task modeling   language, rapid solving method custom ization and combination,   adaptive solving method, user-friendly solution explanation,   efficient execution, and seamless integration within larger systems   and practical applications. We then propose a novel, model-driven,   component and rule-based architecture for such a platform that   better satisfies as a whole this set of requirements than those of   currently available platforms.'),
(1100,'Programming laws are a means of stating properties of programming con-structs and resoning about programs. Also, they can be viewed as a program transformation tool, being useful to restructure object-oriented programs. Usually the appli-cation of a programming law is only allowed under the satisfaction of side-conditions. In this work, we present how the conditions associated to object-oriented program-ming laws are checked by using Prolog. This is a step towards a tool that allows user definable refactorings based on the application of programming laws.'),
(1101,'Despite all the advances brought by LALR parsing   method by DeRemer in the late 60\'s, conflicts reported by LALR   parser generators are still removed in an old fashion and primitive   manner, based on analysis of a huge amount of textual and low-level   data stored on a single log file. For the purpose of minimizing the   effort and time consumed in LALR conflict removal, which is   definitely a laborious task, a methodology is proposed, along with   the set of operations necessary to its realization. We also present   a tool and the ideas behind it to support the methodology, plus its   plugin facility, which permits the interpretation of virtually any   syntax specification, regardless of the specification language   used.'),
(1102,'The technology that supports Aspect-Oriented   Programming (AOP) tools is inherently intrusive, since it changes   the behavior of base application code. Advice weaving performed by   AspectJ compilers must introduce crosscutting behavior defined in   advice into Java programs without causing great performance   overhead. This papershows the techniques applied by the   <I>ajc</I> and <I>abc</I> AspectJ compilers   for around advice weaving, and identifies problems in code they   produce. The problems analyzed are <I>advice and shadow   implementation repetition</I> and <I>context variable   repetition</I>. Performance gain provided by solving these   problems is discussed, showing that bytecodesize, running time and   memory consumption can be reduced by these optimizations. It is   assumed that the reader is familiar with AOP and AspectJ   constructs.'),
(1103,'This paper presents a visual language for   producing animated simulations. The language is implemented on a   tool called <I>Tabajara Animator</I>, using principles   of <I>Programming By Demonstration (PBD)</I>, which is a   technique for teaching the computer new behaviour by demonstrating   actions on concrete examples. The language is based on a formal   model for concurrent update of agents, which represent the animated   characters. The visual rules follow the \"before-after\" style,   adopted by the most important similar tools. New features discussed   by this work may produce a significant reduction on the number of   required rules for producing animated simulations. This paper shows   how these new features are implemented on a visual user-friendly   interface, and how they are translated into structures of the formal   model adopted.'),
(1104,'AspectLua is a Lua-based dynamic aspect-oriented   language that follows the original AspectJ concepts. It suffers from   the same problem of AspectJ-like languages with regard to   limitations in terms of aspect reusability, modularity and   heterogeneous interaction. In this paper we propose RE-AspectLua, a   new version of AspectLua that combines aspect interfaces with   abstract joinpoints and the use of a connection language, the Lua   language, to instantiate, at application composition time, the   abstract joinpoints. Thus, the connection language defines the   composition of reusable aspects with base code. Using these concepts   RE-AspectLua intends to break away from the syntactically manifest   coding of aspects in which joinpoints are hard-coded into aspects,   thereby promoting general reusability and the heterogeneous   composition of an aspect with different base codes. In order to   illustrate RE-AspectLua concepts we present two case studies.'),
(1105,'The spreadsheet metaphor has, over the years,   proved itself valuable for the definition and use of computations by   non-programmers. However, the computation model adopted in   commercial spreadsheets is still limited to non-recursive   computations and lacks abstraction mechanisms that would provide   modularization and better reuse (beyond copy and paste). We   investigate these problems by identifying a minimal set of   requirements for recursive computations, designing a   spreadsheet-based language with an abstraction definition mechanism,   prototyping an interpreter and evaluating it with   examples.'),
(1106,'Multi-processor systems have become the standard   in current computer architectures. Software developers have the   possibility to take advantage of the additional computing power   available to concurrent programs. This paper presents a way to   automatically use additional processors, by performing memory   management concurrently. A new architecture with little explicit   synchronization for concurrent lazy cyclic reference counting is   described. This architecture was implemented and preliminary   performance tests point at significant efficiency improvements over   the sequential counterpart.'),
(1107,'Reference Counting is the memory management   technique of most widespread use today. Very often applications   handle objects that are either permanent or get tenured. This paper   uses this information to make cyclic reference counting more   efficient.'),
(1108,'Scripting languages are used in conjuction with   C code in two ways: as extension languages, where the interpreter is   embedded as a library into an application; or as extensible   languages, where the interpreter loads C code as add-on   modules. These two scenarios share many similarities, as in both of   them two-way communication of code and data needs to take   place. However, the differences between them impose design tradeoffs   that affect the C API that bridges the two languages, often making a   scripting language more suitable for extending than embedding, or   vice-versa. This paper discusses how these tradeoffs are handled in   the APIs of popular scripting languages, and the impact on their use   as embedded or extensible languages.'),
(1109,'Program slicing is a well known family of techniques intended to identify and isolate code fragments which depend on, or are depended upon, specific program entities. This is particularly useful in the areas of reverse engineering, program understanding, testing and software maintenance. Most slicing methods, and corresponding tools, target either the imperative or the object oriented paradigms, where program slices are computed with respect to a variable or a program statement.</Pgt;  <P>Taking a complementary point of view, this paper focuses on the slicing of higher-order functional programs under a lazy evaluation strategy. A prototype of a Haskell slicer, built as proof-of-concept for these ideas, is also introduced.'),
(1110,'The type system of Haskell and some related   systems are based on an open world approach for overloading. In an   open world, the principal type of each overloaded symbol must be   explicitly annotated (in Haskell, annotations occur in type class   declarations) and a definition of an overloaded symbol is required   to exist only when overloading is resolved. In a closed world, on   the other hand, each principal type is determined according to the   types of definitions that exist in the relevant context and,   furthermore, overloading resolution for an expression considers only   the context of the definition of its constituent symbols. In this   paper we formally characterize open and closed worlds, and discuss   their relative advantages. We present a type system that supports   both approaches together, and compare the defined system with   Haskell type classes extended with multi-parameter type classes and   functional dependencies. We show in particular that functional   dependencies are not necessary in order to support multi-parameter   type classes, and present an alternative route.'),
(1111,'Object-Oriented Action Semantics is a semantic   framework for the definition of programming languages. The framework   incorporates some object-oriented concepts to the Action Semantics   formalism. Its main goal is to obtain more readable and reusable   semantic specifications. ObjectOriented Action Semantics provides   support for the definition of syntax-independent specifications, due   to the way its classes are written. In a previous work, a library of   classes (called LFL) was developed to improve specification reuse   and to provide a way to describe semantic concepts, independent from   the syntax of the programming language. This paper aims to address   some problematic aspects of LFL, and presents a case study, where a   specification is built by using the Visitor Pattern technique. The   use of this pattern allows a clear separation between the syntax of   a programming language and its different semantic   aspects.'),
(1112,''),
(1113,'The Semantic Web vision is among others to allow   automatic identification and selection of Web documents and services   to meet the requirements of users. In this work we provide a novel   solution that supports organizational knowledge flow utilizing   non-functional qualitative criteria for web service consumption. A   knowledge management web service selection mediator is presented   based on the Web Services resource framework (WSRF). It enhances   organizational best practices and promotes reusability of successful   services during the process of online Web Service selection. Apart   from meeting functionality requirements, the mediator utilizes   previous domain knowledge to base its decision upon Quality of Web   Service (QoWS) ontology knowledge. The proposed solution is open and   able to host a number of different selection policies and business   logic implementations. We present and experimentally evaluate and   compare four such selection policies. The design of the proposed   mechanism is analyzed and implementation details are   discussed. Evaluation results have shown that the solution is able   to satisfy cases and scenarios that have been derived while studying   and working on web services selection business processes for   different enterprises.'),
(1114,'Ontologies have been recognized as a fundamental   infrastructure for advanced approaches to Knowledge Management (KM)   automation in SOA. Building services communicate with each other by   exchanging self-contained messages. Depending on the specific   requirements of the business model they serve and the application   domain for which services were deployed, a number of mismatches   (i.e. sequence and cardinality of messages exchanges, structure and   format of messages and content semantics), can occur which prevent   interoperation among a prior compatible services. Existing   choreography technologies attempt to model such external visible   behavior. However, they lack the consistent semantic support   required to fully meet the necessities of heterogeneous KM   environments. This paper describes the ontology and grammar of   SOPHIE, a semantic service-based choreography framework for   overcoming conversational pattern mismatches in knowledge intensive   environments. Consequently, the paper provides an overview of the   framework that depicts its main building blocks, so a good   understantind of the ontology and grammar that summarize the   conceptual model is gained. Such ontology allows the desing and   description of fully fledged choreographies that can be used, as a   result of a mediation task, to produce the mediating structures that   in fact allow dynamic service-to-service interoperation. Finally, a   use case centred in the telcomunications field serves as proof of   concept of how SOPHIE is being applied.'),
(1115,'Knowledge management is characterized by many   different activities ranging from the elicitation of knowledge to   its storing, sharing, maintenance, usage and creation. Skill   management is one of such activities, with its own peculiarities, as   it focuses on full exploitation of knowledge individuals in an   organization have, in order to carry out at best given tasks. In   this paper a semantic-based automated Skill Management System is   proposed, which supports competences search and creation. The system   implements an approach exploiting the formalism and the reasoning   services provided by Description Logics. The approach embeds also   non standard Description Logics reasoning services to extend the set   of provided features. Here we present main characteristics of our   system and focus on a novel algorithm exploiting advanced inference   services for the one-to-one assignment of a set of individuals to a   set of tasks, endowed of logical explanation features for   missing/conflicting skills.'),
(1116,'Competence development programmes are   collections of units of learning and learning activities used to   increase the overall effective performance of a learner within a   certain task. The definition of a competence development programme   is fairly complex and subject to variability, depending on the   available learning units and components. Some instructional   engineering approaches have been successfully used to create   courseware by the combination of existing learning resources within   a systematic and iterative method. In this work, a generative,   model-driven engineering approach is used to create and adapt   competence development programmes from families of available   learning components, such as units of learning, learning designs,   and learning services. The process begins from the statement of the   learning goals as feature models, and carries out a number of   transformations from the analysis model down to learning designs and   implementation components. However, shared definitions for   competence-related terms and computational semantics are essential   in this effort. In this paper, ontologies are proposed as a means to   that end. In particular, the transformations between models are   defined with the help of a general competence ontology.'),
(1117,'We are approaching an era where research   materials will be stored more and more as digital resources on the   World Wide Web. This of course will enable easier access to online   publications. As the number of electronic publications expands, it   will, however, become a challenge for individuals to find related or   relevant papers. Related papers could be papers written by the same   team of authors or by one of the authors, or even papers that deal   with the same topic but were written by other authors. This, of   course, raises the issue of linking to papers forward in time, or as   we call it \"links into the future\". To be concrete, while reading a   paper written in the year 1980, it would be nice to know if the same   author has written another related paper in 1990s or if the same   author has written a paper earlier, all this without making an   explicit search. Based on the ascertained interest of a person   reading a particular paper from a digital repository, an   auto-suggestion facility could be useful to indicate papers in the   same area, category and subject that might potentially be of   interest to the reader. One is typically interested in finding   related papers by the same author or by one of the authors of a   paper. This feature can be implemented in two ways. The first is by   creating links from this paper to all the relevant papers and   updating it periodically for new papers appearing on the World Wide   Web. Another way is by going through the references of all papers   appearing on the WWW. Based on the references, one can create mutual   links to the papers that are referred to.</Pgt;  <P>In this paper, we focus on offering personalised services beyond standard global access. We explore means of identifying the relevance (or relatedness) of papers. A related paper can mean different things to different people as explained above. Ideally, related papers are found and made accessible using links into the future that could be customised to suit the needs of individual users. In this paper, we will focus on a subset of the problem. We explore links into the future in the context of a particular journal which has existed for the past 13 years with over 1500 published papers. We discuss problems that arise in this restricted context while providing details of partial implementations. We plan to pursue our ideas in a more general setting in future implementations.'),
(1118,'Classification-based reinforcement learning (RL)   methods have recently been pro-posed as an alternative to the   traditional value-function based methods. These methods use a   classifier to represent a policy, where the input (features) to the   classifier is the state and theoutput (class label) for that state   is the desired action. The reinforcement-learning community knows   that focusing on more important states can lead to improved   performance. In this paper,we investigate the idea of focused   learning in the context of classification-based RL. Specifically, we   define a useful notation of state importance, which we use to prove   rigorous bounds on policyloss. Furthermore, we show that a   classification-based RL agent may behave arbitrarily poorly if it   treats all states as equally important.'),
(1119,'Student satisfaction with distance learning is   impacted by a variety of factors, including interaction with the   instructor and the structure of the course.  Satisfaction with   distance-learning courses also has a strong impact on retention.  In   an earlier article, we determined that student satisfaction as   measured by course evaluation scores in an online discrete   mathematics course taught by the first author was not statistically   significantly different from that of students in traditional   versions of the same course, supporting some previous studies on   distance-learning student satisfaction.  However, the model of   distance-learning studied in our initial work is not the dominant   model used for distance learning at the institution in question.  In   this article we obtain statistically significant results different   from the earlier article when a distance-learning course that uses   the dominant model is considered. In particular, the course   evaluations for online and traditional sections of introductory Java   programming courses varied in some notable ways.'),
(1120,'In this paper we present an experience in the   extraction of named entities from Spanish texts using   stacking. Named Entity Extraction (NEE) is a subtask of Information   Extraction that involves the identification of groups of words that   make up the name of an entity, and the classification of these names   into a set of predefined categories. Our approach is corpus-based,   we use a re-trainable tagger generator to obtain a named entity   extractor from a set of tagged examples. The main contribution of   our work is that we obtain the systems needed in a stacking scheme   without making use of any additional training material or tagger   generators. Instead of it, we have generated the variability needed   in stacking by applying corpus transformation to the original   training corpus. Once we have several versions of the training   corpus we generate several extractors and combine them by means of a   machine learning algorithm. Experiments show that the combination of   corpus transformation and stacking improve the performance of the   tagger generator in this kind of natural language processing   applications. The best of our experiments achieves an improvement of   more than six percentual points respect to the predefined   baseline.'),
(1121,'Evolvable Hardware (EHW) is a new concept that   aims the application of evolutionary algorithms to hardware   design. EHW can adapt itself to unknown environment based on   features of the reconfigurable hardware. This paper presents   outlines of the idea of using some EHW agents in a distributed   system. These agents need to set up a self-organized communication   to achieve the predesigned goal. The experiment that is demonstrated   during the presentation, is to distribute a serial adder into two   EHW parts, where good results has been shown in a co-evolutionary   process.'),
(1122,'Recently, Particle Swarm Optimization (PSO)   algorithm has exhibited good performance across a wide range of   application problems. A quick review of the literature reveals that   research for solving the Quadratic Assignment Problem (QAP) using   PSO approach has not much been investigated. In this paper, we   design a hybrid meta-heuristic fuzzy scheme, called as variable   neighborhood fuzzy particle swarm algorithm (VNPSO), based on fuzzy   particle swarm optimization and variable neighborhood search to   solve the QAP. In the hybrid fuzzy scheme, the representations of   the position and velocity of the particles in the conventional PSO   is extended from the real vectors to fuzzy matrices. A new mapping   is introduced between the particles in the swarm and the problem   space in an efficient way. We also attempt to theoretically prove   that the variable neighborhood particle swarm algorithm converges   with a probability of 1 towards the global optimal. The performance   of the proposed approach is evaluated and compared with other four   different algorithms. Empirical results illustrate that the approach   can be applied for solving quadratic assignment problems   effectively.'),
(1123,'A genetic algorithm (GA) based recurrent fuzzy   neural network modeling method for dynamic nonlinear chemical   process is presented. The dynamic recurrent fuzzy neural network   (RFNN) is constructed in terms of Takagi-Sugeno fuzzy model. The   consequent part is comprised of the dynamic neurons with output   feedback. The number and the parameters of membership functions in   the premise part are optimized by the GA considering both the   approximation capability and structure complexity of RFNN. The   proposed dynamic model is applied to a PH neutralization process and   the advantages of the resulting model are demonstrated.'),
(1124,'It is a difficult problem that using cellular   neural network to make up of analog signal processing circuit. This   paper presented the architecture of new cellular neural network   SCCNN for analog signal processing circuits, designed the neural   cell circuit, and developed the evolutionary design method of the   SCCNN based on selfadapting genetic algorithm. In the architecture   of new cellular neural network SCCNN, each neural cell connects with   four neighborhood neural cells, the neural cell circuit and signal   transfer line between neural cells are controlled by programmable   switches. The validity of the SCCNN architecture and the   evolutionary design method are verified through digital   simulation. The experimental results indicate that the SCCNN   hardware is a universal cellular neural network for analog signal   processing circuit, which can be used to make up of the analog   signal amplifier, analog signal filter,  digit logic circuit, DAC   circuit and so on.'),
(1125,'The current computer forensics approaches mainly   focus on the network actions capture and analysis the evidences   after attacks, which always result in the static methods. Inspired   by the theory of artificial immune systems (AIS ), a novel model of   Computer Forensics System is presented. The concepts and formal   definitions of immune cells are given, and dynamically evaluative   equations for self, antigen, immune tolerance, mature-lymphocyte   lifecycle and immune memory are presented, and the hierarchical and   distributed management framework of the proposed model are   built. Furthermore, the idea of biology immunity is applied for   enhancing the self-adapting and self-learning ability to adapt   continuously variety environments. The experimental results show   that the proposed model has the features of real-time processing,   selfadaptively, thus providing a promising solution for computer   forensics.'),
(1126,'Mobile Ad hoc NETworks (MANETs) are   infrastructure-free, highly dynamic wireless networks, where central   administration or configuration by the user is very difficult. In   hardwired networks nodes usually rely on a centralized server and   use a dynamic host configuration protocol, like DHCP [Droms et   al. 2003], to acquire an IP address. Such a solution cannot be   deployed in MANETs due to the unavailability of any centralized DHCP   server. For small scale MANETs, it may be possible to allocate free   IP addresses manually. However, the procedure becomes impractical   for a large-scale or open system where mobile nodes are free to join   and leave. </Pgt;   <P>Most of the autoconfiguration algorithms proposed for ad   hoc networks are independent of the routing protocols and therefore,   generate a significant overhead. Using the genuine optimization of   the underlying routing protocol can significantly reduce the   autoconfiguration overhead. One of the MANET protocols which have   been promoted to experimental RFC is the OLSR routing protocol   [Jacquet et al. 2003], on which this article focuses. This article   aims at complementing the OLSR routing protocol specifications to   handle autoconfiguration. The corner stone of this autoconfiguration   protocol is an advanced duplicate address detection   algorithm.'),
(1127,'Numerous studies have shown the difficulty for a   single routing protocol to scale with respect to mobility and   network size in wireless ad hoc networks. This paper presents a   cluster-based extension of the DSR protocol called Cluster Source   Routing (CSR)1. The proposed approach improves the scalability of   DSR in high-density and low-mobility networks. The originality of   our proposal is an adaptive use of DSR and CSR routing modes   according to network density and node mobility in order to produce   less overhead and perform efficient routing. Indeed, adaptation is a   key feature for a routing protocol since network dynamics can   suddenly and widely change in wireless ad hoc networks. Thus, the   DSR-CSR protocol achieves enhanced performance over a broader   {network density, node mobility} domain as shown by simulations.'),
(1128,'Ad hoc wireless networks have enormous commercial and military potential becauseof their self-organizing capacity and mobility support. However, some specificities of these networks such as radio interferences and limited resources make more complex the quality of service(QoS) support. Indeed, on the one hand, the bandwidth offered to users is strongly affected by radio interferences. On the other hand, flooding information in such a network must be optimizedin order to save resources. Therefore, we propose in this paper, a solution taking into account radio interferences in mobile ad hoc network routing and optimizing flooding. This solution isbased on a modified version of the OLSR routing protocol that considers bandwidth requests and radio interferences in the route selection process while providing a very efficient flooding.A comparative performance evaluation based on NS simulations shows that despite the overhead due to QoS management, this solution outperforms classical OLSR in terms of QoS perceivedby the users (e.g. bandwidth amount granted to a flow and delivery rate). The efficiency of the optimized flooding is equal to that provided by the native version of OLSR.'),
(1129,'Clustering provides an effective mechanism for   energy-efficient data delivery in wireless sensor networks. To   reduce communication cost, most clustering algorithms rely on a   sensor\'s local properties in electing cluster heads. They often   result in unsatisfactory cluster formations, which may cause the   network to suffer from load imbalance or extra energy   consumption. In this paper, we propose a novel Voting-based   Clustering Algorithm (VCA) for energy-efficient data dissemination   in wireless sensor networks. This new approach lets sensors vote for   their neighbors to elect suitable cluster heads. VCA is completely   distributed, location-unaware and independent of network size and   topology. It combines load balancing, energy and topology   information together by using very simple voting   mechanisms. Simulation results show that VCA can reduce the number   of clusters by 5-25% and prolong the lifetime of a sensor network by   10-30% over that of existing energy-efficient clustering   protocols.'),
(1130,'Enhancing route request broadcasting protocols   constitutes a substantial part of recent research in mobile ad-hoc   network (MANET) routing. We suggest a novel approach to modify route   request broadcast based on node caching. The intuition behind node   caching is that the nodes involved in recent data packet forwarding   have more reliable information about its neighbors and have better   locations (e.g., on the intersection of several data routes) than   other nodes. We cache nodes which are recently involved in data   packet forwarding, and use only them to forward route   requests. Stopping forwarding route requests from the other nodes   considerably reduces routing overhead at the expense of possible   destination missing. The suggested node caching techniques can be   also viewed as a dynamic implementation of a connected dominating   set (CDS). We overcome the known drawback of CDS - overuse of   dominating (cached) nodes - by a new load-balancing scheme.</Pgt;  <P> Our contributions include: (i) a new energy-efficient node caching enhancement of route request broadcast for reactive ad hoc routing protocols; (ii) an extensive simulation study in NS2 of the novel node caching enhancement of AODV (AODV-NC) showing drastic reduction in overhead, significant improvement of the packet delivery ratio and the end-to-end delay and overhead; (iii) an analysis of the forwarding load distribution and energy consumption, and (iv) an extensive simulation study in NS2 of the novel AODV-NC based routing protocol with adaptive workload balancing (AODVNC-WLB) showing considerable improvement in throughput, overhead, delivery ratio and delay over the standard AODV for stressed MANETs.'),
(1131,''),
(1233,'From results of Ishihara it is known that the   weak (that is, binary) form of K&#246;nig\'s lemma (WKL) implies   Brouwer\'s fan theorem (Fan). Moreover, Berger and Ishihara [MLQ   2005] have shown that a weakened form WKL! of WKL, where as an   additional hypothesis it is required that in an effective sense   infinite paths are unique, is equivalent to Fan. The proof that WKL!   implies Fan is done explicitely. The other direction (Fan implies   WKL!) is far less directly proved; the emphasis is rather to provide   a fair number of equivalents to Fan, and to do the proofs   economically by giving a circle of implications. Here we give a   direct construction. Moreover, we go one step further and formalize   the equivalence proof (in the Minlog proof assistant). Since the   statements of both Fan and WKL!  have computational content, we can   automatically extract terms from the two proofs. It turns out that   these terms express in a rather perspicuous way the informal   constructions.'),
(1234,'Exploratory data analysis over foreign language   text presents virtually untapped opportunity. This work incorporates   Nave Bayes classifier with Case-Based Reasoning in order to   classify and analyze Arabic texts related to fanaticism. The Arabic   vocabularies are converted to equivalent English words using   conceptual hierarchy structure. The understanding process operates   at two phases. At the first phase, a discrimination network of   multiple questions is used to retrieve explanatory knowledge   structures each of which gives an interpretation of a text according   to a particular aspect of fanaticism. Explanation structures   organize past documents of fanatic content. Similar documents are   retrieved to generate additional valuable information about the new   document.  In the second phase, the document classification process   based on Na&#239;ve Bayes is used to classify documents into their   fanatic class. The results show that the classification accuracy is   improved by incorporating the explanation patterns with the Nave   Bayes classifier.'),
(1235,'In this paper we present our aims in the mobiDI&#193;AK (i.e. mobile STUDENT) portal that we have been developed and are running. Our essential goal was to create a portal engine, that is: (1) self-organizing, i.e. it has built in mechanisms by means of that it is able to exploit the power of the user community around the portal on behalf of its continuous growth; (2) for educational purposes; (3) mobile, i.e. its main services are also accessible from mobile devices.</P>  <P>We have developed the portal engine ourselves that serves the basis to the portal. We are running the portal hourly at the Faculty of Informatics at the University of Debrecen, Hungary.</P>  <P>We give an overview of the main portal services and also of the content offered by the running portal.'),
(1236,'IMS Learning Design (IMS LD) is an interoperable   and standardized language that enables the computational   representation of Units of Learning (UoLs). However, its adoption   and extensive use in real practice largely depends on the extent to   which teachers can design and author their own UoLs according to the   requirements of their educational situations. Many of the proposed   design processes for facilitating the creation of UoLs are based on   the reuse of complete or non-complete learning design solutions at   different levels of granularity. This paper introduces a comparison   framework that conceptually analyzes and classifies reusable   learning design solutions and processes that drive the creation of   ready-to-run UoLs. The framework provides a comprehensible   representation of such processes and units of reuse over two   dimensions, namely granularity and completeness. It also offers a   frame for discussing issues, such as the proper level of reuse, of   existing and forthcoming proposals. Finally, it opens the path to   other strands for future research such as providing language   independence of learning designs or proposing approaches for the   selection of the reusable solutions.'),
(1237,'Publication bias is the tendency for investigations with primarily nonstatistically significant findings to be withheld from the research record. Because publication bias has serious negative consequences for research and practice, we gathered information about the prevalence and predictors of publication bias in the computer science education literature. From an initial random sample of 352 recent computer science education articles, we reviewed the 38 empirical articles that used inferential statistical analyses. We found that (a) the proportion of articles reporting primarily statistically significant findings in computer science education was very similar to the proportion in medical research, (b) that an article\'s having a female first author was a strong predictor of an article\'s having primarily statistically significant results, and (c) that there was a tendency for authors to emphasize statistically significant findings and deemphasize nonstatistically significant findings. Neither whether an investigation was reported in a journal or conference proceeding nor whether the source of funding was disclosed were significant predictors of an article\'s having statistically positive results.'),
(1238,'The vision of global agent-based e-commerce   environments that enable dy-namic trading between business partners   requires the study and development of suitable formal modeling   frameworks. In particular, negotiation is a necessary and   importantactivity to allow engagement of business parties in   non-trivial business relationships. In this paper we propose a   formal framework based on <I>finite state process   algebra</I> formodeling and analysis of interaction protocols   in agent-based negotiations. The approach is demonstrated by   applying the framework to model agent interactions in asingle-item   English auction scenario.'),
(1239,'Some measures such as mean average precision and   recall level precision are considered as good system-oriented   measures, because they concern both precision and recall that are   two important aspects for effectiveness evaluation of information   retrieval systems. However, such good system-oriented measures   suffer from some shortcomings when partial relevance judgments are   used. In this paper, we discuss how to rank retrieval systems in the   condition of partial relevance judgments, which is common in major   retrieval evaluation events such as TREC conferences and NTCIR   workshops. Four system-oriented measures, which are mean average   precision, recall level precision, normalized discount cumulative   gain, and normalized average precision over all documents, are   discussed. Our investigation shows that averaging values over a set   of queries may not be the most reliable approach to rank a group of   retrieval systems. Some alternatives such as Borda count, Condorcet   voting, and the Zero-one normalization method, are   investigated. Experimental results are also presented for the   evaluation of these methods.'),
(1240,'Multi-Agent Systems (MAS) constitute a well known    approach in modelling dynamical real world systems. Recently, this    technology has been applied to Wireless Communication Systems    (WCS), where efficient resource allocation is a primary goal, for    modelling the physical entities involved, like Base Stations (BS),    service providers and network operators. This paper presents a    novel approach in applying MAS methodology to WCS resource    allocation by modelling more abstract entities involved in WCS    operation, and especially the concurrent network procedures    (services). Due to the concurrent nature of a WCS, MAS technology    presents a suitable modelling solution. Services such as new call    admission, handoff, user movement and call termination are    independent to one another and may occur at the same time for many    different users in the network. Thus, the required network    procedures for supporting the above services act autonomously,    interact with the network environment (gather information such as    interference conditions), take decisions (e.g. call establishment),    etc, and can be modelled as agents. Based on this novel simulation    approach, the agent cooperation in terms of negotiation and    agreement becomes a critical issue. To this end, two negotiation    strategies are presented and evaluated in this research effort and    among them the distributed negotiation and communication scheme    between network agents is presented to be highly efficient in terms    of network performance. The multi-agent concept adapted to the    concurrent nature of large scale WCS is, also, discussed in this    paper.'),
(1241,'Supply chain management (SCM) deals with   planning and coordinating activities such as material procurement,   product assembly, and the distribution of manufactured   products. This paper offers an agent-based solution as a potentially   adequate approach for the automation of supply chain management. The   greatest obstacle in SCM research is obtaining benchmark designed   solutions since it is difficult to simulate real business   environments, while live testing in real-world systems is not an   option. The Trading Agent Competition Supply Chain Management (TAC   SCM) scenario provides a unique testbed for studying and prototyping   SCM agents by providing a challenging game environment where   competing agents engage in complex decision-making activities with   the purpose of maximizing their profit. In this paper, we describe   the TAC SCM environment and present the main features of the   CrocodileAgent, our TAC SCM 2007 entry. Additionally, the   CrocodileAgent?s performance in the competition, as well as in a   series of controlled experiments, is discussed.'),
(1242,'A central characteristic of ad hoc mobile networks is the frequent changes of their topology.  This is the source of many problems that need to be solved. AODV is an on-demand routing protocol for decreasing maintenance overhead in ad hoc networks. However, some path breaks can cause significant overhead and transmission delays. If the maintenance overhead of the ro uting table can be reduced, table -driven routing methods could be an efficient substitution.  In this paper , we propose a knowledge discovery agent for an effective routing method that uses simple bit -map topology information.  The agent node gathers topology knowledge and creates topology bit -map information.  All available paths from a source to a destination can easily be calculate d using the bit-map. All the nodes in the network maintain the bit-map distributed from the agent, and use it for the source of routing. The performance and the correctness of the proposed agent method is verified by computer simulations.'),
(1243,'This paper is dedicated to the issue of   structural performance of multi-agent platforms. Due to the wide   range of all available architectures, we have concentrated only on   Java RMI implementations. The main goal of this paper consists of   two parts. The first one is to investigate and develop the   performance metrics to enable evaluation of distributed systems   without reorganization of the running system. The second part is the   programming verification of two considered Java RMI multi-agent   solutions: Aglets and Jade. We have examined the defined metrics in   many experiments with different network and environment   configurations to provide experimental evidence that these metrics   are adequate in variety of conditions.'),
(1244,'Query transformation is a serious hurdle on semantic peer-to-peer environment. Forinteroperability between peers, queries sent from a source peer have to be efficiently transformed to be understandable to potential peers processing the queries. However, the problem is that thetransformed queries might lose some information from the original one, as continuously traveling along peer-to-peer networks. We mainly consider two factors; i) number of transformations and ii) quality of ontology alignment. In this paper, we propose a new measurement of semanticcentrality, i.e., the power of semantic bridging on semantic peer-to-peer environment. Thereby, we want to build semantically cohesive user subgroups, so that semantic affinities between peerscan be computed. Then, given a query, we find out a path of peers for optimal interoperability between a source peer and a target one, i.e., minimizing information loss by the transformation.We have shown an example for retrieving image resources annotated on peer-to-peer environment by using query transformation based on semantic centrality.'),
(1245,'We propose specification of schema mappings and   agents\' actions in XML data integration task. We discuss the problem   in a highly-dynamic environment consisting of a community of   peer-to-peer cooperating partners (agents). Peers decide how to   describe their local data, when to join and when to leave the   system, how to communicate and share their information with   partners. An agent responds to the query by asking its partners   (friends), which are able to partly answer the query. All the   answers are merged and final result is constructed. A peer   propagates a query along semantic paths existing in the   system. Semantic paths are determined by schema mappings defined   between partners. We propose a method for specifying schema mappings   and to translate them to XQuery expressions. Mappings are   represented by means of logical formulas. We also propose a   declarative specification of semantic-driven communication in the   system. The specification is made in a peer-oriented extension of   Prolog.'),
(1246,''),
(1247,'In recent years, market forecasting by machine learning methods has been flourishing.Most existing works use a past market data set, because they assume that each trader\'s individual decisions do not affect market prices at all. Meanwhile, there have been attempts to analyzeeconomic phenomena by constructing virtual market simulators, in which human and artificial traders really make trades. Since prices in a market are, in fact, determined by every trader\'sdecisions, a virtual market is more realistic, and the above assumption does not apply. In this work, we design several reinforcement learners on the futures market simulator U-Mart (UnrealMarket as an Artificial Research Testbed) and compare our learners with the previous champions of U-Mart competitions empirically.'),
(1248,'Abstract: Prediction of the quality attributes   of software architectures requires technologies that enable the   application of analyt ic theories to component mode ls. However,   available analytic techniques generally opera te on formal models   specified in notations that cannot flexibly and intuitively capture   the architectures of large- scale distributed system s. The   construction of model interpreters that transform architectural m   odels into analysis mode ls has proved to be a complex and difficult   task. This paper (1) de scribes a methodology for performing   automated analysis of architectural models that simplifies the   development of model interpreters and enables effective reuse of   interpreter logic, an d (2) demonstrates how a framework that   utilizes the methodology can be designed, implemented, utilized, and   evaluated.'),
(1249,'The mining of generic software components from legacy systems can be used as an auxiliary technique to revitalize systems. This paper presents a software maintenance approach that uses such technique to revitalize one or more embedded legacy systems simultaneously and, in addition, create a core of reusable assets that can be used to support the development of new similar products. Software Product Line techniques are used to support the tasks of domain modelling and software component development. A real case study in the domain of Point of Sale (POS) terminals is presented and it illustrates the use of the proposed approach to revitalize three similar embedded legacy systems, simultaneously. It also shows how it is possible, through the created core of reusable assets, to deliver variations of these systems to meet the requirements of a wide family of POS terminals with different hardware configurations.'),
(1250,'In a component-based development process the   selection of components is an activity that takes place over   multiple lifecycle phases that span from requirement specifications   through design to implementation and integration. In different   phases, different assumptions are valid and different granularity of   information is available, which has a consequence that different   procedure should be used in the selection process and an automated   tool support for an optimized component selection would be very   helpful in each phase. In this paper we analyze the assumptions and   propose the selection procedure in the requirements phase. The   selection criterion is based on cost minimization of the whole   system while assuring a certain degree of satisfaction of the system   requirements that can be considered before designing the whole   architecture. For the selection and optimization procedure we have   adopted the DEER (DEcision support for componEnt-based softwaRe)   framework, previously developed to be used in the selection process   in the design phase. The output of DEER indicates the optimal   combination of single COTS (Commercial-Off-The-Shelf) components and   assemblies of COTS that satisfy the requirements while minimizing   costs. In a case study we illustrate the selection and optimization   procedure and an analysis of the model sensitivity to changes in the   requirements.'),
(1251,'Nowadays software systems are essential to the   environment of most organizations, and their maintenance is a key   point to support business dynamics. Thus, reverse engineering legacy   systems for knowledge reuse has become a major concern in software   industry. This article, based on a survey about reverse engineering   tools, discusses a set of functional and non-functional requirements   for an effective tool for reverse engineering, and observes that   current tools only partly support these requirements. In addition,   we define new requirements, based on our group?s experience and   industry feedback, and present the architecture and implementation   of LIFT: a Legacy InFormation retrieval Tool, developed based on   these demands. Furthermore, we discuss the compliance of LIFT with   the defined requirements. Finally, we applied the LIFT in a reverse   engineering project of a 210KLOC NATURAL/ADABAS system of a   financial institution and analyzed its effectiveness and   scalability, comparing data with previous similar projects performed   by the same institution.'),
(1252,'Architectural mismatches are a recognized obstacle to successful software reuse. An architectural mismatch occurs when two or more software components are connected to form a system and those components make differing and incompatible assumptions about their interactions or the environment in which they exist.</P>  <P>Mismatch detection and avoidance has been previously discussed in existing literature. These typically take the form of generic rules and guidelines. Service Oriented Architectures (SOA) are becoming one of the main trends in the current engineering of software. Using web services, as defined by W3C Web Services Architecture Working Group, supports the engineering of SOA by providing rules and restrictions that apply to the definition of web services and how they can interact with other components to form a larger system. We see this as an opportunity to define a web services style with corresponding rules to avoid the introduction of architectural mismatches at design time.</P>  <P>In this paper we describe the development of an environment which supports SOA development by enabling their description, as well as facilitating the detection of potential mismatches between web services. Here we define a web services style in the architectural description language ACME  Armani, and present the environment that we developed in ACME Studio using our web services style definition. This is accompanied by a small case study illustrating the use of our environment.'),
(1253,'Nowadays, the complexity of software   applications has brought new challenges to developers, having to   deal with a large number of computational requirements. Among these   requirements, those known as crosscutting concerns transpass   components boundaries, leading to maintainability and comprehension   problems. This paper presents CrossMDA, a framework that encompasses   a transformation process to integrate crosscutting concerns in   model-oriented systems. It uses the concepts of horizontal   separationof concerns from AOPto create independent business and   aspect models, integrating those models throughMDA transformations   (vertical separation of concerns). CrossMDA comprises a development   process, a set of services and support tools. The main advantages of   this approach are to raise the abstraction level of aspect modeling,   to promote the reuse of crosscutting concerns modeled as PIM   elements, besides automating the process of mapping the relationship   of crosscutting concerns and business models through the process of   MDA transformations.'),
(1254,'In this paper, we present a model-based tool for product derivation. Our tool is centered on the definition of three models (feature, architecture and configuration models) which enable the automatic instantiation of software product lines (SPLs) or frameworks. The Eclipse platform and EMF technology are used as the base for the implementation of our tool. A set of specific Java annotations are also defined to allow generating automatically many of our models based on existing implementations of SPL architectures. We illustrated the use and validation of our tool in the preparation of the automatic derivation of the JUnit framework and a J2ME games product line.'),
(1255,'Web applications are widely disseminated, but,   traditional development methods for this type of application still   require a substantial amount of new modeling and   programming. Current methods do not take significant advantage of   reuse techniques, such as software product lines (PL). This paper   presents the WIDE-PL environment focusing on its application   generation process, called Application DEvelopment based on Services   - ADESE. This environment is an evolution of WIDE - Waterloo   Informatics Development Environment. The WIDE-PL environment   supports the generation of Web applications based on the   Service-oriented Architecture (SOA) and the product line   approach. Our solution encompasses a general software architecture,   an application generation process, and a set of mandatory and   optional services. Examples of applications from two different   domains were developed using ADESE to evaluate its feasibility. The   results show that the process offers several advantages including an   increase in reuse and an explicit separation between the services   and the business process connecting those services.'),
(1256,''),
(1263,'In this paper we study various properties of   complements of sets and the Efremovi&#269; separation property in a   symmetric pre--apartness space.'),
(1264,'In this paper we perform a rigorous study of the H&eacute;non map. We prove with computer assistance the existence of symbolic dynamics for h<sup>2</sup> and h<sup>7</sup> and the existence of periodic orbits of all periods but 3 and 5.'),
(1265,'Implementing binary methods in traditional   object-oriented languages isdifficult. Numerous problems arise   regarding the relationship between types and classes in the context   of inheritance, or the need for privileged access to the internal   repre-sentation of objects. Most of these problems occur in the   context of statically typed languages that lack multi-methods   (polymorphism on multiple arguments). The pur-pose of this paper is   twofold: to show why some of these problems are either non-issues,   or easily solved in Common Lisp, and to demonstrate how the Common   Lisp ObjectSystem (Clos) allows us to simply define, implement and   enforce type-safe binary methods. These last considerations involve   re-programming a binary method-specificobject system through the   Clos Meta-Object Protocol (Mop).'),
(1266,'We describe in this paper the implementation and   use of custom specializers in two current dialects of Lisp: Skill   and Common Lisp. We motivate the need for such specializers by   appealing to clarity of expression, referring to experience in   existing industrial applications. We discuss the implementation   details of such user-defined specializers in both dialects of Lisp,   detailing open problems with those implementations, and we sketch   ideas for solving them.'),
(1267,'Presentation types are used in the CLIM   interface library to tag graphical output with a type and establish   an input type context in which the user may use the keyboard to type   input, accepted by a parser associated with that presentation type,   or click on the graphical representation of an object that has an   appropriate presentation type. Presentation types are defined using   a syntax reminiscent of the deftype syntax of Common Lisp; the input   and output actions of the types, as well as aspects of their   inheritance, are implemented using a system of generic functions and   methods directly based on CLOS. The presentation type system is   different enough from the Common Lisp type system that its types,   generic functions and methods do not map directly to those of Common   Lisp. We describe the presentation type implemention in McCLIM which   uses the CLOS Metaobject Protocol to implement presentation type   inheritance, method dispatch and method combination without   implementing an entire parallel object system next to CLOS. Our   implementation supports all types of method combination in the   presentation methods, including user-defined method   combination.'),
(1268,'In this paper we present AmOS, the Ambient   Object System that underlies the Ambience programming language. AmOS   implements a computation model that supports highly dynamic   behaviour adaptation to changing contexts. Apart from being purely   object-based, AmOS features first-class closures, multimethods and   contexts. Dynamic method scoping through a subjective dispatch   mechanism is at the heart of our approach. These features make of   AmOS a very simple and elegant paradigm for context-oriented   programming.'),
(1269,'UCL-GLORP is a Common Lisp implementation and   extension of GLORP (Generic Lightweight Object-Relational   Persistence), an Object-Relational Mapper for the Smalltalk   language. UCL-GLORP is now a mature framework that largely extends   GLORP and that takes advantage of some of Common Lisp unique   features. This paper illustrates UCL-GLORP and discusses some of the   challenges that we faced in order to find suitable replacements, in   Common Lisp, for some of the more esoteric features of Smalltalk   that were explored by GLORP.'),
(1270,'Reasoning about incomplete qualitative temporal   information is an essential topic inmany artificial intelligence and   natural language processing applications. In the domain of natural   language processing for instance, the temporal analysis of a text   yields a set of temporal relationsbetween events in a given   linguistic theory. The problem is first to express events and any   possible temporal relations between them, then to express the   qualitative temporal constraints (as subsetsof the set of all   possible temporal relations) and compute (or count) all possible   temporal relations that can be deduced. For this purpose, we propose   to use the formalism of S-languages, based onthe mathematical notion   of S-arrangements with repetitions [Schwer, 2002]. In this paper, we   present this formalism in detail and our implementation of it. We   explain why Lisp is adequateto implement this theory. Next we   describe a Common Lisp system SLS (for S-LanguageS)which implements   part of this formalism. A graphical interface written using McCLIM,   the free implementation of the CLIM specification, frees the   potential user of any Lisp knowledge. Fullydeveloped examples   illustrate both the theory and the implementation.'),
(1271,''),
(1272,'NASDAQ Market Velocity and Market Forces are two   relatively new data products that attempt to capture market   sentiment, something that was previously only observable if one was   on a trading floor. Given the transient and temporal properties of   the data, we were challenged to create a visualization that would   highlight the ever-changing qualities of Velocity and Forces. To   that end, we developed FireStox, a web application that provides   unified representation and filtering solutions to help market   researchers observe the behavior of these metrics for one or many   companies throughout the course of a trading day.'),
(1273,'Shared-workspace groupware has not become common   in the workplace, despite many positive results from research   labs. One reason for this lack of success is that most shared   workspace systems are designed around the idea of planned, formal   collaboration sessions  yet much of the collaboration that occurs   in a co-located work group is informal and opportunistic. To support   informal collaboration, groupware must be designed and built   differently. We introduce the idea of <I>community-based groupware   (CBG)</I>, in which groupware is organized around groups of people   working independently, rather than shared applications, documents,   or virtual places. Community-based groupware provides support for   three things that are fundamental to informal collaboration:   awareness of others and their individual work, lightweight means for   initiating interactions, and the ability to move into   closely-coupled collaboration when necessary. We demonstrate three   prototypes that illustrate the ideas behind CBG, and argue that this   way of organizing groupware supports informal collaboration better   than other existing approaches.'),
(1274,'The design of the groupware systems is a   progressively extended task, which is difficult to tackle. There are   not proposals to support the joint modeling of collaborative and   interactive issues of this kind of systems, that is, proposals that   allow designing the presentation layer of these applications. In   order to solve this lack we propose a methodological approach, based   on a set of notations of both a graphical and a textual nature.'),
(1275,'The advent of new advances in mobile computing   has changed the manner we do our daily work, even enabling us to   perform collaborative activities. However, current groupware   approaches do not offer an integrating and efficient solution that   jointly tackles the flexibility and heterogeneity inherent to   mobility as well as the <I>awareness</I> aspects intrinsic to collaborative   environments. Issues related to the diversity of contexts of use are   collected under the term <I>plasticity</I>. A great amount of tools have   emerged offering a solution to some of these issues, although always   focused on individual scenarios. We are working on reusing and   specializing some already existing plasticity tools to the groupware   design. The aim is to offer the benefits from plasticity and   awareness jointly, trying to reach a real collaboration and a deeper   understanding of multi-environment groupware scenarios. In   particular, this paper presents a conceptual framework aimed at   being a reference for the generation of plastic User Interfaces for   collaborative environments in a systematic and comprehensive   way. Starting from a previous conceptual framework for individual   environments, inspired on the <I>model-based approach</I>, we introduce   specific components and considerations related to groupware.'),
(1276,'Most of the current academic and professional   work requires collaboration between the members of a working   group. Groupware tools play a prevailing role in supporting this   collaborative work, often from different locations and at the same   time. The research field of CSCW (Computer-Supported Cooperative   Work) studies how to design effective groupware tools. To increase   their potential, groupware systems must be flexible and have the   capacity to adapt themselves to multiple tasks and situations. In   order to provide answers to these challenges, in this article we   propose the use of meta-models and XML-based languages to specify   the most important characteristics of a groupware modeling system,   such as the application domain, the requirements of the tasks to be   carried out, how communication takes place and the regulation of the   shared workspace. These models and techniques have been used to   develop a specific groupware system called SPACE-DESIGN   (SPecification and Automatic Construction of collaborative   Environments of DESIGN), a CSCW tool with support for synchronous   distributed collaborative work that adapts and re-configures itself   as a result of processing the domain specification, the task, the   communication and the system working norms.'),
(1277,'Currently, many models are used to capture   functional software requirements. However, the Software Engineering   community has faded interaction requirements into the background,   dealing with interface mainly in design time. A sound MDA-compliant   software development methodology, called OO-Method, is extended in   this work to bridge this gap. The issue is to define a methodology   for capturing interaction requirements. For this purpose, the formal   notation ConcurTaskTrees (CTT) is used. This notation is a technique   that is well-known in the Human Computer Interaction community. A   set of interaction patterns has been defined to build CTT   models. These patterns are defined with a very precise syntax and   semantics. Moreover, transformation rules are defined to transform   the Task Model into the OO-Method Presentation Model, which   specifies the user interface in an abstract and platform-independent   way. However, since editing the CTT models is hard work, this paper   proposes superimposing a layer to the CTT diagram in order to   capture interaction requirements using sketches. CTT models will be   synchronously generated from these sketches. Because this   transformation is \'transparent\' to the analyst, he only needs to   draw the sketches during the interaction requirements   elicitation. The approach presented in this paper is instantiated   for the environment of the OLIVA<B>NOVA</B>   technology. This environment makes it possible to obtain a final   software product from its corresponding Conceptual Model through a   Model Compilation process, where interaction modeling is properly   embedded with the most conventional data and process   modeling.'),
(1278,'Although usability evaluations have been focused   on assessing different contexts of use, no proper specifications   have been addressed towards the particular environment of academic   websites in the Spanish-speaking context of use.Considering that   this context involves hundreds of millions of potential users, the   AIPO Association is running the UsabAIPO Project. The ultimate goal   is to promote an adequate translation of international standards,   methods and ideal values related to usability in order to adapt them   to diverse Spanish-related contexts of use. This article presents   the main statistical results coming from the Second and Third Stages   of the UsabAIPO Project, where the UsabAIPO Heuristic method (based   on Heuristic Evaluation techniques) and seven Cognitive Walkthroughs   were performed over 69 university websites. The planning and   execution of the UsabAIPO Heuristic method and the Cognitive   Walkthroughs, the definition of two usability metrics, as well as   the outline of the UsabAIPO Heuristic Management System prototype   are also sketched.'),
(1279,'This paper analyses the requirements of   automation and adaptation in the so called perceptive   environments. These environments are places with the ability of   perceiving the context through sensors and other   mechanisms. Focusing on personal/home environments, we present a   first approach and prototype to semi-automatic adaptation of   Perceptive Environments through a system of rule-based, configurable   and modular agents, which are able to explain their behaviors and to   adapt to the changing habits of the users. This prototype has been   implemented over a real environment: a living room equipped with   ambient intelligence capabilities. The core of the system relies on   a set of modular agents equipped with rules. Those rules are   composed of triggers, conditions and actions that enable them to   express desired behaviors of the environment as well as to infer   high-level context from low level context. One of the main   objectives of the system is to leverage the control of the user over   his/her own environment, making it easy to create powerful and   personal behaviors without expert assistance. In this sense this   work follows Greenberg?s thought of making \"simple ideas simple to   be done\" [Greenberg 07].'),
(1280,'So far, the Ambient Intelligence (AmI) paradigm has been applied to the development of a great variety of real systems. They use advanced technologies such as ubiquitous computing, natural interaction and active spaces, which become part of social environments. In the design of AmI systems, the inherent collaboration among users (with the purpose of achieving common goals) is usually represented and treated in an ad-hoc manner. However, the development of this kind of systems can take advantage of rich design models which embrace concepts in the domain of collaborative systems in order to provide the adequate support for explicit or implicit collaboration. Thereby, relevant requirements to be satisfied, such as an effective coordination of human activities by means of task scheduling, demand to dynamically manage and provide group- and context-awareness information. This paper addresses the integration of both proactive and collaborative aspects into a unique design model for the development of AmI systems; in particular, the proposal has been applied to a learning system. Furthermore, the implementation of this system is based on a blackboard- based architecture, which provides a well-defined high-level interface to the physical layer.'),
(1281,'The development, analysis and follow-up of the   processes designed to assure the usability and accessibility of   websites is a tedious work for the moderator or evaluator. This is   why there is a necessity for tools that can automate the processes.   There are some tools that cover some of the technical aspects, but   no tools have been identified that can tackle the process as a   whole. For these reasons the Aragonese Usability Laboratory decided   to develop WebA (Web Analysis), with the objective to have a   complete application. This application is designed through modules   with the objective of covering all of the evaluation and analysis   phases, and it concludes with a process management module. For the   analysis phase a Card Sorting module (open  closed) has been   developed that uses the hierarchical and multidimensional cluster   analysis, which allows a better information architecture. For the   evaluation phases, modules have been developed that allow the   semiautomatic evaluation of usability through user satisfaction   tests based on Nielsen heuristics and ISO standards, the evaluation   of accessibility through the verification of WCAG 1.0   guidelines. The application concludes with a process design and   evaluation management module, and modules that automatically   generate the reports of the analysis carried out.'),
(1282,''),
(1283,'The WWW is currently experiencing a   revolutionary growth due to numerous emerging tools, techniques and   concepts. Digital journals thus need to transform themselves to cope   with this evolution of the web. With their growing information size   and access, conventional techniques for managing a journal and   supporting authors and readers are becoming insufficient. Journals   of the future need to provide innovative administrative tools in   helping its managers to ensure quality. They also need to provide   better facilities for assisting authors and readers in making   decisions regarding their submission of papers and in providing   novel navigational features for finding relevant publications and   collaborators in particular areas of interest. In this paper, we   explore an innovative solution to address these problems by using an   emerging Web 2.0 technology. We explore the application of mash-ups   for J.UCS - the Journal of Universal Computer Science and encourage   readers and authors to try out the applications (see section 11   Conclusions).  J.UCS can then serve as a model for contemporary   electronic journals.'),
(1284,'Feature selection methods are often applied in the context of document classification. They are particularly important for processing large data sets that may contain millions of documents and are typically represented by a large number, possibly tens of thousands of features. Processing large data sets thus raises the issue of computational resources and we often have to find the right trade-off between the size of the feature set and the number of training data that we can taken into account. Furthermore, depending on the selected classification technique, different feature selection methods require different optimization approaches, raising the issue of compatibility between the two. We demonstrate an effective classifier training and feature selection method that is suitable for large data collections. We explore feature selection based on the weights obtained from linear classifiers themselves, trained on a subset of training documents. While most feature weighting schemes score individual features independently from each other, the weights of linear classifiers incorporate the relative importance of a feature for classification as observed for a given subset of documents thus taking the feature dependence into account. We investigate how these feature selection methods combine with various learning algorithms. Our experiments include a comparative analysis of three learning algorithms: Na&#239;ve Bayes, Perceptron, and Support Vector Machines (SVM) in combination with three feature weighting methods: Odds ratio, Information Gain, and weights from the linear SVM and Perceptron. We show that by regulating the size of the feature space (and thus the sparsity of the resulting vector representation of the documents) using an effective feature scoring, like linear SVM, we need only a half or even a quarter of the computer memory to train a classifier of almost the same quality as the one obtained from the complete data set. Feature selection using weights from the linear SVMs yields a better classification performance than other feature weighting methods when combined with the three learning algorithms. The results support the conjecture that it is the sophistication of the feature weighting method rather than its compatibility with the learning algorithm that improves the classification performance.'),
(1285,'In communication systems that guarantee seamless   mobility of users across service areas, repeated attempts occur as a   result of user behavior but also as au- tomatic retries of blocked   requests. Both phenomena play an important role in the system   performance and therefore should not be ignored in its analysis. On   the other hand, an exact Markovian model analysis of such systems   has proven to be infeasible and resorting to approximate techniques   is mandatory. We propose an approximate methodology which   substantially improves the accuracy of existing methods with a   negligible increase of the computational time from the human point   of view. A numer- ical evaluation of the model is carried out to   investigate the impact on performance of the parameters related to   the retry phenomena. As a result, some useful guidelines for setting   up the automatic retries are provided. Finally, we also show how our   model can be used to obtain a tight performance approximation in the   case where reattempts have a deterministic nature.'),
(1286,'Communication processes have become increasingly   important in modern working life. Organizations invest a   surprisingly high amount of financial resources and employee work   time in both face-to-face and virtual meetings, yet this investment   often produces poor results. To overcome this problem, research on   technology-based support over a meeting?s life-cycle has been   increasingly conducted in recent decades. As a result of this   research, particular interest has emerged in meeting information   systems, which may include technology-enhanced meeting rooms as well   as tools for multi-modal meeting recording, automatic meeting   information extraction and annotation, in-meeting support, meeting   information archiving, indexing, retrieval and   visualization. Despite this great interest in and research activity   on meeting information systems, insufficient focus has been paid   into flexible architectures, interchangeability of meeting   information as well as the integration into business processes and   applications. This situation has motivated our research consortium   to direct the research activity within the MISTRAL project towards a   flexible and extendable system that can be easily integrated into   daily working environments for knowledge access and learning   activities. In this paper, we give an overview about electronic   meeting systems, introduce related work on meeting information   systems, outline the MISTRAL concept and its implementation, and   based on that we discuss findings and problems with our   research.'),
(1287,'Inspired by Fagin\'s result that   <I>NP</I> =   <sup>1</sup><sub>1</sub>, we have   developed a partial framework to investigate expressibility inside   <sup>1</sup><sub>1</sub> so as to   have a finer look into <I>NP</I>. The framework uses   interesting combinatorics derived from second-order   Ehrenfeucht-Fra&#239;ss&#233; games and the notion of game   types. Some of the results that have been proven within this   framework are: (1) for any k, divisibility by k is not expressible   by a <sup>1</sup><sub>1</sub>   sentence where (1.i) each second-order variable has arity at most 2,   (1.ii) the first-order part has at most 2 first-order variables, and   (1.iii) the first-order part has quantifier depth at most 3, (2)   adding one more first-order variable makes the same problem   expressible, and (3) inside this last logic the parameter k creates   a proper hierarchy with varying the number of second-order   variables.'),
(1288,'This paper describes the development of an Intellectual Property (IP) core in VHDL able to implement a Multilayer Perceptron (MLP) artificial neural network (ANN) topology with up to 2 hidden layers, 128 neurons, and 31 inputs per neuron. Neural network models are usually developed by using programming languages, such as Matlab. However, their implementation in configurable logic hardware requires the use of some other tools and hardware description languages, such as as VHDL. For easy migration, a Matlab Graphical User Interface (GUI) to automatically translate the ANN architecture to VHDL code has been developed. In addition, the use of an activation function based on fuzzy logic for the implementation of the MLP neural network simplifies the logic and improves the results. The environment was tested using a typical prediction problem, the Mackey-Glass series, where several ANN topologies were generated, tested and implemented in an FPGA. Results show the excellent agreement between the results provided by the software model and the hardware implementation.'),
(1289,'The WWW is currently experiencing a   revolutionary growth due to numerous emerging tools, techniques and   concepts. Digital journals thus need to transform themselves to cope   with this evolution of the web. With their growing information size   and access, conventional techniques for managing a journal and   supporting authors and readers are becoming insufficient. Journals   of the future need to provide innovative administrative tools in   helping its managers to ensure quality. They also need to provide   better facilities for assisting authors and readers in making   decisions regarding their submission of papers and in providing   novel navigational features for finding relevant publications and   collaborators in particular areas of interest. In this paper, we   explore an innovative solution to address these problems by using an   emerging Web 2.0 technology. We explore the application of mash-ups   for J.UCS - the Journal of Universal Computer Science and encourage   readers and authors to try out the applications (see section 11   Conclusions).  J.UCS can then serve as a model for contemporary   electronic journals.'),
(1290,'Although immense efforts have been invested in   the construction of hundreds of learning object repositories, the   degree of reuse of learning resources maintained in such   repositories is still disappointingly low. As the reasons for this   observation are not well understood, we carried out an empirical   investigation with the objectives to identify recurring patterns in   the retrieval and (re-) use of learning resources and to design and   test social networking functionality supporting communities of   practice. The outcomes of this project, which are reported here, aim   to affect the design of a new generation of learning object   repositories, like CampusContent, that tries to eliminate deficits   of current repositories and involve recent contributions in the area   of social software. Object of our investigation was LON-CAPA, a   crossinstitutional learning content management and assessment system   used since 2000. We analyzed hundreds of thousands of log data   collected over a period of three years and detected various kinds of   latent relationships among LON-CAPA users, such as the co-occurrence   of learning resources from independent authors in instructional   materials. To understand the rationale behind these findings, we   conducted a study with LON-CAPA users. One section of the   questionnaire asked for people\'s opinion about the expected benefit   of community support. Nearly 80% of the study participants said that   the formation of communities of practice (CoP) would be an asset to   LON-CAPA. More than 80% would be ready to provide their profiles for   matching up with CoPs and serve the community by spending time on   the evaluation of resources they had used. Finally we sketch a   faceted search functionality we designed to support CoPs among   LON-CAPA users. This functionality is currently tested with two   CoPs.'),
(1291,'Search engines\"web dragons\"are   the portals through which we access society\'s treasure trove of   information. They do not publish the algorithms they use to sort and   filter information, yet what they do and how they do it are amongst   the most important questions of our time. They deal not just with   information <I>per se</I>, but evaluate it in order to   prioritize it for the user. To do this they assess the prestige of   each web page in terms of who links to it. This article explains in   non-technical terms what is known about how web search engines   work. We describe the dominant way of measuring prestige, relating   it to the experience of a surfer condemned to click randomly around   the web foreverand also to standard techniques of bibliometric   evaluation. We review alternatives: some strive to identify   subcommunities of the web; others learn based on implicit user   feedback. We also takes a critical look at how people use search   engines, and identify issues of bias, privacy, and personalization   that crucially affect our world of information today.'),
(1292,'Recently Web Based Training (WBT) starts to be   widely used as a new way of teaching. Unfortunately, this mode of   teaching imposes new requirements and constraints. It has made the   creation of learning material a complex and demanding task for the   instructors as it takes much time and demands a multitude of skills,   in particular technical skills that must be developed and   continuously updated. Hence, we propose a collaborative authoring   methodology based on division of labour as a way to produce WBTs   where the processes of production are clearly separated to meet the   existing and needed skills of persons involved in WBT   production. This paper presents an efficient method to support   instructor?s guidance during the first phase of the WBT production   called the Macro Design using the Rhetorical Structure Theory (RST)   and the taxonomies we developed.'),
(1293,'The digital media and games industry is one of   the biggest IT based indus-tries worldwide. Recent observations   therein showed that current production workflows may be potentially   improved as multimedia objects are mostly created from scratch due   to insucient reusability capacities of existing tools. In this   paper we provide reasons for that, provide a potential solution   based on semantic technologies, show the potential of ontologies,   and provide scenarios for the application of semantic technologies   in the digital media and games industry.'),
(1294,'With the advent of Web 2.0 technologies a new   attitude towards processing contents in the Internet has   emerged. Nowadays it is a lot easier to create, share and retrieve   multimedia contents on the Web. However, with the increasing amount   in contents retrieval becomes more challenging and often leads to   inadequate search results. One main reason is that image clustering   and retrieval approaches usually stick either solely to the images\'   low-level features or their user-generated tags (high-level   features). However, this is frequently inappropriate since the   \"real\" semantics of an image can only be derived from the   combination of low-level and high-level features. Consequently, we   investigated a more holistic view on image semantics based on a   system called Imagesemantics. This system combines MPEG-7   descriptions for low-level content-based retrieval features and   MPEG-7 keywords by a machine learning approach producing joined OWL   rules. The rule base is used in Imagesemantics to improve retrieval   results.'),
(1295,''),
(1296,'Information integration applications combine   data from heterogeneous sources to assist the user in solving   repetitive data-intensive tasks. Currently, such applications   require a high level of expertise in information integration since   users need to know how to extract data from an on-line source,   describe its semantics, and build integration plans to answer   specific queries.  We have integrated three task learning   technologies within a single desktop application to assist users in   creating information integration applications. It includes a tool   for programmatic access to data in on-line information sources, a   tool to semantically model them by aligning their input and output   parameters with a common ontology, and a tool that enables the user   to create complex integration plans using simple text   instructions. Our system was integrated within the Calo Desktop   Assistant and evaluated independently on a range of problems. It   enabled non-expert users to construct integration plans for a   variety of problems in the office and travel domains.'),
(1297,'Most today?s web sources do not provide suitable   interfaces for software programs to interact with them. Many   researchers have proposed highly effective techniques to address   this problem. Nevertheless, ad-hoc solutions are still frequent in   real-world web automation applications.  Arguably, one of the   reasons for this situation is that most proposals have focused on   query wrappers, which transform a web source into a special kind of   database in which some queries can be executed using a query form   and return resultsets that are composed of structured data   records. Although the query wrapper model is often useful, it is not   appropriate for applications that make decisions according to the   data retrieved or processes that use forms that can be modelled as   insert/update/delete operations. This article proposes a new   language for defining web automation processes that is based on a   wide range of real-world web automation tasks that are being used by   corporations from different business areas.'),
(1298,'The number of applications that need to crawl the Web to gather data is growing at an ever increasing pace. In some cases, the criterion to determine what pages must be included in a collection is based on theirs contents; in others, it would be wiser to use a structure-based criterion. In this article, we present a proposal to build structure-based crawlers that just requires a few examples of the pages to be crawled and an entry point to the target web site. Our crawlers can deal with form-based web sites. Contrarily to other proposals, ours does not require a sample database to fill in the forms, and does not require the user to interact heavily. Our experiments prove that our precision is 100% in seventeen real-world web sites, with both static and dynamic content, and that our recall is 95% in the eleven static web sites examined.'),
(1299,'In data-intensive web sites pages are generated   by scripts that embed data from a backend database into HTML   templates. There is usually a relationship between the semantics of   the data in a page and its corresponding template. For example, in a   web site about sports events, it is likely that pages with data   about athletes are associated with a template that differs from the   template used to generate pages about coaches or referees. This   article presents a method to classify web pages according to the   associated template. Given a web page, the goal of our method is to   accurately find the pages that are about the same topic. Our method   leverages on a simple, yet effective model to abstract some   structural features of a web page. We present the results of an   extensive experimental analysis that show the performance of our   methods in terms of both recall and precision regarding a large   number of real-world web pages.'),
(1300,'There are many challenges developers face when   attempting to reliably extract data from the Web. One of these   challenges is the resilience of the extraction system to changes in   the web pages information is being extracted from. This article   compares the resilience of information extraction systems that use   position based extraction with an ontology based extraction system   and a system that combines position based extraction with ontology   based extraction. The findings demonstrate the advantages of using a   system that combines multiple extraction techniques, especially in   environments where web sites change frequently and where data   collection is conducted over an extended period of time.'),
(1301,'As web sites are getting more complicated, the   construction of web information extraction systems becomes more   troublesome and time-consuming. A common theme is the difficulty in   locating the segments of a page in which the target information is   contained, which we call the informative blocks. This article   reports on the Recognising Informative Page Blocks algorithm (RIPB),   which is able to identify the informative block in a web page so   that information extraction algorithms can work on it more   efficiently. RIPB relies on an existing algorithm for vision-based   page block segmentation to analyse and partition a web page into a   set of visual blocks, and then groups related blocks with similar   content structures into block clusters by using a tree edit distance   method. RIPB recognises the informative block cluster by using tree   alignment and tree matching. A series of experiments were performed,   and the conclusions were that RIPB was more than 95% accurate in   recognising informative block clusters, and improved the efficiency   of information extraction by 17%.'),
(1302,''),
(1303,'A goal of software product lines is the   economical assembly of programs in a family of programs. In this   paper, we explore how theorems about program properties may be   integrated into feature-based development of software product   lines. As a case study, we analyze an existing Java/JVM compilation   correctness proof for defining, interpreting, compiling, and   executing bytecode for the Java language. We show how features   modularize program source, theorem statements and their proofs. By   composing features, the source code, theorem statements and proofs   for a program are assembled. The investigation in this paper reveals   a striking similarity of the refinement concepts used in   <I>Abstract State Machines (ASM)</I> based system   development and <I>Feature-Oriented Programming (FOP)</I>   of software product lines. We suggest to exploit this observation   for a fruitful interaction of researchers in the two   communities.'),
(1304,'In this paper, we present a concrete textual notation, called <I>AsmetaL</I>, and a general-purpose simulation engine, called <I>AsmetaS</I>, for Abstract State Machine (ASM) specifications. They have been developed as part of the ASMETA (ASMs mETAmodelling) toolset, which is a set of tools for ASMs based on the metamodelling approach of the Model-driven Engineering. We briefly present the ASMETA framework, and we discuss how the language and the simulator have been developed exploiting the advantages offered by the metamodelling approach. We introduce the language AsmetaL used to write ASM specifications, and we provide the AsmetaL encoding of ASM specifications of increasing complexity. We explain the AsmetaS architecture, its kernel engine, and how the simulator works within the ASMETA tool set. We discuss the features currently supported by the simulator and how it has been validated.'),
(1305,'In this paper, we present the Timed Abstract   State Machine (TASM) language, which is a language for the   specification of embedded real-time systems. In the engineering of   embedded real-time systems, the correctness of the system is defined   in terms of three aspects - function, time, and resource   consumption. The goal of the TASM language and its associated   toolset is to provide a basis for specification-based real-time   system engineering where these three aspects can be specified and   analyzed. The TASM language is built on top of Abstract State   Machines (ASM) by including facilities for compact and legible   specification of non-functional behavior, namely time and resource   consumption. The TASM language provides a notation which is   well-suited to the specification needs of embedded real-time   systems. We begin the presentation of the language with a historical   survey on the use of ASM in specifying real-time systems. The core   difference between the TASM language and ASM is that steps are   inherently durative instead of being instantaneous and steps consume   resources. These concepts capture the reality of physical systems in   a flexible abstract model. We present the syntax and semantics of   the language and illustrate the concepts using an extended version   of the production cell case study.'),
(1306,'We describe a prototype of a simulator for   reactive timed abstract state machines (ASM) that checks whether the   generated runs verify a requirements specification represented as a   formula of a First Order Timed Logic (FOTL). The simulator deals   with ASM with continuous or discrete time. The time constraints are   linear inequalities. It can treat two semantics, one with   instantaneous actions and another one with delayed actions, the   delays being bounded and non-deterministic.'),
(1307,'This paper gives a definition of ASM refinement suitable for the verification that a protocol implements atomic transactions. We used this definition as the basis of the formal verification of the refinements of the Mondex case study with the interactive theorem prover KIV. The refinement definition we give differs from the one we gave in earlier work which preserves partial and total correctness assertions of ASM runs. The reason is that the main goal of the refinement of the Mondex protocol is to preserve a security invariant, while total correctness is not preserved. To preserve invariants, the definition of generalized forward simulation is limited to the use of \"small\" diagrams, which contain of a single protocol step. We show a technique that allows to use the natural \"big\" diagrams that consist of an atomic action being refined by a full protocol run.'),
(1308,'We give a survey on work we did in the past   where we have successfully applied the ASM methodology to provide   abstract models for a number of problem areas that are commonly   found in Service Oriented Architectures (SOA). In particular, we   summarize our work on (1) service behavior mediation, (2) service   discovery, and (3) service composition, showing that the   corresponding solutions can be described as variations of a   fundamental abstract processing modelthe <I>Virtual   Provider</I>.'),
(1309,'As soon as major protocol flaws were discovered empirically - a good luck that is not older than the early 1990s -- this title question came up to the world. It was soon realised that some notion of formal correctness was necessary to substantiate the confidence derived from informal analyses. But protocol correctness was born in a decade when security in general was only beginning to ferment. <P> Security protocols aim at a large variety of goals. This is partly due to the increasing domains where the protocols are finding an application, such as secure access to localarea network services, secure e-mail, e-commerce, public-key registration at certification authorities and so on. Also, several interpretations are possible about each goal. <P> Clearly, it is impossible to study protocol correctness profitably without a universal and unambiguous interpretation of its goals. What may be typical of security problems is that it is at least as important to state a detailed and appropriate model of threats that a secure system is meant to withstand. This has been a second and significant source of perhaps useless debates around many protocols. <P> These are certain to be some of the reasons why dozens of papers appeared about one, now popular, protocol attack in just a few years of the second half of the last decade. One of the protocol designers firmly refused those \"findings\" because his protocol had been conceived within a different threat model -- and perhaps for different goals -- from the one that the publications had been constructed upon. <P> It seems obvious that an ant may survive under a single sheet of paper but certainly will not under a hard-back bulky book. It should be clarified what an ant and a bulky book precisely are. With particular attention to similar issues, this position paper discusses some findings of the author\'s in the area of protocol formal analysis. Their significance mostly is methodical rather than specific for particular protocols. The paper then outlines the author\'s favourite tool, the Inductive Method, and concludes with a few open problems.'),
(1310,''),
(1311,''),
(1312,'Software adaptation techniques appear in many   disparate areas of research literature, and under many guises. This   paper enables a clear and uniform understanding of the related   research, in three ways. Firstly, it surveys a broad range of   relevant research, describing and contrasting the approaches of each   using a uniform terminological and conceptual vocabulary. Secondly,   it identifies and discusses three commonly advocated principles   within this work: component models, first-class connection and loose   coupling. Thirdly, it identifies and compares the various   modularisation strategies employed by the surveyed work.'),
(1313,'This paper describes a Java-based framework for developing componentbased software systems supporting adaptation with logic laws and considering component interactions as a first-class aspect.</P>  <P>On the one side, the framework makes it possible to specify the logic of interaction at the component-level, in terms of input and output interfaces, the events generated and observed by a component, and related information about the management of the control flow. On the other side, it is possible to specify the logic of interaction at the inter-component level, providing a modelling and linguistic support for designing and (dynamically) programming the glue among the components, enabling general forms of adaptation, observation and construction of the interaction space.</P>  <P>As a result, the framework supports the adaptation of components at different levels: from interoperability among heterogeneous and unknown components, to the support for dynamic introduction, removal and update of components, to general coordination patterns, such as workflow.</P>  <P>The framework uses first-order logic as the reference computational model for describing and defining the logic of interaction: the modalities adopted by components to interact, the adaptation laws gluing the components and the interaction events occurring in the system are expressed as facts and rules. They compose the (evolving) logic theories describing and defining the interaction at the system level, and can be observed and controlled at runtime to allow dynamic re-configurability.'),
(1314,'Reuse of software entities such as components or Web services raise composition issues since, most of the time, they present mismatches in their interfaces. These mismatches may appear at different interoperability levels: signature, behaviour, quality of service and semantics. The behavioural level is crucial and behavioural mismatches must all be corrected, although this is a difficult task. So far, most adaptation approaches which deal with behavioural mismatches work on a fixed description of components where all ports involved in their interfaces are known at design-time. Here, we focus on systems in which composition is affected by run-time behaviour of the system. This is the case in pervasive systems where a client interacts with a specific service by using new communication channels dynamically created. These are of special interest to allow private interaction between several entities.</P>  <P>In this article, we define a behavioural model inspired by the ss-calculus to specify behavioural interfaces of components. Our model is particularly suitable for creating new channels dynamically, also taking concurrent behaviours into account. The dynamic nature of the systems we are dealing with obliges to apply adaptation at run-time, avoiding at the same time the costly generation of full descriptions of adaptors. The main contribution of this article is an adaptation engine that allows the dynamic creation of channels and applies at run-time a composition specification built at designtime. All the underlying formal foundations of our proposal have been implemented in a prototype tool that has been applied to system designs. Aspect-Oriented Programming has been studied as well, as a way to implement our engine for further application to real software components.'),
(1315,'One focus of current software development is the   re-use of components in the construction of systems. Software   Adaptation facilitates the consequent need to adapt these components   to the new environment by employing adaptors which are obtained   automatically and hence with a certain guarantee of suitability,   from formal descriptions of the interface behaviour. One appropriate   technique for Software Adaptation is Aspect-Oriented Programming   (AOP) which makes use of aspects to facilitate the dynamic   adaptation of components transparently and non-intrusively.   However, owing to the way that aspects are integrated, these can   unexpectedly modify the functionality of the system, and   consequently completely alter its semantics.  It is hence necessary   to study the final behaviour of the system to ensure its correctness   after adding aspects for its adaptation. This study must go beyond   just detecting problems at the protocol level, to analyze the   potential semantic problems.  This is the main focus of the present   communication.  We start from the Unified Modeling Language (UML   2.0) specification of both the initial system and the aspects.  This   specification is validated by generating an algebraic Calculus of   Communicating Systems (CCS) description of the system.  Next,   extended (finite) state machines are automatically generated to   verify, simulate, and test the modeled system\'s behaviour.  The   result of that process can also be compared with the behaviour of   the new running system.  To facilitate this task, we propose   grouping components so as to centre the study on the points actually   affected by the behaviour of the aspects.'),
(1316,'The development of Web applications, both   functionality and Web User Interfaces (UIs), has been facilitated   over the last few years using Web models and methodologies. However,   new requirements that overcome traditional HTML-based Web 1.0 User   Interfaces limits have arisen. Developers and tool vendors have   answered these limits introducing Rich Internet Applications   (RIAs). RIA technologies provide Web 2.0 UI capabilities such as   high interactivity and native multimedia support among   others. Currently, numerous developers are adapting many of their   legacy Web 1.0 applications to Web 2.0 introducing Web 2.0 UI   capacities while maintaining the business logic. Nevertheless, there   is a lack of methodologies to support this adaptation process. In   this paper we show how to use a model driven method called   RUX-Method for the systematic adaptation of existing Web 1.0 UIs to   Web 2.0 multidevice UIs. This method focuses on new UI capabilities   provided by RIAs while taking advantage of functionality already   provided by existing Web models. The proposal follows a common UI   design for all the devices and an ad-hoc design approach for each   device attending to its specific features.'),
(1317,''),
(1318,'Intelligence and Knowledge play more and more important roles in building complex intelligent systems, for instance, intrusion detection systems, and operational analysis systems. Knowledge processing in complex intelligent systems faces new challenges from the increased number of applications and environment, such as the requirements of representing domain and human knowledge in intelligent systems, and discovering actionable knowledge on a large scale in distributed web applications. In this paper, we discuss the main challenges of, and promising approaches to, intelligence metasynthesis and knowledge processing in open complex intelligent systems. We believe (1) ubiquitous intelligence, including data intelligence, domain intelligence, human intelligence, network intelligence and social intelligence, is necessary for OCIS, which needs to be meta-synthesized; and (2) knowledge processing should pay more attention to developing innovative and workable methodologies, techniques, tools and systems for representing, modelling, transforming, discovering and servicing the uncertain, large-scale, deep, distributed, domain-oriented, human-involved, and actionable knowledge highly expected in constructing open complex intelligent systems. To this end, the meta-synthesis of ubiquitous intelligence is an appropriate way in designing complex intelligent systems. To support intelligence meta-synthesis, m-interaction can play as the working mechanism to form m-spaces as problem-solving systems. In building such m-spaces, advancement in knowledge processing is necessary.'),
(1319,'Adaptive behavior and learning are required of   software agents in many application domains. At the same time agents   are often supposed to be resource-bounded systems, which do not   consume much CPU time, memory or disk space. In attempt to satisfy   both requirements, we propose a novel framework, called APS   (standing for Analysis of Past States), which provides agent with   learning capabilities with respect to saving system resources. The   new solution is based on incremental association rule mining and   maintenance. The APS process runs periodically in a cycle, in which   phases of agent\'s normal performance intertwine with learning   phases. During the former ones an agent stores observations in a   history. After a learning phase has been triggered, the history   facts are analyzed to yield new association rules, which are added   to the knowledge base by the maintenance algorithm. Then the old   observations are removed from the history, so that in the next   learning runs only recent facts are processed in search of new   association rules. Keeping the history small can save both   processing time and disk space as compared to batch learning   approaches.'),
(1320,'Market Surveillance plays important mechanism   roles in constructing market models. From data analysis perspective,   we view it valuable for smart trading in designing legal and   profitable trading strategies and smart regulation in maintaining   market integrity, transparency and fairness. The existing trading   pattern analysis only focuses on interday data which discloses   explicit and high-level market dynamics. In the mean time, the   existing market surveillance systems available from large exchanges   are facing crucial challenges of diversified, dynamic, distributed   and cyber-based misuse, mis-disclosure and misdealing of   information, announcement and orders in one market or crossing   multiple markets. Therefore, there is a crucial need to develop   innovative and workable methods for smart trading and   surveillance. To deal with such issues, we propose the innovative   concept microstructure pattern analysis and corresponding approaches   in this paper. Microstructure pattern analysis studies trading   behaviour patterns of traders in market microstructure data by   utilizing market microstructure knowledge. The identified market   microstructure patterns are then used for powering market trading   and surveillance agents for automatically detecting/designing   profitable and legal trading strategies or monitoring abnormal   market dynamics and trader?s behaviour. Such trading/surveillance   agent-driven market trading/surveillance systems can greatly enhance   the analytical, discovery and decision-support capability of market   trading/surveillance than the current predefined rule/alert-based   systems.'),
(1321,'Non-repudiation of a mobile payment transaction ensures that when a buyer (B) sends some messages to a seller (S), neither B nor S can deny having participated in this transaction. An evidence of a transaction is generated by wireless PKI (WPKI) mechanism such that B and S cannot repudiate sending and receiving the purchase order respectively. Broker generates a mobile agent for B which carries encrypted purchase order to S. A trusted third party (TTP) acts as a lightweight notary for evidence generations. One advantage of this agent-based non-repudiation protocol is to reduce inconvenience for mobile clients such as connection time and search for suitable merchant servers, etc.; it provides necessary security mechanisms for fair mobile payment transactions.'),
(1322,'Nowadays, while large-sized multimedia objects   are becoming very popular throughout the Internet, one of the   important issues appears to be the acceleration of content delivery   network (CDN) performance. CDN is a web system that delivers the web   cached objects to the client and accelerates the web   performance. Therefore the performance factor for any CDN is vital   factor in determining the quality of services. The performance   improvement can be achieved through load balancing technique, so the   server load could be distributed to several clustered groups of   machines and processed in parallel. Also the performance of CDN   heavily depends on caching algorithm which is used to cache the web   objects. This study investigates a method that improves the   performance of delivering multimedia content through CDN while using   RADS algorithm for caching large-sized objects separately from   small-sized ones. We will also consider the efficient distribution   of requests outgoing from local servers in order to balance the CDN   load. This method uses various types of factors such as CPU   processing time, I/O access time and Task Queue between nearby   servers. At the end of the paper, we will see the experimental   results derived from implementing the proposed optimization   technique and observe how it could contribute to the effectiveness   of CDN.'),
(1323,'This paper considers the problem of resource   allocation in the service industries approached from an agent-based   perspective. Agent technologies seem to be well suited to this   domain by providing a distributed environment, are network centric,   semi-autonomous and collaborative and can communicate with each   other to achieve better optimisation with little human   intervention. The paper describes the context of this solution, a   general power model and several pathways with corresponding example   implementations with results and discussion The novelty of the   solution resides in the fact that it is a natural and versatile   formulation that combines an agent-based model with various   artificial intelligence and operations research techniques such as   rule-based expressions of allocation strategies and multi-criteria   optimisation expressions of allocation objectives.'),
(1324,'The paper presents a multi-agent system for   modelling and optimising city traffic. Our attention is focused on the   prevention of crisis situations and detection of anomalies. Analysed   critical situations include traffic jams, whereas an anomaly is when   there is a decrease in average vehicle velocity in the whole city or   its part. The methods of crisis situation prevention are based on   the choice and configuration of the appropriate algorithms of city   light management or on modification of intersection and road network   topologies.'),
(1325,'Location-based service is one of the most   popular buzzwords in the field of U-cities. Positioning a user is an   essential ingredient of a location-based system in a U-city. For   outdoor positioning, GPS based practical solutions have been   introduced. However, the measurement error of GPS is too big for it   to be used for U-campus services, because the size of a campus is   smaller than that of a city. We propose the Relative-Interpolation   Method to improve the accuracy of outdoor positioning. However,   indoor positioning is also necessary for a U-campus because the GPS   signal is not available inside buildings. For indoor positioning,   various systems including Cricket, Active Badge, and so on have been   introduced. These methods require special equipment dedicated to   positioning. Our method does not require such equipment because it   determines the user?s position based on the received signal strength   indicators (RSSIs) from access points (AP) which are already   installed for WLAN. The algorithm we use for indoor positioning is a   kind of fingerprinting method. However, our algorithm builds a   <I>decision tree</I> instead of a look-up table in the   <I>off-line</I> phase. Therefore, the proposed method is   faster than the existing indoor positioning methods in the   <I>real-time</I> phase. We integrated our indoor and   outdoor positioning methods and implemented a prototype   indoor-outdoor positioning system on a laptop. The experimental   results are discussed in this paper. In implementing the prototype,   we also implemented a C# library function which can be used to read   the RSSIs from the APs.'),
(1326,'Web searching techniques have been investigated   and implemented in many aspects.Particularly, in case of   personalization, more important issue is how to manipulate the   results retrieved from search engines for better user   understandability and satisfaction. Such manipula-tion processes are   <I>i)</I> ranking the results in accordance with user   relevance, and <I>ii)</I> exchangingthe results between   users who have similar tastes. Thus, our work has been mainly   focusing on relevance-based ranking mechanism as well as sharing   schemes for the results retrieved from het-erogeneous web   information sources. In this paper, we propose a hybrid model for   meta search agent systems with three main functionalities, i.e.,   <I>i)</I> URL filtering method for preprocessing,   <I>ii)</I> tag-based information conceptualization   scheme for ranking, and <I>iii</I>) ontology-based   stan-dardization scheme for sharing. It means that the proposed meta   search agent model exploits semantized tags to formalize and share   heterogeneous information obtained from multiple searchengines and   to finally maintain the shared information. Within the tag-based   information space, a conceptual distance between retrieval interest   and search results can be efficiently computed. Byconducting some   experimentations, we have shown the semantized tag model can   conceptualize the retrieved results, and make them sharable. We also   compare performance of the proposedsystem with hyperlink-based   methodologies.'),
(1327,'Nowadays Personal Computers (PCs) are often   equipped with powerful, multi-core CPU. However, the processing   power of the modern PC does not depend only of the processing power   of the CPU and can be increased by proper use of the GPGPU,   i.e. General-Purpose Computation Using Graphics Hardware. Modern   graphics hardware, initially developed for computer graphics   generation, appeared to be flexible enough for general-purpose   computations. In this paper we present the implementation of two   optimization algorithms based on the tabu search technique, namely   for the traveling salsesman problem and the flow shop scheduling   problem. Both algorithms are implemented in two versions and   utilize, respectively, multi-core CPU, and GPU. The extensive   numerical experiments confirm the high computation power of GPU and   show that tabu search algorithm run on modern GPU can be even 16   times faster than run on modern CPU.'),
(1328,'In this paper, a new multi-layer level set   method is proposed for multi-phase image segmentation. The proposed   method is based on the conception of image layer and improved   numerical solution of bimodal Chan-Vese model. One level set   function is employed for curve evolution with a hierarchical form in   sequential image layers. In addition, new initialization method and   more efficient computational method for signed distance function are   introduced. Moreover, the evolving curve can automatically stop on   true boundaries in single image layer according to a termination   criterion which is based on the length change of evolving   curve. Specially, an adaptive improvement scheme is designed to   speed up curve evolution process in a queue of sequential image   layers, and the detection of background image layer is used to   confirm the termination of the whole multi-layer level set evolution   procedure. Finally, numerical experiments on some synthetic and real   images have demonstrated the efficiency and robustness of our   method. And the comparisons with multi-phase Chan-Vese method also   show that our method has a less time-consuming computation and much   faster convergence.'),
(1329,'The definition of the Fuzzy Rule Base is one of   the most important and difficult tasks when designing Fuzzy   Systems. This paper discusses the results of two different hybrid   methods, previously investigated, for the automatic generation of   fuzzy rules from numerical data. One of the methods, named   DoC-based, proposes the creation of Fuzzy Rule Bases using genetic   algorithms in association with a heuristic for preselecting   candidate rules based on the degree of coverage. The other, named   BayesFuzzy, induces a Bayesian Classifier using a dataset previously   granulated by fuzzy partitions and then translates it into a Fuzzy   Rule Base. A comparative analysis between both approaches focusing   on their main characteristics, strengths/weaknesses and easiness of   use is carried out. The reliability of both methods is also compared   by analyzing their results in a few knowledge domains.'),
(1330,'Evolutionary algorithms are effective search tools for tackling difficult optimization problems. In this paper an algorithm based on living processes where cooperation is the main evolutionary strategy is applied to the Prize Collecting Steiner Tree Problem, an NP-hard combinatorial optimization problem. The Transgenetic Algorithm presented here is hybridized with path-relinking. Computational results of an experiment performed with benchmark instances are reported. The results obtained for the Prize Collecting Steiner Tree Problem with the application of the hybrid Transgenetic Algorithm are compared with the results of three effective approaches presented previously. The computational experiment shows that the proposed approach is very competitive concerning both quality of solution and processing time.'),
(1331,'This paper describes a new hybrid technique that   combines a Greedy Randomized Adaptive Search Procedure (GRASP) and a   genetic algorithm with simulation features in order to solve the   Bus-Network Scheduling Problem (BNSP). The GRASP is used as an   initialization method to find the routes between bus stops. T he   Genetic Algorithm is used to find the whole configuration of the bus   network, together with a simulation tool that finds the values of   the environmentally dependent dynamic variables. The new method was   tested with an academic case of study, and the results clearly   satisfy the requirements of both the transport user and the   transport operator.'),
(1332,'Synchronous finite state machines are very important for digital sequential designs. Among other important aspects, they represent a powerful way for synchronizing hardware components so that these components may cooperate adequately in the fulfillment of the main objective of the hardware design. In this paper, we propose an evolutionary methodology to solve one of the problems related to the design of finite state machines. We optimally solve the state assignment NP -complete problem using a quantum inspired evolutionary algorithm. This is motivated by the fact that with an optimal state assignment one can physically implement the state machine using a minimal hardware area and response time.'),
(1333,'Wireless Sensor Networks (WSN) allow, thanks to   the use of small wireless devices known as sensor nodes, the   monitorization of wide and remote areas with precision and liveness   unseen to the date without the intervention of a human operator. For   many WSN applications it is fundamental to achieve full coverage of   the terrain monitored, known as <I>sensor field</I>. The   next major concerns are the energetic efficiency of the network, in   order to increase its lifetime, and having the minimum possible   number of sensor nodes, in order to reduce the network cost. The   task of placing the sensor nodes while addressing these objectives   is known as WSN layout problem. In this paper we address a WSN   layout problem instance in which full coverage is treated as a   constraint while the other two objectives are optimized using a   multiobjective approach. We employ a set of multi-objective   optimization algorithms for this problem where we define the energy   efficiency and the number of nodes as the independent optimization   objectives. Our results prove the efficiency of multi-objective   metaheuristics to solve this kind of problem and encourage further   research on more realistic instances and more constrained scenarios.'),
(1334,'T his paper proposes a variant of genetic   algorithm - GADYM, Genetic Algorithm with <I>Gender-Age   structure, DYnamic parameter tuning and Mandatory self perfection   scheme</I>.  The motivation of this algorithm is to increase   the diversity throughout the search procedure and to ease the   difficulties associated with the tuning of GA parameters and   operators. To promote diversity , GADYM combines the concept of   gender and age in individuals of a traditional Genetic Algorithm and   implements the self perfection scheme through sharing. To ease the   parameter tuning process, the proposed algorithm uses dynamic   environment in which heterogeneous crossover and selection   techniques are used and parameters are updated based on   deterministic rules. Thus, GADYM uses a combination of genetic   operators and variable parameter values whereas a traditional GA   uses fixed values of those. The experim ental results of the   proposed algorithm based on a mechanical design problem show   promising result.'),
(1335,'In this paper we propose a new approach to Swarm   Intelligence called Two-Step Swarm Intelligence. The basic idea is   to split the heuristic search performed by agents into two   stages. In the first step the agents build partial solutions which,   are used as initial states in the second step. We have studied the   performance of this new approach for the Feature Selection Problem   by using Ant Colony Optimization and Particle Swarm   Optimization. The feature selection is based on the reduct concept   of the Rough Set Theory. Experimental results obtained show that   Two-step approach improves the performance of ACO and PSO   metaheuristics when calculating reducts in terms of computation time   cost and the quality of reducts.'),
(1336,'This paper discusses the parallelization of   Stochastic Evolution (StocE) metaheuristic, for a distributed   parallel environment. VLSI cell placement is used as an optimization   problem. A comprehensive set of parallelization approaches are   tested and an effective strategy is identified in terms of two   underlying factors: workload division and the effect of   parallelization on metaheuristic\'s search intelligence. The   strategies are compared with parallelization of another similar   evolutionary metaheuristic called Simulated Evolution (SimE). The   role of the two mentioned underlying factors is discussed in   parallelization of StocE.'),
(1337,''),
(1338,'This paper analyses aspects about the   recommendation process in distributed information systems. It   extracts similarities and differences between recommendations in   e-stores and the recommendations applied to an e-learning   environment. It also explains the phenomena of self-organization and   cooperative emergence in complex systems coupled with bio-inspired   algorithms to improve knowledge discovery and association   rules. Finally, the present recommendation is applied to e-learning   by proposing recommendation by emergence in a Multi-Agent System   architecture.'),
(1339,'Decision Support Systems are proliferating   rapidly in many areas of human endeavour including clinical medicine   and psychology. While these are typically based on rule-based   systems, decision trees, or Artificial Neural Networks, this paper   argues that Bayes? Theorem can be applied fruitfully to support   expert decisions both in dynamically changing situations requiring   the system progressively to adapt, and when this is not the   case. One example of each of these two types is given. One provides   diagnostic support for human decision makers; the other, an e-health   mental intervention system provides decision rules enabling it to   respond and provide the most appropriate training modules to input   from clients with changing needs. The contributions of psychological   research underlying both systems is summarized.'),
(1340,'Human-Computer Interaction (HCI) is a   challenging discipline that is currently concerned with the design,   implementation and evaluation of interactive systems for human use,   as well as the study of major phenomena surrounding them. Indeed,   interdisciplinary communities formed by scientists, university   teachers and students, people coming from the industry and customers   related to HCI are emerging in different parts of the world. In   particular, this article overviews the HCI community in the   Ibero-American context, which involves hundreds of millions of   people working or studying in HCI, whose cultural background is   primarily associated with the Spanish and Portuguese languages and   cultures, regardless of ethnic and geographical differences. Our   final goal is to improve the visibility of this particular HCI   community, enhancing the self awareness of its members and their   individual motivation and future exchanges.'),
(1341,'Cognitive Ergonomics is a discipline that   contributes with its knowledge to construct better machines in the   sense of being easier to use by human beings. Cognitive Ergonomists   perform a cognitive analysis of interaction to: (1) shorten the time   to accomplish interaction tasks; (2) reduce the number of mistakes   made by humans; (3) reduce learning time; and (4) improve people?s   satisfaction with a system. An appropriate methodology for   performing this cognitive analysis of interaction could be based on   what I call the \"Principle of Mutual Dependency\" [Ca&#241;as et al   2004]. This principle determines that: (1) The optimal interface   functions will be those that fit the human cognitive functions   involved in the task; (2) The human cognitive functions that are   involved in the task depend on the interface functions; (3) The   modification, replacement, or introduction of a new interface   function implies the adaptation of the human cognitive functions;   (4) The development (e.g., learning) or limitation (e.g., Elderly   users) of the human cognitive functions will imply limitations on   the possible interface functions. I will describe this principle   with examples from research projects in which our research group   participates.'),
(1342,'In human-computer interaction and computing, mobile phone usage is mostly addressed from a feature-driven perspective, i.e. which features do a certain user group use, and/or a usability perspective, i.e. how do they interact with these features. Although the feature driven and usability focus carry value, it is not the full picture. There is also an alternative or wider perspective: mobile phone use is influenced by demographic, social, cultural, and contextual factors that complicate the understanding of mobile phone usage. Drawing on concepts and models from sociology, computer-supported cooperative work, human-computer interaction and marketing, we researched the influence of culture on mobile phone adoption using interviews and two surveys. The contribution of this research is a model that includes culture as one of the factors that influence mobile phone adoption and usage. The proposed model represents the influence of mediating factors and determining factors on actual mobile phone use. The proposed model has been evaluated from both a qualitative and quantitative perspective.'),
(1343,'In this article we explain how we apply the CIAM   methodology based on the CIAN notation in order to generate user   interfaces in collaborative applications. CIAM has been applied   successfully in the development of desktop applications, such as   Domosim-TPC, demonstrating its effectiveness in the definition of   user interfaces for collaborative applications where a shared   context is required. We present the AULA system modeled by means of   CIAM. The results in the application of this Methodology show the   necessity to include those aspects closely related with context   modeling and the synchronization of contents; that is why we make an   outline of the way to take into account these characteristics as a   future work.'),
(1344,'The aim of this paper is to review the best   known methodologies for web applications development as well as the   existing supporting tools and techniques from an   accessibility-centric perspective. To this end, a number of   development methodologies with their respective characteristics are   described: model-based methodologies, user-centred processes,   usability engineering methodologies and accessibility engineering   methodologies. Some of these methodologies are provided with   specific supporting tools which facilitate the accomplishment of   specified tasks. However, there are methodologies which are not   supported by specific tools. Therefore, web developers must deal   with diverse tools in order to perform the corresponding   activities. In this context, the development of accessible web   applications is even more difficult. This paper concludes that there   is not currently a holistic development framework to be used   throughout the whole development process. Our contribution relies on   a set of tools that support the different phases of the   process. Since these tools are developed upon a common   methodological basis, a high rate of interoperability is   obtained. This cohesion allows their integration in a comprehensive   framework so that the development of accessible web applications is   facilitated.'),
(1345,'Cognitive Ergonomics is discussed as a   systematic base for user interface design. The history of the   discipline, explicitly existing now for about 25 years, is   discussed, from participatory design, through various flavors of   user centered design, to contextual design. Several persistent   misunderstandings regarding the need for user interface design are   analyzed. The concept of activity centered design is proposed as   state of the art approach, and several techniques that support this   paradigm are mentioned and illustrated.'),
(1346,''),
(1347,'Authoring in adaptive educational hypermedia   environment is complex activity. In order to promote a wider   application of this technology, the teachers and course designers   need specific methods and tools for supporting their work. In that   sense, data mining is a promising technology. In fact, data mining   techniques have already been used in E-learning systems, but most of   the times their application is oriented to provide better support to   students; little work has been done for assisting adaptive   hypermedia authors through data mining. In this paper we present a   proposal for using data mining for improving an adaptive hypermedia   system.  A tool implementing the proposed approach is also   presented, along with examples of how data mining technology can   assist teachers.'),
(1348,'At present a large amount of research exists   into the design and implementation of adaptive systems. However, not   many target the complex task of authoring in such systems, or their   evaluation. In order to tackle these problems, we have looked into   the causes of the complexity. Manual annotation has proven to be a   bottleneck for authoring of adaptive hypermedia. One such solution   is the reuse of automatically generated metadata. In our previous   work we have proposed the integration of the generic Adaptive   Hypermedia authoring environment, MOT (My Online Teacher), and a   semantic desktop environment, indexed by Beagle++. A prototype,   Sesame2MOT Enricher v1, was built based upon this integration   approach and evaluated. After the initial evaluations, a web-based   prototype was built (web-based Sesame2MOT Enricher v2 application)   and integrated in MOT v2, conforming with the findings of the first   set of evaluations. This new prototype underwent another   evaluation. This paper thus does a synthesis of the approach in   general, the initial prototype, with its first evaluations, the   improved prototype and the first results from the most recent   evaluation round, following the next implementation cycle of the   spiral model [Boehm, 88].'),
(1349,'Authoring ITS domain models is a difficult task   requiring many skills. We explored whether modeling ontology reduces   the problem by giving the students of an e-learning summer school   the task of developing the model for a simple domain in under sixty   minutes using ontology. Some students also used our tool to develop   a complete tutor in around eight hours, which is much faster than   they could be expected to author the system without the tool. The   results suggest this style of authoring can lead to very rapid ITS   development. We further extend the ontological approach with domain   schema: high-level abstractions that describe the semantics of the   domain model for a class of domains. Using domain schema reduces the   authoring effort to one of describing only those aspects that are   unique to this particular domain, and enables the ontology-based   approach to model domains with different semantic   requirements.'),
(1350,'This paper describes the process of translating   an adaptive sequencing strategy designed using Sequencing Graphs to   the semantics of IMS Learning Design. The relevance of this   contribution is twofold. First, it combines the expressive power and   flexibility of Sequencing Graphs, and the interoperability   capabilities of IMS. Second, it shows some important limitations of   IMS specifications (focusing on Learning Design) for the sequencing   of learning activities.'),
(1351,'Social-aware computing is an emerging trend   based on ubiquitous computing technologies and collaborative work. A   successful design demands a better understanding of group tasks,   adaptation mechanisms and support for dynamic changes in a nomadic   computing paradigm. This paper proposes the use of a hypermedia   model to describe and support group activities in intelligent   environments. The resulting system integrates adaptive context-aware   information on the basis of user/group models in order to provide a   structured access to dynamic task scheduling. In particular, we   propose the use of the calendar metaphor as an ongoing connection   between active spaces and collaborative tasks. This proposal   provides the appropriate support for an easier human coordination to   achieve common objectives in blended learning scenarios, and thus,   extending authoring social tasks to physical spaces.'),
(1352,'One of the key problems in developing standard   based adaptive courses is the complexity involved in the design   phase, especially when establishing the hooks for the dynamic   modelling to be performed at runtime. This is particularly critical   when the courses are based on adaptation-oriented learning   scenarios, where the full eLearning cycle (design, publication, use   and auditing) is considered. Based on the problems we experienced in   developing such scenarios with a reusable, platform independent,   objective-based approach in the aLFanet project we have established   an alternative framework in the ADAPTAPlan project, which focuses on   dynamically generating learning design templates with the support of   user modelling, planning and machine learning techniques. In   particular, in this paper we describe the problems we are tackling   and how we are relaxing the design work by automatically building   the IMS learning design of the course from a simplified set of data   required from the course authors.'),
(1353,'The Cultural Artefacts in Education (CAE)   questionnaire is used to determine the educational values of   different cultures. In this paper I examine the results for ten   countries, specifically focussing on their attitudes towards   adaptive hypermedia in an educational setting. These results can   inform the authoring process for adaptive systems  content,   with Adaptive Hypermedia systems being able to employ stereotype   adaptation to deliver content pre-adjusted to a learner?s cultural   background.'),
(1354,'The use of digital games in education is well   documented in the literature. They have been used in preschool,   K-12, the university. A specific type of digital games is board   games. Adding board games to the educational process can lead to an   interactive stimulating learning experience. With a board game,   players often learn from one another while at the same time having   fun in a competitive environment. In this paper we propose the   ELG game, an e-learning board game that adopts the basic   elements of a racing board game but fosters students? creativity,   problem-solving skills, and imagination as students are trying to   reach the end by improving their performance in a variety of   learning activities. The innovative feature of the ELG is that it   offers an adaptive authoring tool that enables teachers to customize   their games according to the needs, interests and motives of   students. The teacher enters hierarchically categorized learning   activities according to the learning goals of a course, sets the   rules and assesses the learning progress easily and simply. Students   participate in a discovery or exploration trying to reach the   goals. After attaining them their level of activities is upgraded   and they are challenged to reach the next learning goal. The dice in   ELG is not randomized but controlled by the teachers in order that   they can customize adaptive learning rules. The educational benefits   of exploiting ELG in the learning process is that the teacher can   define the levels of difficulty according to the students? needs and   interests, facilitate and monitor the learning rate of each student,   combine a variety of evaluation techniques, and address potential   learning problems in a timely manner.'),
(1355,''),
(1356,'Mass digitization of document collections with   further processing and semantic annotation is an increasing activity   among libraries and archives at large for preservation, browsing and   navigation, and search purposes. In this paper we propose a software   architecture for the process of converting high volumes of document   collections to semantically annotated digital libraries. The   proposed architecture recognizes two sources of knowledge in the   conversion pipeline, namely document images and humans. The Image   Analysis module and the Correction and Validation module cover the   initial conversion stages. In the former information is   automatically extracted from document images. The latter involves   human intervention at a technical level to define workflows and to   validate the image processing results. The second stage, represented   by the Knowledge Capture modules requires information specific to   the particular knowledge domain and generally calls for expert   practitioners. These two principal conversion stages are coupled   with a Knowledge Management module which provides the means to   organise the extracted and acquired knowledge. In terms of data   propagation, the architecture follows a bottom-up process, starting   with document image units, called <I>terms</I>, and   progressively building meaningful <I>concepts</I> and   their <I>relationships</I>. In the second part of the   paper we describe a real scenario with historical document archives   implemented according to the proposed architecture.'),
(1357,'During the last decades, digital objects have become the primary medium to create, shape, and exchange information. However, in contrast to analog objects such as books that directly represent their content, digital objects are not usable without a corresponding technical environment. The fast changes in these environments and in formats and technologies mean that digital documents have a short lifespan before they become obsolete. Digital preservation, i.e. actions to ensure longevity of digital information, thus has become a pressing challenge. The dominant strategies prevailing today are migration and emulation; for each strategy, different tools are available. When converting an object to a different representation, a validation of the content is needed to verify that the transformed objects are still authentically representing the same intellectual content. This validation so far is largely done manually, which is infeasible for large collections.</P>  <P>Preservation planning supports decision makers in reaching accountable decisions by evaluating potential strategies against well-defined requirements. Especially the evaluation of different migration tools for digital preservation has to rely on validating the converted objects and thus on an analysis of the logical structure and the content of documents. Existing approaches for characterising and describing objects do not attempt to fully extract the informational content of digital objects and thus are not suffficient for an in-depth validation of transformed content.</P>  <P>This paper describes the eXtensible Characterisation Languages (XCL) that support the automatic validation of document conversions and the evaluation of migration quality by hierarchically decomposing a document and representing documents from different sources in an abstract XML language. The description language XCDL provides an abstract representation of digital content in XML, while the extraction language XCEL allows an extraction engine to create such an abstract description by mapping file format structures to XCDL concepts.</P>  <P>We present the context of the development of these languages and tools and describe the overall concept and features of the languages. We further give examples and show how the languages can be applied to the evaluation of digital preservation solutions in the context of preservation planning.'),
(1358,'In this paper we provide a study about crime   scenes and its features used in criminal investigations. We argue   that the crime scene provides a large set of features that can be   used to corroborate the conclusions emitted by the experts. We also   propose a set of features to classify the violent crime considering   two classes: attack from inside or outside of the scene. The   classification stage is based on conventional MLP (Multiple-Layer   Perceptron) Neural Network and SVM (Support Vector Machine). The   experimental results reveal an error rate of 30.3% (MLP), 22.8%   (SVM-linear), and 19.4% (SVM-polynomial) using a database composed   of 400 crime scenes. This paper presents an experiment based on a   stereoscopic projection that allows to experts analyze and take   decisions about the crime scene and its dynamic.'),
(1359,'Linguistics and stylistics have been   investigated for author identification for quite awhile, but   recently, we have testified a impressive growth in the volume with   which lawyers and courts have called upon the expertise of linguists   in cases of disputed authorship. This motivatescomputer science   researchers to look to the problem of author identification from a   different perspective. In this work, we propose a stylometric   feature set based on conjunctions and ad-verbs of the Portuguese   language to address the problem of author identification. Two   different approaches of classification were considered. The first   one is called writer-independent and it re-duces the pattern   recognition problem to a single model and two classes, hence, makes   it possible to build robust system even when few genuine samples per   writer are available. The second oneis called the personal model, or   writer-dependent, which very often performs better but needs a   bigger number of samples per writer. Experiments on a database   composed of short articlesfrom 30 different authors and Support   Vector Machine (SVM) as classifier demonstrate that the proposed   strategy can produced results comparable to the   literature.'),
(1360,'In this paper, we introduce and evaluate a   system capable of recognizing words extracted from ultra low   resolution images such as those frequently embedded on web   pages. The design of the system has been driven by the following   constraints. First, the system has to recognize small font sizes   between 6-12 points where anti-aliasing and resampling filters are   applied. Such procedures add noise between adjacent characters in   the words and complicate any a priori segmentation of the   characters. Second, the system has to be able to recognize any words   in an open vocabulary setting, potentially mixing different   languages in Latin alphabet. Finally, the training procedure must be   automatic, i.e. without requesting to extract, segment and label   manually a large set of data. These constraints led us to an   architecture based on ergodic HMMs where states are associated to   the characters. We also introduce several improvements of the   performance increasing the order of the emission probability   estimators, including minimum and maximum width constraints on the   character models and a training set consisting all possible   adjacency cases of Latin characters. The proposed system is   evaluated on different font sizes and families, showing good   robustness for sizes down to 6 points.'),
(1361,'We address the problems of structuring and   annotation of layout-oriented documents.We model the annotation   problems as the collective classification on graph-like structures   with typed instances and links that capture the domain-specific   knowledge. We use the relational de-pendency networks (RDNs) for the   collective inference on the multi-typed graphs. We then describe a   variant of RDNs where a stacked approximation replaces the Gibbs   sampling in orderto accelerate the inference. We report results of   evaluation tests for both the Gibbs sampling and stacking inference   on two document structuring examples.'),
(1362,'Document binarization is an active research area   for many years. The choice of the most appropriate binarization   algorithm for each case proved to be a very difficult procedure   itself. In this paper, we propose a new technique for the validation   of document binarization algorithms. Our method is simple in its   implementation and can be performed on any binarization algorithm   since it doesn?t require anything more than the binarization   stage. As a demonstration of the proposed technique, we use the case   of degraded historical documents. Then we apply the proposed   technique to 30 binarization algorithms. Experimental results and   conclusions are presented.'),
(1363,'BigBatch is an image processing environment   designed to process batches of thousands of monochromatic   documents. One of the flexibilities and pioneer aspects of BigBatch   is offering the possibility of working in distributed environments   such as clusters and grids. This paper presents an overview of   BigBatch image processing features and analyzes the results of a   number of experiments devised to compare its cluster and grid   configurations. Although preliminary results were published earlier   on, the new data shown here that sheds new lights onto this   aspect. The results obtained exhibit almost no difference in total   execution times for some grid and cluster configurations, but   significant differences for others, indicating that the choice   between such configurations must take into account a number of   details in order to reach peak performance. Besides those, there are   other qualitative aspects that may impact this choice. This paper   analyzes these aspects and provides a general picture of how to   successfully use BigBatch to process document images employing   computers in parallel for this task.'),
(1364,''),
(1365,'A precise model of the behavioral dynamics is a   necessary precondition for the development of collaborative   environments. In this paper we present a specification framework for   collaborative environments. In particular we highlight the interplay   of task specifications and domain models. The framework consists of   two components: A formal specification language (called CTML) and   the tool CTML Editor and Simulator. CTML has a precisely defined   syntax and semantics and is designed to model actors, roles,   collaborative tasks and their dependency and impact on the   domain. The CTML Editor and Simulator is an Eclipse IDE for the   interactive creation and simulation of CTML   specifications.'),
(1366,'This paper presents a proposal to tackle the design and development of user interfaces for groupware applications. This proposal includes important design and implementation issues of special relevance for this kind of interfaces. In particular, group awareness requirements in the development of groupware applications are addressed, both in the sense of the basic manipulation actions of the interface widgets, as well as in the sense of other kinds of group awareness in relation to the presence of actors, the roles they play in a concrete moment, etc. The design proposal we present is part of a complete development process (called TOUCHE) which defines a set of facets to describe Abstract Interaction Objects. These objects, at design level, provide the basis for the definition of Concrete Interaction Objects at implementation level within a software platform intended to facilitate the development of user interfaces for groupware applications. This way, we get an integral approach to tackle the development of this kind of user interfaces, taking into account in an explicit way the perception of the joint activity of a group of users involved in a common task and thus achieving a more effective collaboration.'),
(1367,'When the user interface is specified, a picture   is worth a thousand words, and the worst thing one can do is write a   natural-language specification for it. Because this practice is   still common, it is a challenging task to move from text-based   requirements and problem-space concepts to a final UI design, and   then back again. However, this activity is required frequently and   is necessary to drive creative ideas. In our research we found that   advanced UI specifications should therefore be made up of   interconnected artefacts that have distinct levels of   abstraction. With regards to the transparency and traceability of   the rationale of the specification process, transitions and   dependencies must be visual and traversable. For this purpose, we   introduce a model-based user interface specification method and a   corresponding experimental tool that interactively integrates   interdisciplinary and informal models with different levels of   fidelity of user-interface prototyping. With innovative styles of   interaction and user input, our proposed tool supports the   collaboration required in a multidisciplinary context.'),
(1368,'Abstract We present an alternative interface   that allows users to perceive new sensations in virtual   environments. Gaze-based interaction in virtual environments creates   the feeling of controlling objects with the mind, arguably   translating into a more intense immersion sensation. Additionally,   it is also free of some of the most cumbersome aspects of   interacting in virtual worlds. By incorporating a real-time physics   engine, the sensation of moving something <I>real</I> is further   accentuated.</P>  <P>We also describe various simple yet effective techniques that allow eyetracking devices to enhance the three-dimensional visualization capabilities of current displays. Some of these techniques have the additional advantage of freeing the mouse from most navigation tasks.  <P>This work focuses on the study of existing techniques, a detailed description of the implemented interface and the evaluation (both objective and subjective) of the interface. Given that appropriate filtering of the data from the eye tracker used is a key aspect for the correct functioning of the interface, we will also discuss that aspect in depth.'),
(1369,'This paper presents Musimage, a novel system   which displays pictures according to the songs being played at the   same time. By using the interface the user selects the songs to be   played, but the pictures are automatically selected. For each song   to be played, the system selects a set of pictures, according to   various criteria corresponding to some features of the song. In this   sense, the pictures to be shown are, metaphorically, triggered by   the songs.'),
(1370,'The following paper introduces the work   conducted to create a relative virtual mouse based on the   interpretation of head movements and face gesture through a low cost   camera and the optical flow of the images. This virtual device is   designed specifically as an alternative non-contact pointer for   people with mobility impairments in the upper extremities and   reduced head control. The proposed virtual device was compared with   a conventional mouse, a touchpad and a digital joystick. Validation   results show performances close to a digital joystick but far away   from a conventional mouse.'),
(1371,'This paper summarizes a model of interaction for   CVEs inspired by the process followed in human communication in the   real world, detailing both the main elements and the communication   process itself. The model proposed copies some properties of the   real world communication but also allows the easy integration of   Task Analysis to the design of CVEs, helping the developer in the   design of the application. Furthermore, some of the benefits that   the usage of this model brings to the user are also shown. Finally,   some implementation details of a prototype supporting the described   model are given. This prototype is used all along the paper to   illustrate the explanation of some parts of the model.'),
(1372,'Many methods in the area of Human-Computer   Interaction have been developed for deriving user interfaces   considering individual users. However, nowadays information systems   include more than single interaction, there is a need to explicitly   include multi user interaction during the design process of   information systems. One of this approaches are workflow   systems. Workflow information systems present the view of the   organization, while modelling their business process; workflow   systems define explicitly the role of each actor during the   performance of the different tasks. The introduction of aspects such   as: organizational units, agendas, Wok list, user stereotypes and   resources, allows the design of more robust systems, especially when   all of them are considered during the development of the User   Interfaces needed. In this paper, we present a model-driven approach   to derive user interfaces of a workflow information system from a   series of models. A graphical editor has been developed. It is   described and exemplified on a real-world case study for designing   the user interfaces of a workflow information system.'),
(1373,'Accessibility and usability guidelines are   available to design web sites accessible to blind users. However,   the actual usability of accessible web pages varies depending on the   type of information the user is dealing with. Museum web sites,   including specimens and hall descriptions, need specific   requirements to allow vision-impaired users, who navigate using a   screen-reader, to access pieces of information that are mainly based   on visual perception. Here we address a methodology to be applied   for the proper creation and elaboration of alternative image   descriptions in museum web pages. Such methodology has been applied   to a gallery of the Museum of Natural History and Territory   (University of Pisa). Such indications allow the user: (1) to   address indexed contents and to link to information in more details,   (2) to calibrate image descriptions (with a command providing   alternative explanations for specimens), and (3) to access extra   information for the blind (via hidden labels). A multidisciplinary   approach is necessary to obtain effective and comprehensive   descriptions. In this perspective, a cooperative environment is   eventually proposed for team work facilitation.'),
(1374,'The evaluation of interactive systems has been   an active subject of research for many years. Many methods and tools   have been proposed but most of them do not take architectural   specificities of agent-based interactive systems into account. In   addition, electronic informers are popular evaluation tools but   current ones have often some limits. In order to solve these   problems, we propose an electronic informer to evaluate agent-based   interactive systems. This tool captures interaction events occurred   in agent-based interactive systems and then, based on such captured   data, it realizes treatments such as calculations, statistics and   generates Petri Nets (PNs) to assist evaluators in evaluating three   aspects of the system: user interface, non-functional properties   (e.g. response time, reliability, etc.)  and user?s properties   (e.g. abilities, preferences, etc.). The approach has been validated   by applying it to evaluate an agent-based interactive system used   for the supervision of urban transport network.'),
(1375,'This paper describes the taxonomy for designing   interactive groupware systems. The taxonomy defines the objectives,   methods and principles for classifying models and facilitates their   integration. In particular, we show the integration process of   models in two notations such as CIAN, which considers collaboration   and human-computer interaction issues, and UML, which allows   specifying the functionality of groupware systems. The proposed   integration process is based on a software tool, called CIAT,   developed to put our proposal into practice.'),
(1376,'We developed a free form deformation application for an immersive environment in which users can interact freely using data gloves. To ensure better comfort and performances, we added the possibility of bi-manual interaction in our environment. To investigate the actual gain obtained by this interaction technique we designed an experimental protocol based on spatial input tasks. In our experiment, we asked our subjects to use only the dominant hand to achieve the different tasks or, on the contrary, to use both hands. Comparison of users\' performances - i-e, time and precision - shows that, without proper training, executing a task using two hands can be more time consuming than using one hand. In fact, the degree of symmetry of the tasks performed with each hand seem to have a significant impact on whether or not users take advantage of bi-manual possibilities. Our results also show that bi-manual interaction can introduce proprioceptive cues that can be of help to achieve more precision in the placement or selection only when proper visual information are missing. In this study, we also wanted to investigate if bi-manual interaction can help users in their perception of the task. Even if there aren\'t statistically significant, our results shows that using symmetric bi-manual interaction, proprioception cues can improve user\'s perception.'),
(1377,'For most applications, data-intensive   applications in particular, dialog modeling makes little sense   without a domain model. Since domain models usually are developed   and used outside the dialog modeling activity, it is better to   integrate dialog modeling languages with existing domain modeling   languages and tools, than inventing your own. This paper describes   how the Diamodl language, editor and runtime have been integrated   with the Eclipse Modeling Framework.'),
(1378,'In this article we present the development of a   web application called SHARP Online: An Adaptive Hypermedia System   Applied to Mathematical Problem Solving. The pedagogical basis of   this application is found in the support techniques for heuristic   learning in mathematical problem solving developed according to the   Schoenfeld model. The adaptivity of this tool is achieved by way of   the utilization of an adaptive algorithm which has been developed   for it and is described in this article. This algorithm implements   mechanisms that make it possible for the user to construct   mathematical knowledge adaptively using training methods. This   application also provides the teacher with the following complete   set of tools for managing the entire process: the inclusion of   contents through a collaborative application with support; a shared   work space; the adaptivity of the algorithm variables; and the   supervision of the students? progress, etc. through specific   modules. This application was originally developed for educational   contexts in the area of teaching mathematics, and therefore includes   a module for editing and visualizing mathematical formulas for a Web   environment.'),
(1379,'Information Technology (IT) has evolved over   time from its traditional use as administrative support towards a   more strategic role to enforce business processes (BP). But several   organizations that adopt BP modeling as a source to implement   enterprise systems struggle to maintain such a link. To solve this   problem, most researches are focused on software engineering to   specify the association between models from business and IT to   support propagating changes. But even though there are several   techniques to control the alignment of BP and their systems, there   lacks a solution that addresses a major aspect of systems: their   User Interfaces (UI). The negative impact of focusing only on   functional aspects is that many changes on business processes that   affect user interfaces are not carefully considered. Our solution   proposes a method for the alignment of business processes with user   interfaces of systems by adopting a model-driven approach. Such   support is targeted at large organizations in order to enable them   to be more capable of managing those links. The method along with   the tool guarantees that all the models used to develop enterprise   systems are internally mapped and that any attempt to make changes   in at least one of them is alerted with warnings about the possible   impacts. We propose a practical method, adaptable to specic   organizational structures, that enables professionals to focus on   achieving organizational goals, and still puts forward users\' need   for a richer user interaction.'),
(1380,''),
(1381,'Proxima is a generic structure editor suitable for a wide range of structured document types. It allows edit operations on the document structure as well as on its screen representation (i.e. free-text editing), without the need to switch between the two modes. The system maintains a bidirectional mapping between the document structure and its presentation. Besides obvious applications, such as word-processor and spread-sheet editors, Proxima is especially well-suited for defining source editors for programming languages.  <P>Presentation-oriented edit operations require that an edited presentation can be parsed to yield an updated document structure. However, conventional parsing techniques cannot readily be applied, since presentations in Proxima are not restricted to text but may also contain graphical elements. For example, an exponential may be presented as 32. Although this graphical presentation may not be directly edited at the presentation level, its components may. Hence, instead of simply parsing the changed representation, we have to take into account the existing structure.  <P>This paper explains the scanning and parsing process for presentations that are a possibly nested combination of text and graphical elements. For textual parts of the presentation a Haskell combinator parser needs to be provided. The parser for graphical parts, on the other hand, is constructed by Proxima, based on information in the presentation. White space in the presentation can be handled automatically, if desired.'),
(1382,'Functional programs often combine separate parts   of the program using intermediate data structures for communicating   results. Programs so defined are easier to understand and maintain,   but suffer from inefficiency problems due to the generation of those   data structures. In response to this problematic, some program   transformation techniques have been studied with the aim to   eliminate the intermediate data structures that arise in function   compositions. One of these techniques is known as shortcut   fusion. This technique has usually been studied in the context of   purely functional programs. In this work we propose an extension of   shortcut fusion that is able to eliminate intermediate data   structures generated in the presence of monadic effects. The   extension to be presented can be uniformly defined for a wide class   of data types and monads.'),
(1383,'Despite all the advance brought by LALR parsing   method by DeRemer in the late 60\'s, conflicts continue to be removed   in a non-productive way, by means of analysis of a huge amount of   textual and low level data dumped by the parser generator tool. For   the purpose of changing this scenario, we present a parser generator   capable of automatically removing some types of conflicts, along   with a supported methodology that guides the process of manual   removal. We also discuss the internal algorithms and how the created   parsers are compact in terms of memory usage.'),
(1384,'This paper1 presents an instruction scheduling   algorithm based on the Subgraph Isomorphism Problem. Given a   Directed Acyclic Graph (DAG) G1, our algorithm looks for a subgraph   G02 in a base graph G2, such that G02 is isomorphic to G1. The base   graph G2 represents the arrangement of the processing elements of a   high performance computer architecture named 2D-VLIW and G02 is the   set of those processing elements required to execute operations in   G1. We have compared this algorithm with a greedy list scheduling   strategy using programs of the SPEC and MediaBench suites. In our   experiments, the average Operation Per Cycle (OPC) and Operations   Per Instruction (OPI) achieved by our algorithm are 1.45 and 1.40   times better than the OPC and OPI obtained by the list scheduling   algorithm.'),
(1385,'Weak References constitute an elegant mechanism   for an application to interact with its garbage collector. In most   of its typical uses, weak references are used through weak tables   (e.g., Java\'s <tt>WeakHashMap</tt>). However, most implementations of weak   tables have a severe limitation: Cyclic references between keys and   values in weak tables prevent the elements inside a cycle from being   collected, even if they are no longer reachable from outside. This   ends up bringing difficulties to the use of weak tables in some   kinds of applications.  <P>In this work, we present our   approach for overcoming this problem in the context of the Lua   programming language. Our approach consists of an adaptation of the   ephemerons mechanism to tables. We modified the garbage collector of   the Lua virtual machine in order to offer support to this   mechanism. With this adapted garbage collector we could verify the   efficiency and effectiveness of the implementation in solving the   problem of cycles on weak tables in Lua.'),
(1386,'The popularization of multi-core processors and   of technologies such as hyper-threading demonstrates a fundamental   change in the way processors have been evolving and also increases   interest in concurrent programming, particularly as a means to   improve software performance. However, concurrent programming is   still considered complex, mostly due to difficulties in using the   available programming models, which have been subject to recurring   criticism. The increased interest in concurrency and the lack of   proper models to support it stimulates the development of proposals   aimed at providing alternative models for concurrent programming. In   this paper, we work with some of Lua\'s facilities to explore such a   model, based on user threads and message passing. We also   demonstrate why Lua was particularly well suited for this objective,   describe the main characteristics of the explored model and present   a library developed to implement it, along with results of a   performance evaluation.'),
(1387,'Aspect languages provide different mechanisms to   control when an aspect should apply based on properties of the   execution context. They however fail to explicitly identify and   cleanly capture a property as basic as that of reentrancy. As a   result, aspect developers have to resort to low-level and complex   pointcut descriptions that are error prone and hamper the   understandability of aspect definitions. We analyze the issue of   aspect reentrancy, illustrate how current languages fail to properly   support it, and define a new linguistic construct to control aspect   reentrancy. Considering aspect reentrancy from the start in the   design of an aspect language simplifies the task of aspect   programmers by raising the level of abstraction of aspect   definitions.'),
(1388,'Extending AspectJ to experiment with new   language features can be cumbersome, even with an extensible   implementation. Often, a language designer only needs a rapid   prototyping environment, but has to deal with a full compiler   infrastructure, and must address low-level implementation   issues. This work completes a lightweight extensible implementation   of AspectJ with a declarative assimilation layer based on   Stratego. This layer brings together an extensible syntax definition   of AspectJ and the core semantics provided by the Reflex AOP   kernel. Using this implementation, language extensions are defined   using declarative high-level constructs, significantly reducing the   cost of the extension process.'),
(1389,'The latest versions of AspectJ, the most popular   aspect-oriented extension for Java, must cope with the complex   changes that occurred in the Java type system, specially with those   that introduced type parameters for classes and methods. In this   work we study the influence of raw types, i.e. parameterless   instantiations of class types, over the semantics of an AspectJ-like   language. As a result, we define an operational semantics and a type   system for a calculus, named Raw Aspect Featherweight Generic Java   (Raw-AFGJ), that represents a minimal aspect-oriented extension of   Raw Featherweight Generic Java. Through our calculus it is possible   to achieve a better understanding of several subtleties of aspect   weaving with the restrictions imposed by raw types support in the   type system.'),
(1390,'Denotational semantics is a powerful technique   to formally define programming languages. However, language   constructs are not always orthogonal, so many semantic equations in   a definition may have to be aware of unrelated constructs   semantics. Current approaches for modularity in this formalism do   not address this problem, providing, for this reason, tangled   semantic definitions. This paper proposes an incremental approach   for denotational semantic specifications, in which each step can   either add new features or adapt existing equations, by means of a   formal language based on function transformation and aspect   weaving.'),
(1391,'The abstract syntax and static semantics of UML,   the widely-used generalpurpose graphical modeling language, have   been standardized in a four-layer metamodeling framework. However   UML\'s dynamic semantics, such as UML Precise Action Semantics and   the behaviors like activities, interactions and state machines, are   only standardized in a natural language-English. It is commonly   argued that such informal description inevitably involves   ambiguities and lacks rigorousness, precluding the early simulation   and reasoning about a UML system design. Here we select Action   Semantics (AS) as the vehicle to formalize UML. AS is a mature   semantics description framework which has advantages of   intelligibility, modularity and practicability. In our approach, we   formalize UML indirectly by formalizing its textual   correspondent-an extended Action Language, which plays a key role   as the interface between UML and its action semantics.'),
(1392,''),
(1393,'Software Product Lines (SPL) may be adopted by   either bootstrapping existing software products into a SPL, or   extending an existing SPL to encompass an additional software   product. Program refactorings are usually applied for carrying out   those tasks. The notion of SPL refactoring is an extension of the   traditional definition of refactoring; it involves not only program   refactorings, but also Feature Model (FM) refactorings, in order to   improve configurability. However, FM refactorings are hard to   define, due to the incompleteness of the refactoring catalogs   developed as of today. In this paper, we propose a complete, sound   catalog of algebraic laws, making up special FM refactorings that   preserve configurability. This catalog is also defined as minimal,   as one law cannot be derived from another one in the same   catalog. In addition, a theory for FMs is presented, in the context   of a theorem prover.'),
(1394,''),
(1395,'After more than ten years of research aiming at defining methods and techniques to deliver personalized instruction, Adaptive Educational Hypermedia Systems have not made the jump into real practice systems. Reasons for this include the complexity of their development, their use of exclusive methods for defining adaptivity and educational elements, and their lack of interoperation amongst courses and applications. A possible alternative to cope with these issues is using as a common notational method the IMS Learning Design specification. This paper attempts to bring AEHS and IMS LD closer to each other in order to define adaptivity behaviour. To this end, it outlines how IMS LD could be used to define personalization properties and adaptive techniques and, based on that, it proposes a component called Adaptive Learning Designs, and an authoring tool to create these components. Furthermore, the paper discusses the benefits and limitations of IMS LD to define adaptivity behaviour, and ends suggesting additional research lines.'),
(1396,'Working constructively, we discuss two types of   maximality for ideals in a commutative ring with identity, showing   also that the results are the best possible.'),
(1397,'In order to enhance their global business   performance, organizations must be careful with the quality of their   information since it is one of their main assets. Analogies to   quality management of classical products demonstrate that   Information Quality is also preferably attainable through management   by integrating some corresponding Information Quality management   activities into the organizational processes. To achieve this goal   we have developed an <I>Information Quality Management   Framework</I> (IQMF). It is articulated on the concept of   <I>Information Management Process</I> (IMP), based on   the idea of Software Process. An IMP is a combination of two   sub-processes: the first, a production process, aimed to manufacture   information from raw data, and the second to adequately manage the   required Information Quality level of the first. IQMF consists of   two main components: an <I>Information Quality Management   Maturity Model</I> (IQM3), and a <I>Methodology for the   Assessment and Improvement of Information Quality   Management</I> (MAIMIQ), which uses IQM3 as a reference model   for the assessment and for the improvement goal of an   IMP. Therefore, as a result of an assessment with MAIMIQ, an IMP can   be said to have raised one of the maturity levels described in IQM3,   and as improvement goal, it would be desirable to achieve a higher   maturity level. Since an Information System can be seen as a set of   several IMPs sharing several resources, it is possible to improve   the Information Quality level of the entire Information System by   improving the most critical IMPs. This paper is focused only on   describing the foundations and structure of IQM3, which is based on   staged CMMI.'),
(1398,'Since all the algebras connected to logic have,   more or less explicitely, an associated order relation, it follows   that they have two presentations, dual to each other. We classify   these dual presentations in \"left\" and \"right\" ones and we consider   that, when dealing with several algebras in the same research, it is   useful to present them unitarily, either as \"left\" algebras or as   \"right\" algebras. In some circumstances, this choice is essential,   for instance if we want to build the ordinal sum (product) between a   BL algebra and an MV algebra. We have chosen the \"left\" presentation   and several algebras of logic have been redefined as particular   cases of BCK algebras.</P>  <P>We introduce several new properties of algebras of logic, besides those usually existing in the literature, which generate a more refined classification, depending on the properties satisfied. In this work (Parts I-V) we make an exhaustive study of these algebras - with two bounds and with one bound - and we present classes of finite examples, in bounded case.</P>  <P>In this Part I, divided in two because of its length, after surveying chronologically several algebras related to logic, as residuated lattices, Hilbert algebras, MV algebras, divisible residuated lattices, BCK algebras, Wajsberg algebras, BL algebras, MTL algebras, WNM algebras, IMTL algebras, NM algebras, we propose a methodology in two steps for the simultaneous work with them (the first part of Part I).</P>  <P>We then apply the methodology, redefining those algebras as particular cases of reversed left-BCK algebras. We analyse among others the properties Weak Nilpotent Minimum and Double Negation of a bounded BCK(P) lattice, we introduce new corresponding algebras and we establish hierarchies (the subsequent part of Part I).'),
(1399,'This paper presents efficient methods for online   fault detection and diagnosis of Network-on-Chip (NoC) switches. The   fault model considered in this research is a system level fault   model based on the generic properties of NoC switch   functionality. The proposed method is evaluated by fault simulation   in a platform using this system level fault model. The experimental   results show that with a relatively low area overhead,a large   number of NoC switch faultscan be detected and diagnosed.'),
(1400,'The investments needed to bring a software   project to the market are substantial and can extend over several   years. Managing software development requires not only technical   expertise, but communication with funders and economists.  This   paper presents methods to estimate a parameter which captures the   effective investment time, lag.  The lag parameter is useful in   assessing progress towards the goal of having a quality product,   while scheduling resources, assessing the risk, considering options,   capitalization of investments, and predicting taxation   consequences. The paper presents the lag estimation methods for a   new product, for additional versions of a product, and for complete   product replacement.'),
(1401,'The desideratum of semantic interoperability has   been intensively discussed in medical informatics circles in recent   years. Originally, experts assumed that this issue could be   sufficiently addressed by insisting simply on the application of   shared <I>clinical terminologies</I> or   <I>clinical information models</I>. However, the use of   the term \'<I>ontology</I>? has been steadily increasing   more recently. We discuss criteria for distinguishing   <I>clinical ontologies</I> from <I>clinical   terminologies</I> and <I>information   models</I>. Then, we briefly present the role clinical   ontologies play in two multicentric research projects. Finally, we   discuss the interactions between these different kinds of knowledge   representation artifacts and the stakeholders involved in developing   interoperational real-world clinical applications. We provide   ontology engineering examples from two EU-funded projects.'),
(1402,'Most information in Hospitals is still only   available in text format and the amount of this data is immensely   increasing. Consequently, text mining is an essential area of   medical informatics. With the aid of statistic and linguistic   procedures, text mining software attempts to dig out (mine)   information from plain text. The aim is to transform data into   information. However, for the efficient support of end users, facets   of computer science alone are insufficient; the next step consists   of making the information both usable and useful. Consequently,   aspects of cognitive psychology must be taken into account in order   to enable the transformation of information into knowledge of the   end users. In this paper we describe the design and development of   an application for analyzing expert comments on magnetic resonance   images (MRI) diagnoses by applying a text mining method in order to   scan them for regional correlations. Consequently, we propose a   calculation of significant co-occurrences of diseases and defined   regions of the human body, in order to identify possible risks for   health.'),
(1403,'The recent literature on business process   modeling notations contains numerous contributions to the so-called   OR-join (or inclusive merge gateway) problem. We analyze the problem   and present an approach to solve it without compromising any of the   two major concerns that are involved: a) a clear semantical   definition (design), which also clarifies what has to be implemented   to achieve the intended generality of the construct, and b) a   comprehensive set of static and dynamic analysis methods   (verification of properties of business process models using the   construct). We provide a conceptually simple scheme for dynamic   OR-join synchronization policies, which can be implemented with low   run-time overhead and allows the practitioner to effectively link   the design of business process models with OR-joins to an analysis   of the intended model properties. The definitions have been   experimentally validated by a graph-based simulator.'),
(1404,'Unlike relational tables in a database, data   sources on the Web typically can only be accessed in limited   ways. In particular, some of the source fields may be required as   input and thus need to be mandatorily filled in order to access the   source. Answering queries over sources with access limitations is a   complex task that requires a possibly recursive evaluation even when   the query is non-recursive. After reviewing the main techniques for   query answering in this context, in this article we consider the   impact of functional and inclusion dependencies on dynamic query   optimization under access limitations. In particular, we address the   implication problem for functional dependencies and simple   full-width inclusion dependencies, and prove that it can be decided   in polynomial time. Then we provide necessary and sufficient   conditions, baseon the dependencies together with the data retrieved   at a certain step of the query answering process, that allow   avoiding unnecessary accesses to the sources.'),
(1405,'Segerberg established an analogue of the   canonical model theorem in modal logic for infinitary modal   logic. However, the logics studied by Segerberg and Goldblatt are   based on denumerable sets of pairs ,  of   sets  of well-formed formulae and well-formed formulae   . In this paper I show how a generalisation of the infinite   cut-rule used by Segerberg and Goldblatt enables the removal of the   limitation to denumerable sets of sequents.'),
(1406,'Verification of concurrent algorithms has been   the focus of much research over a considerable period of time, and a   variety of techniques have been developed that are suited to   particular classes of algorithm, for example algorithms based on   message passing or mutual exclusion. The development of   <I>nonblocking</I> or <I>lock-free</I>   algorithms, which rely only on hardware primitives such as Compare   And Swap, present new challenges for verification, as they allow   greater levels of currency and more complex interactions between   processes.  </P> <P>In this paper, we describe and   compare two approaches to reasoning about nonblocking algorithms. We   give a brief overview of the <I>simulation</I> approach   we have used in previous work. We then give a more detailed   description of an approach based on Lipton\'s   <I>reduction</I> method, and illustrate it by verifying   two versions of a shared counter and two versions of a shared   stack. Both approaches work by transforming a concurrent execution   into an equivalent sequentia-execution, but they differ in the way   that executions are transformed and the way that transformations are   justified.'),
(1407,'We study inference systems of weak functional dependencies in relational and complex-value databases. Functional dependencies form a very common class of database constraints. Designers and administrators proficiently utilise them in everyday database practice. Functional dependencies correspond to the linear-time decidable fragment of Horn clauses in propositional logic. Weak functional dependencies take advantage of arbitrary clauses, and therefore represent full propositional reasoning about data in databases. Moreover, they can be specified in a way that is very similar to functional dependencies.</P>  <P>In relational databases the class of weak functional dependencies is finitely axiomatisable and the associated implication problem is <I>coNP</I>-complete in general. Our first main result extends this axiomatisation to databases in which complex elements can be derived from atomic ones by finitely many nestings of record, list and disjoint union constructors. In particular, we construct two nested tuples that can serve as a counterexample relation for the implication of weak functional dependencies. We further apply this construction to show an equivalence to truth assignments that serve as counterexamples for the implication of propositional clauses. Hence, we characterise the implication of weak functional dependencies in complex-value databases in completely logical terms. Consequently, state-of-the-art SAT solvers can be applied to reason about weak functional dependencies in relational and complex-value databases.'),
(1408,'Transaction management is an essential component of database management systems. It enables multiple users to access the database concurrently while preserving transactional properties such as atomicity, consistency, isolation, and durability.</P>  <P>In this paper, we propose a formal framework specification for transaction processing. Our work can be seen as an extension of previous work by Gurevich et al. who have presented a formalism for general database recovery processing. Based on this formalism, we incorporate additional mechanisms that remove several explicit constraints, support normal transaction processing, and, most importantly, apply the approach to more advanced recovery mechanisms.'),
(1409,'A common approach in designing relational databases is to start with a universal relation schema, which is then decomposed into multiple subschemas. A good choice of subschemas can be determined using integrity constraints defined on the schema, such as functional, multivalued or join dependencies.</P>  <P>In this paper we propose and analyze a new normal form based on the idea of minimizing overall storage space and update costs, and as a consequence redundancy as well. This is in contrast to existing normal forms such as BCNF, 4NF or KCNF, which only characterize the absence of redundancy (and thus space and update time minimality) for a single schema. We show that our new normal form naturally extendexisting normal forms to multiple schemas, and provide an algorithm for computing decompositions.'),
(1410,'The rapid growth of the World Wide Web has   resulted in a dramatic increase in semistructured data usage,   creating a growing need for effective and efficient utilization of   semistructured data. In order to verify the correctness of   semistructured data design, precise descriptions of the schemas and   transformations on the schemas must be established. One effective   way to achieve this goal is through formal modeling and automated   verification. This paper presents the first step towards this   goal. In our approach, we have formally specified the semantics of   the ORA-SS (Object-Relationship-Attribute data model for   Semistructured data) data modeling language in PVS (Prototype   Verification System) and provided automated verification support for   both ORA-SS schemas and XML (Extensible Markup Language) data   instances using the PVS theorem prover. This approach provides a   solid basis for verifying algorithms that transform schemas for   semistructured data.'),
(1411,'OLAP applications are widely used in business applications. They are often (implicitly) defined on top of OLTP systems and extensively use aggregation and transformation functions. The main OLAP data structure is a multidimensional table with three kinds of attributes: so-called dimension attributes, implicit attributes given by aggregation functions and fact attributes. Domains of dimension attributes are structured and thus support a variety of aggregations. These aggregations are used to generate new values for the fact attributes. In this paper we systematically develop a theory for OLAP applications. We first define aggregation functions and use these to introduce an OLAP algebra. Based on these foundations we derive properties that guarantee or contradict correctness of OLAP computations. Finally, for pragmaticatreatment of OLAP applications the OLTP-OLAP specification frame is introduced.'),
(1412,'We investigate properties of coincidence ideals   in subattribute lattices that occur in complex value datamodels,   i.e. sets of subattributes, on which two complex values coincide. We   let complex values be defined by constructors for records, sets,   multisets, lists, disjoint union and optionality, i.e. the   constructors cover the gist of all complex value data models. Such   lattices carry the structure of a Brouwer algebra as long as the   union-constructor is absent, and for this case sufficient and   necessary conditions for coincidence ideals are already known. In   this paper, we extend the characterisation of coincidence ideals to   the most general case. The presence of the disjoint union   constructor complicates all results and proofs significantly. The   reason for this is that the union-constructor causes non-trivial   restructuring rules to hold. The characterisation of coincidence   ideal is of decisive importance for the axiomatisation of (weak)   functional dependencies.'),
(1413,'On-line analytical processing (OLAP) systems   deal with analytical tasks that support decision making. As these   tasks do not depend on the latest updates by transactions, it is   assumed that the data required by OLAP systems are kept in a data   warehouse, which separates the input from operational databases from   the outputs to OLAP. However, user requirements for OLAP systems   change over time. Data warehouses and OLAP systems thus are rather   dynamic and the design process is continuous. In order to easily   incorporate new requirements and at the same time ensure the quality   of the system design, we suggest to apply the Abstract State Machine   (ASM) based development method. This assumes we capture the basic   user requirements in a ground model and then apply stepwise   refinements to the ground model for every design decisions or   further new requirements. In this article, we show that a   systematical approach which is tailored for data warehouse design   with a set of formal refinement rules can simplify the work in   dynamic data warehouse design and at the same time improves the   quality of the system.'),
(1414,''),
(1415,'Business models involving buyers of digital   goods in the distribution process are called superdistribution   schemes. We review the state-of-the art of research and application   of superdistribution and propose a systematic approach to market   mechanisms using super-distribution and technical system   architectures supporting it. The limiting conditions on such markets   are of economic, legal, technical, and psychological nature. Large   scale applications of superdistribution such as video-on-demand and   multimedia over peer-to-peer type networks pose particular   requirements on security and efficiency.'),
(1416,'At present, the proposed authentication schemes can be classified into three categories. The first category is the watermarking authentication schemes in which the watermark is independent of the multimedia content. The second category is the signature-based authentication schemes in which the signature is generated by the multimedia content and is not embedded into the multimedia content. The third category is the content-based watermarking authentication schemes in which the watermark is generated by the multimedia content. However, there exists the security question in the above-mentioned three categories of the authentication schemes. In this paper, a novel concept that is called \"authentication set\" is used to analyze the security of the authentication schemes in detail. Several novel concepts on the authentication set are defined, which are called \"Cover Authentication Set\", \"Attack Authentication Set\", \"Watermark-based Authentication Set\" or \"Signature-based Authentication Set\", \"Verified Authentication Set\" and \"Malicious-attack Authentication Set\". According to the relation among the aforementioned sets, the security of the authentication schemes is exploited. Furthermore, a conclusion is drawn according to the analysis result. At the same time the principle that guides the design of the more secure authentication schemes is presented. Finally, as an example, a novel authentication design method based on multi-feature watermarks is proposed according to the design principle. The experimental results prove the validity of the design method and the significance of the guide principle.'),
(1417,'A distinguishing feature of today\'s large-scale platforms for multimedia distribution and communication, such as the Internet, is their heterogeneity, predominantly manifested by the fact that a variety of communication protocols are simultaneously running over different hosts. A fundamental question that naturally arises for such common settings of heterogeneous multimedia systems concerns the presence (or not) of stability properties when individual greedy, contention-resolution protocols are composed in a large packet-switched multimedia network. A network is stable under a greedy protocol (or a composition of protocols) if, for any adversary of injection rate less than 1, the number of packets in the network remains bounded at all times. We focus on a basic adversarial model for packet arrival and path determination for which the time-averaged arrival rate of packets requiring a single edge is no more than 1. Within this framework, we study the property of stability under various compositions of contention-resolution protocols (such as LIS (<I>Longest-in-System</I>), FIFO (<I>First-In-First-Out</I>), FFS (<I>Furthest-from-Source</I>), and NTG (<I>Nearest-to-Go</I>)) and different packet trajectories trying to characterise this property in terms of network topologies. Such a characterisation provides us with the family of network topologies that, under specific compositions of protocols, can be made unstable by some adversarial traffic pattern. Finally, we present an experimental evaluation of the stability behaviour of specific network constructions with different protocol compositions under an adversarial strategy. Interestingly, some of our results indicate that such a composition leads to worst stability behaviour than having a single unstable protocol for contention-resolution. This suggests that the potential for instability incurred by the composition of protocols may be worse than that of any single protocol.'),
(1418,'Peer to peer (P2P) network has received in past few years a significant attention, especially such file sharing network as eDonkey [Kulbak and Kirkpatrick, 2005] or BitTorrent [Cohen 2008]. The shift from the classical client-server based paradigm of the Internet, with a clear distinction between information providers and consumers, towards consumers sharing information among each other led to the rise of the P2P paradigm. This distributed architecture, enables users to share content autonomously; Information remains at end-users\' computers at the edge of the Internet and is not gathered and organized at central servers. While P2P has emerged as a new hot communication concept among the Internet users, security concerns still taking its first steps. The deployment of classic security protocols to provide services such as node authentication, content integrity or access control, presents several difficulties, most of them are due to the decentralized nature of these environments and the lack of central authorities. The fast emergence and the open nature of P2P applications, make appearing new attacks, so it is extremely important to study them and develop new counter measurements. Furthermore, existing studies focus on attacks that disrupt the overlay functioning and does not take in account their impact on ISPs (Internet Service Provider) infrastructure. In this paper, we present the Topology Change Attack [Abdelouahab et al. 2008] that harms the underlying networks (ISPs infrastructure) by unbalancing the P2P workload repartition. In order to evaluate and validate the TCA impact, we developed a new cycle-based simulator which simulates eDonkey clients hosted on different ISPs. The obtained results are very interesting and show the increasing of inter-ISPs traffic when a Topology Change Attack is conducted.'),
(1419,'This study has proposed a new detection method   for DDoS attack traffic based on two-sample t-test. We first   investigate the statistics of normal SYN arrival rate (SAR) and   confirm it follows normal distribution. The proposed method   identifies the attack by testing 1) the difference between incoming   SAR and normal SAR, and 2) the difference between the number of SYN   and ACK packets. The experiment results show that the possibilities   of both false positives and false negatives are very low. The   proposed mechanism is also demonstrated to have the capability of   detecting DDoS attack quickly.'),
(1420,''),
(1421,'This paper presents findings from the   development process of a general innovation framework for an ongoing   Nordic RD project on e-business and media. It focuses on the   current state of the Danish news media sector and the conclusions we   can draw from the Web 2.0 activity of the Danish   newspapers. The paper concludes that the Web 2.0 implies the need   for fundamental re-thinking of the business models of the news media   sector and for developing a new framework for business modelling for   this sector.'),
(1422,'In today\'s information environments, tagging is   widely used to provide information about arbitrary types of digital   resources. This information is usually created by end users with   different motivations and for different kinds of purposes. When   aiming to support users in the tagging process, these differences   play an important role. In this paper several approaches to generate   tag recommendations are discussed, and a prototypical recommender   system for the social resource sharing platform ALOE is   presented. This interactive system allows users to control the   generation of the recommendations by selecting the sources to be   used as well as their impact. The component was introduced at DFKI,   and a first evaluation showed that the recommender component was   considered as helpful by a majority of users.'),
(1423,'In this paper we propose a classification and   systematic description structure based on the pattern paradigm for   interaction scripts in Second Life that aim at facilitating on the   one side knowledge sharing and knowledge integration in groups, and   on the other side knowledge creation in formal and informal ways. We   present 13 examples of interaction patterns, a description structure   to formalize them, and classify them into four classes according to   their design effort and added value. Based on this classification we   distinguish among sophisticated 3D collaboration patterns, seamless   patterns, decorative patterns, and pseudo patterns.'),
(1424,'One of the benefits firms can derive from using   Open Source Software (OSS) is informal development collaboration,   and the primary tool for collaboration and coordination are group   mailing lists. The purpose of the paper is modelling mailing lists   behaviour in OSS projects, using a set of descriptors that could   inform about their quality and their evolution. As a case study, a   mailing list focused on ARM embedded Linux has been   selected. Messages posted to this list from 2001 to 2006 have been   extracted, and factor analysis has been applied to obtain the   underlying patterns of behaviours. Theory about communities of   practice has been used to understand the meaning of the extracted   patterns. Their time distribution is finally described. The paper   provides new insights into the behaviour of mailing list as a source   of support for OSS projects and highlights the importance of an   involved core of individuals inside the community.'),
(1425,'Structural aspects modify the structure of a   program, for instance by adding fields and methods to existing   classes. Like behavioral aspects, which operate on execution events,   structural aspects may interact and raise conflicts. Current aspect   systems however do not thoroughly handle this issue. This paper   discusses how complete support for structural aspect composition can   be integrated in an AOP kernel, that is, a generic transformation   framework on top of which aspect languages are defined. An iterative   composition process is proposed that involves the programmer in a   cycle of automatic detection of interactions and explicit,   declarative resolution of these interactions. Beyond a general   analysis of the issue of composition of structural aspects and an   associated composition process, this work reports on the concrete   extension of the Reflex AOP kernel to fully support the requirements   drawn from our analysis. Based on a structural model supporting   per-aspect subjective views, and using the power of an embedded   logic engine, the result is a versatile aspect system supporting   automatic detection of various kinds of structural aspect   interactions, extensible reporting tools, and declarative mechanisms   for the resolution of interactions between structural   aspects.'),
(1426,'Adequacy criteria provide an objective   measurement of test quality. Although these criteria are a major   research issue in software testing, little work has been   specifically targeted towards the testing of database-driven   applications. In this paper, two structural coverage criteria are   provided for evaluating the adequacy of a test suite for SQL queries   that retrieve information from the database. The first deals with   the way in which the queries select and join information from   different tables and the second with the way in which selected data   is further processed. The criteria take into account both the   structure and the data loaded in the database, as well as the syntax   and semantics of the query. The coverage criteria are subsequently   used to develop test inputs of queries drawn from a real-life   application. Finally, a number of issues related to the kind of   faults that can be detected and the size of the test suite are   discussed.'),
(1427,'The participation of an e-notary, acting as an   on-line Trusted Third Party is required in some scenarios, such as   Business to Business, Intellectual Property Rights contracting, or   even as a legal requirement, in contract signing is frequently   necessary. This e-notary gives validity to the contract or performs   some tasks related to the contract, e.g. contract registration. In   the abovementioned contracting scenarios, two important additional   features are needed: the negotiation of the e-contract and   confidentiality. However, until now, e-contract signing protocols   have not considered these issues as an essential part of the   protocol. In this paper, we present a new protocol which is designed   to make negotiation and contract signing processes secure and   confidential. Moreover, compared to other previous proposals based   on an on-line Trusted Third Party, this protocol reduces the   e-notary?s workload. Finally, we describe how the protocol is being   used to achieve agreements on the rights of copyrighted   works.'),
(1428,'Besides the well established access control models, Discretionary Access Control (DAC) and Mandatory Access Control (MAC), the policy neutral Role Based Access Control (RBAC) is gaining increasingly momentum. An important step towards a wide acceptance of RBAC has been achieved by the standardization of RBAC through the American National Standards Institute (ANSI) in 2004.</P>   <P>While the concept of sessions specified in the ANSI RBAC standard allows for differentiated role selections according to tasks that have to be performed by users, it is very likely that more roles will be activated in a session than are effectively needed to perform the intended activity. Dynamic Sessions in RBAC (DS RBAC) is an extension to the existing RBAC ANSI standard that dynamically deactivates roles in a session if they are not exercised for a certain period of time. This allows for the selection of an outer-shell of possibly needed permissions at the initation of a session through a user, while adhering to the principle of least privilege by automatically reducing the effective permission space to those roles really exercised in the session.</P>   <P>Analogous to the working set model known from virtual memory, only the minimal roles containing permissions recently exercised by the user are left in a session in the DS RBAC model. If the user tries to access a role that has aged out due to inactivity, a role fault occurs. A role fault can be resolved by the role fault handler that is responsible for re-activating the expired role. As will be presented in this paper, role re-activation may be subject to constraints that have to be fulfilled by the user in order to re-access the aged role.'),
(1429,'In this paper we review the existing   <I>linear</I> and <I>quadratic</I>   complexity (upper) bounds on the values of the positive roots of   polynomials and their impact on the performance of the   Vincent-Akritas-Strzebo&#324;ski (<tt>VAS</tt>)   continued fractions method for the isolation of real roots of   polynomials. We first present the following four linear complexity   bounds (two \"old\" and two \"<I>new</I>\" ones,   respectively): <I>C</I>auchy\'s, (<I>C</I>),   <I>K</I>ioustelidis\', (<I>K</I>),   <I>F</I>irst-Lambda, (<I>FL</I>) and   <I>L</I>ocal-<I>M</I>ax,   (<I>LM</I>); we then state the quadratic complexity   extensions of these four bounds, namely: <I>CQ</I>,   <I>KQ</I>, <I>FLQ</I>, and   <I>LMQ</I>  the second, (<I>KQ</I>),   having being presented by Hong back in 1998. <I>All</I>   eight bounds are derived from Theorem 5 below. The estimates   computed by the quadratic complexity bounds are less than or equal   to those computed by their linear complexity counterparts. Moreover,   it turns out that <tt>VAS(lmq)</tt>  the   <tt>VAS</tt> method implementing <I>LMQ</I>  is 40%   faster than the original version   <tt>VAS(cauchy)</tt>.'),
(1430,'During the last few years homomorphic encryption   techniques have been studied extensively since they have become more   and more important in many different cryptographic protocols such as   voting protocols, lottery protocols, anonymity, privacy, and   electronic auctions.</P> <P> This paper critically summarizes the current state-of-art of homomorphic cryptosystems. It recalls the basic ideas, discusses their parameters, performances and security issues. And, finally we present their capabilities in the future applications.'),
(1431,''),
(1432,'In this paper, Direction Cosine Matrix (DCM)   method for attitude and orientation estimation is discussed. DCM   method was chosen due to some advantages over the popular methods   such as namely Euler Angle, Quaternion in light of reliability,   accuracy and computational efforts. Proposed model for each method   is developed for methodology comparison. It is shown that normal   Kalman Filter in DCM method is better than extended Kalman Filter in   Euler and Quaternion based method because it helps avoid the first   order approximation error.  Methodology errors are verified using   Aerospace Blockset of Matlab Simulink.'),
(1433,'Most previous research on ontology integration   has focused on similarity measure-ments between ontological   <I>entities</I>, e.g., <I>lexicons</I>,   <I>instances</I>, <I>schemas</I> and   <I>taxonomies</I>, resulting in high computational costs   of considering all possible pairs between two given ontologies. In   this paper, we propose a novel approach to reducing computational   complexity in ontology integration. Thereby, we address the   <I>importance</I> and <I>types of   concepts</I>, for priority matching anddirect matching between   concepts, respectively. <I>Identity-based similarity</I>   is computed, to avoid comparisons of all properties related to each   concept, while matching between concepts. Theproblem of conflict in   ontology integration has initially been explored on the   <I>instance-level</I> and   <I>concept-level</I>. This is useful to avoid many cases   of mismatching.'),
(1434,'This paper presents an efficient data preprocessing procedure for the of support vector clustering (SVC) to reduce the size of a training dataset. Solving the optimization problem and labeling the data points with cluster labels are time-consuming in the SVC training procedure. This makes using SVC to process large datasets inefficient. We proposed a data preprocessing procedure to solve the problem. The procedure contains a shared nearest neighbor (SNN) algorithm, and utilizes the concept of unit vectors for eliminating insignificant data points from the dataset. Computer simulations have been conducted on artificial and benchmark datasets to demonstrate the effectiveness of the proposed method.'),
(1435,'Differential evolution (DE) algorithm puts emphasis particularly on imitating the microscopic behavior of individuals, while estimation of distribution algorithm (EDA) tries to estimate the probabilistic distribution of the entire population. DE and EDA can be extended to multi-objective optimization problems by using a Pareto-based approach, called Pareto DE (PDE) and Pareto EDA (PEDA) respectively. In this study, we describe a novel combination of PDE and PEDA (PDE-PEDA) for multi-objective optimization problems by taking advantage of the global searching ability of PEDA and the local optimizing ability of PDE, which can, effectively, maintain the balance between exploration and exploitation. The basic idea is that the offspring population of PDE-PEDA is composed of two parts, one part of the trial solution generated originates from PDE and the other part is sampled in the search space from the constructed probabilistic distribution model of PEDA. A scaling factor Pr used to balance contributions of PDE and PEDA can be adjusted in an on-line manner using a simulated annealing method. At an early evolutionary stage, a larger Pr should be adopted to ensure PEDA is used more frequently, whereas at later stage, a smaller Pr should be adopted to ensure that offspring is generated more often using PDE. The hybrid algorithm is evaluated on a set of benchmark problems and the experimental results show that PDE-PEDA outperforms the NSGA-II and PDE algorithms.'),
(1436,'For de novo pattern mining in genomic sequences, the main issues are constructingpattern definition model (PDM) and mining sequence patterns (MSP). The representations of PDMs and the discovery of patterns are functionally dependent; the performances thus dependon the adopted PDMs. The popular PDMs provide only descriptive patterns; they lack multifaceted considerations. Many of existing MSP methods are tied up with the exclusively devisedPDMs, and the specialized and sophisticated models make the mined results hard to be reused. In this research, an integrative pattern mining system is proposed, which consists of a computation-oriented PDM (CO-PDM) and general-purpose MSP (GP-MSP) methods. The CO-PDM defines four computational concerns (CCs) as facets of MSP: expression (E), location (L), range (R)and weight (W), which are integrated into a frame-relayed pattern model (FRPM). The GP-MSP develops a frame-relayed search strategy to resolve the ELR-CCs firstly, with the aids of critical-parameter automating (CPA) procedure; and then the W-CC is determined by hybridizing particle swarm optimization (PSO) and artificial neural network (ANN). The proposed FRPM andGP-MSP had been implemented and applied to 22,448 human introns; from the results, all the well-known patterns were recovered and some new ones were also discovered. Furthermore, theeffectiveness of identified patterns were verified by a two-layered k-nearest neighbor (k-NN) classifier; the average precision and recall are 0.88 and 0.92, respectively. By the case study, theintegrative PDM-MSP system is believed to be effective and reliable; it is optimistic the proposed CO-PDM and GP-MSP are both widely applicable and reusable for mining sequence patterns inthe eukaryotic protein-coding genes.'),
(1437,'This paper presents a quantum-inspired immune algorithm (QIA) for Hybrid flow shop problems (HFSP) to minimize makespan. Since HFSP have been proved to be NP-hard in a strong sense when the objective is to minimize the makespan, an effective immune algorithm (IA) is used to solve the problems. IA is a kind of evolutional computation strategies, which is developed on the basis of a real immune mechanism in the human body, and has been employed to tackle complex scheduling problems and produce a reasonable manufacturing schedule. In order to achieve better results, the standard IA is combined with quantum algorithm (QA), which is based on Q-bit and uses quantum rotation gate to update. A real number representation is proposed to convert the Q-bit representation to job permutation for evaluating value of solutions. The proposed QIA can overcome the limitations of IA, quicken up convergence speed and improve the solution. Forty one benchmarks are examined to validate the efficiency of the proposed algorithm. The computational experiments show that the proposed QIA can also obtain both better and more robust results than IA and QA.'),
(1438,'In this paper we present an approach to detect traffic guidance signs and recognise the structure of junction information on them. The detection algorithm is based on using differently exposed images. These images are combined into one using tone mapping technique in order to minimize effects of bad environment conditions and low dynamic range of CCD-cameras. This technique allows robust sign detection in various lighting conditions. To localize sign candidates color segmentation is used. To minimize number of false detection filtering operations based on geometrical and color properties is applied. Recognition process is based on graph theory. Each sign candidate is decomposed into principal components and the region which represents junction structure is mapped into a graph. This graph is checked for possible mapping mistakes. Finally, the graph is analyzed in order to extract all possible paths of junction crossing. These paths must represent the real structure of the junction and correspond to the road law. The proposed method allows more effective detection in different lighting and environmental conditions such as insufficient or excessive lighting, rain, fog etc compared with conventional approaches.'),
(1439,'In this paper a novel technique for identifying   lexical contexts in web resources is presented. The basic idea is to   consider web site anchortexts as lexicalized descriptions of an   individual ontology organized in the form of a graph of concept   words. In the search for peculiar semantic patterns, the concept of   web minutia (transposed from the forensic domain) is introduced. The   proposed technique consists in searching for web minutiae in the   analyzed web sites by means of a golden ontology. Web minutiae act   as fingerprints for context-specific web resources; in this sense   they are a powerful computational tool to identify and categorize   the Web. The WordNet database has been used as golden ontology for   our experiments on English web documents. WordNet allows for   indexing and retrieving word senses and inter-word taxonomical   relations like hyponymy and hypernymy. It has proven to be an   efficient mediator between web ontologies and context-dependent   taxonomies. Our experiments have been carried out on a preliminary   data set of several tens of thousand links taken by web sites of   thirteen UK universities. Preliminary results seem to confirm the   ability of web minutiae to identify lexical contexts across the Web.'),
(1440,'Inferring gene regulatory networks from data   requires the development of algorithms devoted to structure   extraction. When time-course data is available, gene interactions   may be modeled by a Bayesian Network (BN). Given a structure, that   models the conditional independence between genes, we can tune the   parameters in a way that maximize the likelihood of the observed   data. The structure that best fit the observed data reflects the   real gene network\'s connections. Well known learning algorithms   (greedy search and simulated annealing) devoted to BN structure   learning have been used in literature. We enhanced the fundamental   step of structure learning by means of a classical evolutionary   algorithm, named GA (Genetic algorithm), to evolve a set of   candidate BN structures and found the model that best fits data,   without prior knowledge of such structure. In the context of genetic   algorithms, we proposed various initialization and evolutionary   strategies suitable for the task. We tested our choices using   simulated data drawn from a gene simulator, which has been used in   the literature for benchmarking [Yu et al. (2002)]. We assessed the   inferred models against this reference, calculating the performance   indicators used for network reconstruction.  The performances of the   different evolutionary algorithms have been compared against the   traditional search algorithms used so far (greedy search and   simulated annealing). Finally we individuated as best candidate an   evolutionary approach enhanced by Crossover-Two Point and Selection   Roulette Wheel for the learning of gene regulatory networks with BN.   We show that this approach outperforms classical structure learning   methods in elucidating the original model of the simulated dataset.   Finally we tested the GA approach on a real dataset where it reach   62% of recovered connections (sensitivity) and 64% of direct   connections (precision), outperforming the other algorithms.'),
(1441,'Recently, classifier ensemble methods are   gaining more and more attention in the machine-learning and   data-mining communities. In most cases, the performance of an   ensemble is better than a single classifier. Many methods for   creating diverse classifiers were developed during the past   decade. When these diverse classifiers are generated, it is   important to select the proper base classifier to join the   ensemble. Usually, this selection process is called pruning the   ensemble. In general, the ensemble pruning is a selection process in   which an optimal combination will be selected from many existing   base classifiers. Some base classifiers containing useful   information may be excluded in this pruning process. To avoid this   problem, the multilayer ensemble pruning model is used in this   paper. In this model, the pruning of one layer can be seen as a   multimodal optimization problem. A novel multi-sub-swarm particle   swarm optimization (MSSPSO) is used here to find multi-solutions for   this multilayer ensemble pruning model. In this model, each base   classifier will generate an oracle output. Each layer will use   MSSPSO algorithm to generate a different pruning based on previous   oracle output. A series of experiments using UCI dataset is   conducted, the experimental results show that the multilayer   ensemble pruning via MSSPSO algorithm can improve the generalization   performance of the multi-classifiers ensemble system. Besides, the   experimental results show a relationship between the diversity and   the pruning technique.'),
(1442,'In this paper we analyze existing methods and   languages for modeling agent interactions and propose a SDLMAS   Framework for rapid design, development and runtime support of   multi-agent systems. The framework provides a simple but expressive   declarative language for modeling complex interactions among   agents. Proposed language is based on scenarios, sequences of   conversation actions directed towards achieving a goal. Scenario   descriptions are converted into program code for a chosen target   agent platform and system execution is supported by a runtime   framework.'),
(1443,'This paper proposes a personalized URL   re-ranking method based on psychological characteristics of users   browsing. The characteristics are classified into three groups,   which are \"common-mind,\" \"uncommon-mind,\" and \"extremely   uncommonmind.\" Our personalization method constructs an index of the   anchor text retrieved from the web pages that the user has clicked   during his/her past searches. Our method provides different weights   to the anchor text according to the psychological characteristics   for re-ranking URLs. In the experimental section, we show that our   method can provide better performance than Google and another web   personalization method in terms of the average rank.'),
(1444,'A lot of work is devoted to analysing architectures for coordinating the behaviour of individual agents. However, providing agents with abilities to migrate continues to be a highly challenging problem. We propose a novel multi-agent framework, called Agent-based Migration (AM). We begin by defining the principal objective, which is the migration phenomenon applied, in our case, to distributed calculation of prime numbers. We present the AM architecture in detail. Then, we introduce different types of migration and the communication scheme. We also conduct a set of experiments in two environments: 4 heterogeneous computers and 45 (almost) homogeneous computers. Specifically, we are looking for a way to find optimal configurations for migration in both environments. The conclusion from this work is that introducing propagation to a system in a form of agent migration in both networks could considerably decrease the execution time according to used algorithms and established assumptions.'),
(1445,''),
(1446,'Traditional paper documents are not likely to   disappear in the near future as they are present everywhere in daily   life, however, paper-based documentation lacks the link with the   digital world for agile and automated processing. At the same time   it is prone to cloning, alteration and counterfeiting   attacks. E-passport defined by ICAO and implemented in 45 countries   is the most relevant case of hybrid documentation (i.e. paper format   with electronic capabilities) to date, but, as the advantages of   hybrid documentation are recognized more and more will undoubtedly   appear. In this paper, we present the concept and security   requirements of general-use e-documents, analyze the most   comprehensive security solution (i.e. ePassport security mechanisms)   and its suitability for general-purpose e-documentation. Finally, we   propose alternatives for the weakest and less suitable protocol from   ePassports: the BAC (Basic Access Control). In particular, an   appropriate key management infrastructure for access control to   document memory is discussed in conjunction with a prototype   implementation.'),
(1447,'This paper deals with a description of wireless   sensor networks at the beginning. Further follows the introduction   to the agent platform suitable for wireless networks. Mobile code   and sensor networks suffer from considerable security problems. Our   proposal of countermeasure is based on combination of smartcards   with sensor nodes. Smartcards as a tamper resistant devices offer   solution for the most of commonly required security   objectives. Analysis of proposed hardware cryptographic platform   includes link level communication, transport protocol description,   application interface description and demands for power   consumption. Today smartcards are highly standardized devices that   offer common communication interface and platform could be used with   various smartcards in accordance with ISO/EMV standards. At the end,   we discuss the combination of agents in wireless sensor networks in   conjunction with usage of cryptographic protocols for securing of   wireless networks.'),
(1448,'Wireless networks, telecommunications, and   information technologies connected de-vices in pervasive computing   environments require a high speed encryption for providing a high   security and a privacy. The CHESS-64 based on various controlled   operations is designed forsuch applications. In this paper, however,   we show that CHESS-64 doesn\'t have a high security level, more   precisely, we present two related-key differential attacks on   CHESS-64. The first at-tack requires about   2<sup>44</sup> data and 2<sup>44</sup> time   complexities (recovering 20 bits of the master key)while the second   attack needs about 2<sup>39</sup> data and   2<sup>39</sup> time complexities (recovering 6 bits of   themaster key). These works are the first known cryptanalytic   results on CHESS-64 so far.'),
(1449,'Normally, the multimedia content provider and network service providers are separated in mobile TV systems. The TV programs are broadcasted from the content provider to the mobile terminals through Digital Video Broadcasting Transmission System for Handheld Terminals (DVB-H), and the access information is unicasted from the service provider to the user via General Packet Radio Services (GPRS) networks. Due to the network architecture heterogeneity, protocols variation and algorithms difference, securing mobile TV content is becoming a significant challenge. In this paper, we present the architecture, protocol, user identification and digital right management (DRM) for protecting mobile TV multimedia content. The network architecture describes the integrated DVB-H and GPRS to provide secure mobile TV services. The efficient protocols and algorithms are proposed to encrypt the content and also decrypt the coded content. The user identification is able to identify the legal user by matching the username-password pair or the scanned fingerprint. The DRM is able to protect the data from both DVB-H and GPRS. Following this framework, the illegal usage of the mobile TV services can be efficiently prevented and the real-time multimedia Quality-of-Service (QoS) with respect to delay can be guaranteed. The real implementation has demonstrated the effectiveness of the multimedia content protection in the heterogeneous mobile networks. In addition, the delay is sufficiently low to provide live TV.'),
(1450,'In this paper, we consider password-based   authenticated key exchange with different passwords, where the users   only share a password with the trusted server but do not share   between themselves. The server helps the users share a   cryptographically secure session key by using their different   passwords. We propose a light-weight password-based authenticated   key exchange protocol with different passwords, i.e., it requires   only 2 rounds and 4 modular exponentiations per user. The protocol   provides forward secrecy, known-key secrecy, key secrecy against the   curious server, and security against undetectable online dictionary   attacks <I>without</I> random oracles.'),
(1451,'Ubi-Com has emerged as an exciting new paradigm   to provide intelligent computig and communications at anytime and   anywhere. But, In order to take the advantages of such services, it   is important that intelligent security framework be suitable for   Ubi-Com. In this paper, we propose privacy and access control scheme   by surveillance which is one of core security technologies for   ubiquitous hybrid intelligent security framework. In this scheme,   the device information and the signature information can be added to   the image data obtained by the image capturing device to maintain   security of the image data and use the image data as digital proof   when a specific event is generated.'),
(1452,'In this paper we propose an Image Analysis   technique for Vascular Pattern of Hand Palm, which in turn leads   towards Palm Vein Authentication of an individual. Near-Infrared   Image of Palm Vein pattern is taken and passed through three   different processes or algorithms to process the Infrared Image in   such a way that the future authentication can be done accurately or   almost exactly. These three different processes are: a. Vascular   Pattern Marker Algorithm (VPMA); b. Vascular Pattern Extractor   Algorithm (VPEA); and c. Vascular Pattern Thinning Algorithm   (VPTA). The resultant Images will be stored in a Database, as the   vascular patterns are unique to each individual, so future   authentication can be done by comparing the pattern of veins in the   palm of a person being authenticated with a pattern stored in a   database.'),
(1453,'Operations of mobile ad hoc networks rely on the   collaboration of participating nodes to route data for each other.   This standard approach using a fixed set of nodes for each   communication link cannot cope with high mobility due to a high   frequency of link breaks.  A recent approach based on virtual   routers has been proposed to address this problem.  In this new   environment, virtual routers are used for forwarding data.  The   functionality of each virtual router is provided by the mobile   devices currently within its spatial proximity.  Since these routers   do not move, the communication links are much more robust compared   to those of the conventional techniques.  In this paper, we   investigate techniques to enforce collaboration among mobile devices   by identify and punish misbehaving users in supporting the virtual   router functionality.  Simulation results based on various system   configurations are given.  They indicate that the proposed technique   is effective.'),
(1454,'Pervasive smart computing environments make people get accustomed to convenient and secure services. The overall goal of this research is to classify vehicles along the I215 freeway in Salt Lake City, USA. This information will be used to predict future roadway needs and the expected life of a roadway. The classification of vehicles will be performed by a synthesis of multiple sets of features. All feature sets have not yet been determined; however, one such set will be the reduced wavelet transform of the image of a vehicle. In order to use such a feature, it is necessary that the image be normalized with respect to size, position, and so on. For example, a car in the right most lane in an image will appear smaller than one in the left most lane, because the right most lane is closest to the camera. Likewise, a vehicle?s size will vary depending on where in a lane its image is captured. In our case, the image capture area for each lane is approximately 100 feet of roadway. A goal of this paper is to normalize the image of a vehicle so that regardless of its lane or position in a lane, the features will be approximately the same. The wavelet transform itself will not be used directly for recognition. Instead, it will be input to a neural network and the output of the neural network will be one element of the feature set used for recognition.'),
(1455,''),
(1456,'We revise and extend the foundation of computable topology in the framework of Type-2 theory of effectivity, TTE, where continuity and computability on finite and infinite sequences of symbols are defined canonically and transferred to abstract sets by means of notations and representations. We start from a computable topological space, which is a T0-space with a notation of a base such that intersection is computable, and define a number of multi-representations of the points and of the open, the closed and the compact sets and study their properties and relations. We study computability of boolean operations. By merely requiring \"provability\" of suitable relations (element, non-empty intersection, subset) we characterize in turn computability on the points, the open sets (!), computability on the open sets, computability on the closed sets, the compact sets(!), and computability on the compact sets. We study modifications of the definition of a computable topological space that do not change the derived computability concepts. We study subspaces and products and compare a number of representations of the space of partial continuous functions. Since we are operating mainly with the base elements, which can be considered as regions for points (\"pointless topology\"), we study to which extent these regions can be filled with points (completions). We conclude with some simple applications including Dini\'s Theorem as an example.'),
(1457,'A finite-time computable function is a partial   function from <sup></sup> to    <sup></sup> whose value is constructed by   concatenating a finite list with a suffix of the argument. A   finite-time computability preserving conversion  :   <I>X</I>  <I>Y</I> for   <I>X</I>, <I>Y</I>    <sup></sup> is a bijection which preserves   finite-time computability. We show that all the finite-time   computability preserving conversions with the domain   <sup></sup> are extended sliding block   functions.'),
(1458,'We study the computability properties of   symmetric hyperbolic systems of PDE <IMG   SRC=\"computing_the_solution_operators/images/img1.jpg\">, with   the initial condition <IMG   SRC=\"computing_the_solution_operators/images/img2.jpg\"> =   (<I>x</I><sub>1</sub>,...,<I>x<sub>m</sub></I>). Such   systems first considered by K.O. Friedrichs can be used to describe   a wide variety of physical processes. Using the difference equations   approach, we prove computability of the operator that sends (for any   fixed computable matrices <I>A</I>,   <I>B</I><sub>1</sub>, ...,   <I>B<sub>m</sub></I> satisfying certain   conditions) any initial function     <I>C<sup>p</I>+1</sup>(<I>Q</I>,   <I><sup>n</sup></I>) (satisfying   certain conditions), <I>p</I>  2, to the unique   solution <B>u</B>    <I>C</I><sup><I>p</sup></I>(<I>H</I>,   <I><sup>n</sup></I>), where   <I>Q</I> =   [0,1]<sup><I>m</I></sup> and   <I>H</I> is the nonempty domain of correctness of the   system.'),
(1459,'The Tietze-Urysohn Theorem states that every   continuous real-valued function defined on a closed subspace of a   normal space can be extended to a continuous function on the whole   space. We prove an effective version of this theorem in the Type Two   Model of Effectivity (TTE). Moreover, we introduce for qcb-spaces a   slightly weaker notion of normality than the classical one and show   that this property suffices to establish an Extension Theorem for   continuous functions defined on functionally closed   subspaces. Qcb-spaces are known to form an important subcategory of   the category <tt>Top</tt> of topological   spaces. <tt>QCB</tt> is cartesian closed in contrast to   <tt>Top</tt>.'),
(1460,'Consider the initial value problem of the   first-order ordinary differential equation <br> <IMG   SRC=\"topological_complexity_of_blowup/images/img1.jpg\"> <br> where the   locally Lipschitz continuous function <IMG   SRC=\"topological_complexity_of_blowup/images/img2.jpg\"> with open   domain and the initial datum   (<I>t</I><sub>0</sub>,   <I>x</I><sub>0</sub>)  <IMG   SRC=\"topological_complexity_of_blowup/images/img4.jpg\"> are given. It is shown   that the solution operator producing the maximal \"time\" interval of   existence and the solution on it is computable. Furthermore, the   topological complexity of the blowup problem is studied for   functions <I>f</I> defined on the whole space. For each   such function <I>f</I> the set <I>Z</I> of   initial conditions (<I>t</I><sub>0</sub>,   x<sub>0</sub>) for which the positive solution does not   blow up in finite time is a <IMG   SRC=\"topological_complexity_of_blowup/images/img3.jpg\">-set. There   is even a computable operator determining <I>Z</I> from   <I>f</I>. For <I>l</I>  2 this upper   <IMG   SRC=\"topological_complexity_of_blowup/images/img3.jpg\">-complexity   bound is sharp. For <I>l</I> = 1 the blowup problem is   simpler.'),
(1461,'To gain insight into the relationship between physical theories and computation, we examine the link between measurement devices and computers in the framework of TTE. Starting from a formal definition of a measurement procedure, different approaches to associate a representation with a measurement procedure are studied, and an equivalence class of representations suitable for representing the results of a measurement is defined for each measurement procedure.'),
(1462,'We discuss the integral and Fubini\'s Theorem for   a Fine-computable function <I>F</I>(<I>x,   y</I>) on the upper-right open unit square [0, 1) x [0,1). The   core objective is Fine-computability of   <I>f</I>(<I>x</I>) = <sub>   [0,1)</sub>   <I>F</I>(<I>x,y</I>)<I>dy</I> as   a function of <I>x</I>  [0,1).'),
(1463,'We construct the Urysohn metric space in   constructive setting without choice principles. The Urysohn space is   a complete separable metric space which contains an isometric copy   of every separable metric space, and any isometric embedding into it   from a finite subspace of a separable metric space extends to the   whole domain.'),
(1464,'We investigate under what conditions a   co-recursively enumerable set <I>S</I> in a computable   metric space (, <I>d</I>, ) is   recursive. The topological properties of <I>S</I> play   an important role in view of this task. We first study some   properties of computable metric spaces such as the effective   covering property. Then we examine co-r.e. sets with disconnected   complement, and finally we focus on study of chainable and   circularly chainable continua which are co-r.e. as subsets of   . We prove that, under some assumptions on , each   co-r.e. circularly chainable continuum which is not chainable must   be recursive. This means, for example, that each co-r.e. set in <b>R</b><I><sup>n</sup></I>   or in the Hilbert cube which has topological type of the Warsaw   circle or the dyadic solenoid must be recursive. We also prove that   for each chainable continuum <I>S</I> which is   decomposable and each  > 0 there exists a recursive   subcontinuum of <I>S</I> which is -close to   <I>S</I>.'),
(1465,'We consider the uniform model of computation   over any structure with two constants. For several structures, we   construct oracles which imply that the relativized versions of P and   NP are equal or are not equal. We construct universal oracles which   imply the equality of the relativized versions of P and NP and we   show that we lose the possibility to define these oracles   recursively if we try to compress their elements to tuples of fixed   length. Moreover we give new oracles for the BSS model in order to   separate the classes P and NP relative to these oracles.'),
(1466,'In this paper we consider the computability of   the solution of the initialvalue problem for differential equations   and for differential inclusions with semicontinuous right-hand   side. We present algorithms for the computation of the solution   using the \"ten thousand monkeys\" approach, in which we generate all   possible solution tubes, and then check which are valid. In this   way, we show that the solution of a locally Lipschitz differential   equation is computable even if the function is not effectively   locally Lipschitz, and recover a result of Ruohonen, in which it is   shown that if the solution is unique, then it is computable. We give   an example of a computable locally Lipschitz function which is not   effectively locally Lipschitz. We also show that the solutions of a   convex-valued upper-semicontinuous differential inclusion are   upper-semicomputable, and the solutions of a lower-semicontinuous   one-sided Lipschitz differential inclusion are   lower-semicomputable.'),
(1467,'We construct a computable Banach space which   possesses a Schauder basis, but does not possess any computable   Schauder basis.'),
(1468,''),
(1469,'This paper presents a new vision of e-Learning   and quality in e-Learning based on two main assumptions: e-Learning   cannot be seen as a \"one-size-fits all\" solution as it has as many   definitions as the fields and sectors where it is implemented, and   to define quality in e-Learning one must consider the influence of   visions of stakeholders on quality perception. The paper analyses   in-depth the e-Learning territory concept and provides an innovative   view on quality approaches for e-Learning as well as a set of   recommendations addressing e-Learning stakeholders.'),
(1470,'Multi-disciplinary teams might provide a   multi-faceted perspective of web educational systems that integrates   experience from different fields. Each expert has a view of the   system and she uses domain specific languages in order to express   solutions to the problems she is concerned with. In this way, the   final system can be seen as a combination of a number of   complementary views, each of which focuses on problems of a   different nature. However, such views are expressed with different   specification tools so that they have to be integrated to produce a   common design that is complete and consistent. Creating a common   language encompassing multi-disciplinary design views is a   challenging endeavor but it might impose a cognitive overload to   each member of the group who is exposed to unfamiliar concepts of   other disciplines in her design view. Alternatively, this paper   describes an approach called MODUWEB that consists of letting each   designer use the tool she is proficient in for her design tasks and   then complementary design perspectives are integrated using   meta-modeling techniques. MODUWEB also includes a number of   constraints and semantic rules that guarantee the completeness and   consistency of the resulting model.'),
(1471,'Learning Objects Repositories are becoming increasingly available on the Internet. Learning Objects Pool (LOP) built around the \"stock exchange\" metaphor, brings a new concept of Learning Objects Repositories pushing users motivation to produce good LOs as well as increasing the cooperation between users, either by submitting suggestions and comments or rating existing LOs. To achieve such high level of motivation and interest some kind of healthy competition is promoted, assigning credits to users and setting a value cost for each LO. This credit-based system rewards users that collaborate by creating LOs or by adding valuable information. It consequently increases the value of the most popular LOs, and also allows the creation of users and LOs rankings. This paper provides a comprehensive overview of the LOP system and, in particular, describes some application scenarios where, through configuration and parameterization we show the LOP?s high levels of versatility.'),
(1472,'Intelligent Tutoring Systems are computer   programs that aim at providing personalized instruction to   students. In recent years, conversational robots, usually known as   chatterbots, become very popular in the Internet, and ALICE   (Artificial Linguistic Internet Computer Entity) is probably the   most popular one. ALICE brain is written in AIML (Artificial   Intelligence Markup Language), an open XML language. We have   considered the combination of both approaches, i.e, the use of   AIML-based bots for tutoring purposes in open e-Learning platforms   such as Claroline or Moodle. With that aim in mind, we have   developed a bot (chatterbot) for helping the students during their   learning process and for supporting the activities of the   teacher. This bot (TQ-Bot) is able to analyse the requests made by   the learners in written natural language and to provide adequate and   domain specific answers orienting the student to the right course   contents. Besides, TQ-Bot is able to track and supervise the student   progress by means of personalized questionnaires. This bot has been   developed and integrated as user-friendly modules in Claroline and   Moodle.'),
(1473,'Eduquito is intended to be a virtual environment   for digital/social inclusion where people can exercise their   citizenship with interaction and personal development. As a virtual   space, it comprises a Learning by Project Environment which provides   not only access resources for People with Special Educational Needs   but also an array of special tools, which foster a process of   creative dialogue as well as dynamic individual and collective   development.'),
(1474,'Information and Communication Technologies (ICT)   permit the innovation of teaching and learning processes. ICT allow   teachers to create or select and adjust contents that take advantage   of the digital environment and interaction between peers. Teaching   methodologies and strategies should be adjusted to the learning   styles of students, offering them, in turn, the possibility to   reflect about the way in which they might learn better. This article   introduces a work of creation and validation of a web-based   application, which aims to enhance the Management of Learning Styles   (MLS) on the part of students and teachers, based on   Felder-Soloman?s Index of Learning Styles Questionnaire (ILS) and   Honey-Mumford?s Learning Styles Questionnaire (LSQ). The prototype   has been validated and the results suggest its applicability and the   relevance of the information this tool is capable of obtaining -    reports on the learning styles profiles by student, teacher and   class - with the objective of supporting them in the selection of   strategies to improve teaching and learning, developing at the same   time skills which will allow them to learn throughout their lives.'),
(1475,'This article presents Semantic Spiral Timelines   (SST) as an interactive visual tool aimed at the exploration and   analysis of additional academic information stored in current   e-learning platforms. Despite the development of contents   specifically for these platforms, and in spite of the various   features they provide, knowledge of the actual use made by   individual participants is emerging as an unavoidable necessity, so   as to ensure proper operation and effective use of e-learning   platforms. SST supports the discovery of temporal patterns by   incorporating an innovative highly interactive visual   representation, which can be explored at various levels.  This tool   makes it possible to assess, at first glance, the use of the   e-learning platform during the development of courses; one can also   perceive how it is used by class participants. Then, through   different interaction mechanisms, it is possible for students and   professors to uncover specific details about courses, which would   otherwise remain hidden.'),
(1476,'This article describes the educational tool   VAST. We designed VAST to be used in compiler and language   processing courses. The current version allows generat- ing and   visualizing syntax trees and their construction process. The main   advantages of VAST follow: it is designed to be as independent from   the parser generator as possible, it allows students to visualize   the behavior of parsers they develop, and it has an inter- face   designed to easily handle huge syntax trees. Finally, we describe   two satisfactory preliminary evaluations from the usability and   educational points of view.'),
(1477,'Learning to program is an important subject for   students of Computer Science. Mentoring these students is a   time-consuming and complex task. In this paper, we present a   learning and tutoring environment that integrates task/solution   delivery, assessment support and tutor?s annotations, by extending   Eclipse to a \"Real World Integrated Development Environment\". We   will present a distributed system that uses Tuple Space architecture   to integrate Eclipse with an evaluation module and a hand-writing   annotation feature.'),
(1478,'');

/*!40101 SET SQL_MODE=@OLD_SQL_MODE */;
/*!40014 SET FOREIGN_KEY_CHECKS=@OLD_FOREIGN_KEY_CHECKS */;
/*!40014 SET UNIQUE_CHECKS=@OLD_UNIQUE_CHECKS */;
/*!40111 SET SQL_NOTES=@OLD_SQL_NOTES */;
